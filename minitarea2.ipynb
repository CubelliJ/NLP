{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2obO44rRIDIm"
   },
   "source": [
    "# Minitarea 2\n",
    "\n",
    "Nombre: Joaquín Cubelli "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JstKx3TiKcIp"
   },
   "source": [
    "---------------------------\n",
    "## Language Models (3 pts)\n",
    "Estas preguntas son teóricas y deben ser resueltas con desarrollo, pero sin código. Por favor usen $\\LaTeX$ para las fórmulas. El material del curso y los videos deberian ser suficientes para que puedan responder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hwW-07MrRpt"
   },
   "source": [
    "\n",
    "### Pregunta 1 (1 pt)\n",
    "Asuma que tiene un modelo de lenguaje de trigramas (simple) entrenado sobre un corpus C. Muestre cómo el modelo le asigna una probabilidad a la oración `el perro le ladra al gato` usando los parámetros estimados a partir del corpus  $q(w_i | w_{i-2}, w_{i-1})$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VUjDx0NrYbg"
   },
   "source": [
    "**Respuesta:** \n",
    "\n",
    "Utilizando los tokens $*$ y *STOP*, se obtiene la siguiente probabilidad:\n",
    "\n",
    "P(El,perro,le,ladra,al,gato) = $q(El|*,*)*q(perro|*,El)*q(le|el,perro)*q(ladra|perro,le)*q(al|le,ladra)*q(gato|ladra,al)*q(STOP|al,gato)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwNkPIXure0C"
   },
   "source": [
    "### Pregunta 2 (1 pt)\n",
    "Muestre cómo se calcularía  $q(w_{i} | w_{i-2}, w_{i-1})$  usando un modelo que interpola un modelo de lenguajes de trigramas, bigramas, y unigrama.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeLZAd0Tr9ne"
   },
   "source": [
    "**Respuesta:**\n",
    "\n",
    "$q(w_{i} | w_{i-2}, w_{i-1})$ = $λ_1 * q_{ML}(w_{i} | w_{i-2}, w_{i-1})$ + $λ_2 * q_{ML}(w_{i} | w_{i-1})$ + $λ_3 * q_{ML}(w_{i})$\n",
    "\n",
    "Donde: \n",
    "\n",
    "$ q_{ML}(w_{i} | w_{i-2}, w_{i-1}) $ = $\\frac{Count(w_{i-2},w_{i-1},w_{i})}{Count(w_{i-2},w_{i-1})}$\n",
    "\n",
    "$ q_{ML}(w_{i} | w_{i-1}) $ = $\\frac{Count(w_{i-1},w_{i})}{Count(w_{i-1})}$\n",
    "\n",
    "$ q_{ML}(w_{i}) $ = $\\frac{Count(w_{i})}{Count()}$\n",
    "\n",
    "Y los λ se calculan buscando el mínimo Perplexity, dado que son hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHqcRJ7Vr_8x"
   },
   "source": [
    "### Pregunta 3 (1 pt)\n",
    "¿Qué ventajas tiene el modelo interpolado sobre el modelo de trigramas simple?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6F5R3Ji7sHjt"
   },
   "source": [
    "**Respuesta:**\n",
    "\n",
    "Disminuye la sparsness, al hacer que tanto trigramas como bigramas que tendrían probabilidad cero bajo un modelo de trigramas simple porque no fueron encontradas en el corpus, tengan una probabilidad dada mayor a cero en base a sus bigramas y unigramas que lo conforman, permitiendo suavizar de cierta manera las probabilidades y generalizando mejor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdmB-07ZKfaa"
   },
   "source": [
    "-----------------------\n",
    "## Naive Bayes (3 pts)\n",
    "En esta parte de la minitarea deberan programar Naive Bayes Multinomial usando Laplace Smothing. Las referencias las pueden encontrar en el material del curso y los videos del profesor.\n",
    "\n",
    "A continuacion se presentan un conjunto de documentos de training divididos en 2 categorias distintas. Ustedes deberan clasificar los documentos del test set usando Naive Bayes con Laplace Smothing.\n",
    "\n",
    "Por favor, documenten su codigo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en algun lugar de su codigo hacen algo que creen que debe ser explicado, pongan comentarios, son bienvenidos. \n",
    "\n",
    "\n",
    "**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reemplacenlas por sumas de logaritmos para prevenir errores de precision. Revisen la diapo 69 de las slides. \n",
    "\n",
    "NOTA: Sobre las `namedtuple`s. Es el tipo de los documentos. Son objetos inmutable que tienen dos atributos: `words` donde estan las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`. Son libres de implementar su solucion como quieran, si no les gusta este tipo de dato usen otro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "HLi8PxdV2VQX",
    "outputId": "130221d2-3313-4660-a1b8-45bf1f0aad1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train documents:\n",
      "(document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0), document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1), document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0), document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1), document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1), document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=2), document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0), document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1))\n",
      "\n",
      "Test documents:\n",
      "(document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None), document(words=('w03', 'w06', 'w05', 'w02', 'w05', 'w02', 'w01'), class_=None), document(words=('w03', 'w03', 'w07', 'w01', 'w06', 'w09', 'w01', 'w08'), class_=None))\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "document = namedtuple(\n",
    "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
    ")\n",
    "\n",
    "train_set = (\n",
    "    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
    "    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
    "    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
    "    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
    "    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
    "    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=2),\n",
    "    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
    "    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",
    ")\n",
    "print(\"Train documents:\")\n",
    "print(train_set)\n",
    "\n",
    "\n",
    "test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n",
    "            document(words=('w03', 'w06', 'w05', 'w02', 'w05', 'w02', 'w01'), class_=None),\n",
    "            document(words=('w03', 'w03', 'w07', 'w01', 'w06', 'w09', 'w01', 'w08'), class_=None)\n",
    ")\n",
    "print(\"\\nTest documents:\")\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # Se importa math para calcular logaritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, se crea el vocabulario en base al train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(set): # Funcion que crea el vocabulario en base al train_set\n",
    "    V = []\n",
    "    for j in range(len(train_set)):\n",
    "        for i in set[j][0]:\n",
    "            V.append(i)\n",
    "    return list(dict.fromkeys(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w03', 'w01', 'w02', 'w06', 'w08', 'w07', 'w05', 'w04', 'w00', 'w09']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = create_vocab(train_set)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos generalizar, por lo que buscamos en el train_set todos los nombres de las clases, es más, se modificó el train set y se creó una clase nueva para ver si generalizaba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesNames(set):\n",
    "    C = []\n",
    "    for j in range(len(train_set)):\n",
    "        i = set[j][1]\n",
    "        C.append(i) # Se agregan todas los nombres de las clases a una lista.\n",
    "    return (list(dict.fromkeys(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = classesNames(train_set)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_c(set): \n",
    "    n = len(set)\n",
    "    c = classesNames(set)\n",
    "    doc_count = [0]*len(c) # Se genera una lista que va a ser usada para contar los documentos de cada clase.\n",
    "    for j in range(n):\n",
    "        cls = set[j][1] # Clase documento j.\n",
    "        for i in range(len(c)):\n",
    "            if cls == c[i]:\n",
    "                doc_count[i]+=1\n",
    "    for i in range(len(c)): # Una vez contados todos los documentos, se divide en el numero total, y se aplica el log.\n",
    "        doc_count[i] = math.log(doc_count[i]/n)\n",
    "    \n",
    "    return doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9808292530117262, -0.6931471805599453, -2.0794415416798357]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_c(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_w_hyperword(set, vocab, classname): # Se cuentan todas las palabras de cada asociada a cada clase, y se\n",
    "                                             # calcula su probabilidad\n",
    "    C = []\n",
    "    l = len(vocab)\n",
    "    for i in set: \n",
    "        if i[1] is classname:\n",
    "            for j in i[0]:\n",
    "                C.append(j)\n",
    "    Count = [1]*l\n",
    "    log_probc = Count\n",
    "    for i in range(0,l):\n",
    "        for j in range(0,len(C)):\n",
    "            if C[j]==vocab[i]:\n",
    "                Count[i]+=1 \n",
    "    Word_n = 0\n",
    "    #print(Count)\n",
    "    for i in Count:\n",
    "        Word_n+=i\n",
    "    #print(Word_n)\n",
    "    for i in range(l):\n",
    "        log_probc[i]=math.log(Count[i]/Word_n)\n",
    "    return log_probc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.3978952727983707,\n",
       " -2.3978952727983707,\n",
       " -1.8870696490323797,\n",
       " -2.3978952727983707,\n",
       " -1.8870696490323797,\n",
       " -2.1102132003465894,\n",
       " -2.803360380906535,\n",
       " -2.3978952727983707,\n",
       " -2.3978952727983707,\n",
       " -2.803360380906535]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_w_hyperword(train_set, V, 0)\n",
    "#for i in prob_w_hyperword(train_set, V, 0):\n",
    "#    print(math.exp(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test, train): \n",
    "    # Operaciones de train:\n",
    "    vocab = create_vocab(train)\n",
    "    p_c = prob_c(train)\n",
    "    c = classesNames(train)\n",
    "    nc = []\n",
    "    for i in range(len(c)): # Se calculan las probabilidades de que cada palabra pertenezca a cada clase\n",
    "        nc.append(prob_w_hyperword(train, vocab, i))\n",
    "    # Operaciones en test:\n",
    "    p = []\n",
    "    for i in range(len(c)):\n",
    "        p.append(prob_c(train))\n",
    "\n",
    "    for i in range(len(test)):\n",
    "        for j in test[i][0]:\n",
    "            for m in range(len(c)):\n",
    "                for k in range(len(vocab)):\n",
    "                    if j == vocab[k]:\n",
    "                        p[i][m] += nc[m][k]\n",
    "    \n",
    "    #for i in p:\n",
    "        #print(i) #prueba, sirve para ver el máximo de las clases para cada oración.\n",
    "    \n",
    "    # Se entrega el resultado en base al valor más alto logaritmicamente en probabilidad.\n",
    "    for i in range(len(test)):\n",
    "        print(\"Oración \"+str(i)+\" corresponde a clase \",end=\"\")\n",
    "        for m in range(len(c)): \n",
    "            if p[i][m]==max(p[i]):\n",
    "                print(c[m])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 0 corresponde a clase 0\n",
      "Oración 1 corresponde a clase 2\n",
      "Oración 2 corresponde a clase 0\n"
     ]
    }
   ],
   "source": [
    "classify(test_set, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atención:** Se cambió el database para crear un código más generalizado, que pudiese procesar más data de prueba simultaneamente, y más clases."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "minitarea2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
