{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:30:18.109327Z",
     "start_time": "2020-03-19T18:30:18.103344Z"
    },
    "colab_type": "text",
    "id": "q5CSRY4oNCHK"
   },
   "source": [
    "\n",
    "# Minitarea 3\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "Nombre: Joaquin Cubelli\n",
    "\n",
    "Fecha de Entrega: Domingo 9 de Junio\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "- El ejercicio consiste en:\n",
    "\n",
    "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
    "    \n",
    "    - Entrenar Word2Vec y FastText sobre un pequeño corpus.\n",
    "    \n",
    "    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n",
    "\n",
    "- La minitarea es INDIVIDUAL.\n",
    "\n",
    "- Está demás decir que no se admiten copias, ni de código, ni de respuestas escritas. \n",
    "\n",
    "- La entrega debe ser por u-cursos.\n",
    "\n",
    "- Atrasos: se descontará un punto por día hábil de atraso tanto para las mini-tareas como para las competencias.\n",
    "\n",
    "- En el horario de auxiliar se abrirán horarios de consulta en donde podrán preguntar acerca del ejercicio y en general, de todo el curso. \n",
    "\n",
    "- Cada sección tiene un punto base y se evalúa sobre 6 puntos.\n",
    "\n",
    "- Al revisar, tu código será ejecutado. Verifica que tu entrega no tenga errores.\n",
    "\n",
    "\n",
    "## Referencias   \n",
    "\n",
    "Vídeos: \n",
    "\n",
    "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
    "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
    "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4wYf0vgnbTv"
   },
   "source": [
    "## Preguntas Teóricas\n",
    "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5hUG6-8ngoK"
   },
   "source": [
    "### Parte 1: Modelos Lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yRvZbhsoi8f"
   },
   "source": [
    "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irsqBVmCnx3M"
   },
   "source": [
    "**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. (3 puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Representación escogida del documento de entrada: Bag of Words, cuya largo lo denotaremos con |V|, probablemente utilice stopwords para disminuir la sparseness.\n",
    "\n",
    "Parámetros del modelo: $W ϵ R^{|V| x 4}, b ϵ R^{4}$\n",
    "\n",
    "Transformaciones necesarias: Softmax, dado que el output es un vector con una distribuicion de probabilidad de la pertenencia a cada clase.\n",
    "\n",
    "Función de pérdida escogida: Categorical Cross Entropy Loss con regularización L1, debido a que el problema es multiclase, estamos buscando una pertenencia con distribuicion de probabilidad y nuestro vector x posee elementos caracteristicos que debiesen tener mucho peso al momento de clasificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5FaWqBVvL90"
   },
   "source": [
    "**Pregunta 2**: Explique el proceso de entrenamiento y evaluación del modelo. (3 puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Se divide el dataset inicialmente en dos conjuntos, uno de Entrenamiento y otro de Testing, la proporcion de ellos es de 4:1 aproximadamente.\n",
    "\n",
    "El dataset de entrenamiento se subdivide posteriormente en la información que va a entrenar al modelo y el dataset que valida el modelo, con una proporción de 2:1.\n",
    "\n",
    "Se entrena el algoritmo con la información de entrenamiento transformandola en un Bag of Words y procesando los tokens. \n",
    "Se calcula la pérdida de la salida $\\vec{y}$ del algoritmo respecto al y gold, y se ajustan los parámetros en base a los gradientes de la pérdidas respecto a cada peso de la red.\n",
    "Este proceso se itera varias veces, hasta converger.\n",
    "\n",
    "Luego, se evalua el dataset de validación en el algoritmo, para observar el desempeño de éste y poder realizar ajustes.\n",
    "\n",
    "Una vez que se tiene un buen rendimiento en el clasificador, asegurandose de no sobreajustar, se realiza la prueba final, en donde se clasifica el dataset de Testing, otorgando la evaluación final.\n",
    "\n",
    "Hay que tener en cuenta que para clasificar adecuadamente, y no sobreajustar, es una buena practica hacer shuffles entre el training data y el validation data, y que el testing data no tiene que utilizarse en ningun momento previo a la evaluación final, para evitar que el clasificador se aprenda el conjunto de testing y no aprenda a generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkK7pc54njZq"
   },
   "source": [
    "### Parte 2: Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUbJjlj_9AFC"
   },
   "source": [
    "Supongamos que tenemos la siguiente red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obUfuOYB_TOC"
   },
   "source": [
    "![Red](https://drive.google.com/uc?id=1Yd0s9g5SlB1-XuVokGQO2J-yDudQe2Kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2z-8zKW0_6q"
   },
   "source": [
    "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, represéntela matemáticamente, entregando las dimensiones de las matrices y vectores. (3 Puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Formula:\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h (\\vec{h^3}) W^4$\n",
    "\n",
    "$\\vec{h^3} = f (\\vec{h^2}) W^3 + \\vec{b^3}$\n",
    "\n",
    "$\\vec{h^2} = g (\\vec{h^1}) W^2 + \\vec{b^2}$\n",
    "\n",
    "$\\vec{h^1} = (\\vec{x}) W^1 + \\vec{b^1}$\n",
    "\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h (f (g ((\\vec{x}) W^1 + \\vec{b^1}) W^2 + \\vec{b^2}) W^3 + \\vec{b^3}) W^4$\n",
    "\n",
    "$\\vec{x} ∈ R^{3}; W^1 ∈ R^{3 x 2}; b^1, h^1, z^1 ∈ R^{2}; W^2 ∈ R^{2 x 3}; b^2, h^2, z^2 ∈ R^{3}; W^3 ∈ R^{3 x 1}; b^3, h^3, z^3 ∈ R^{1}; W^4 ∈ R^{1 x 4}; \\vec{y} ∈ R^{4}$\n",
    "\n",
    "**Pregunta 2**: Qué es backpropagation? Cuales serían los parámetros a evaluar en la red neuronal anterior? (1 punto)\n",
    "\n",
    "**Respuesta**: Es un método para evaluar de manera eficiente el gradiente de una funcion de perdida L para una red neuronal Feed Forward, para poder entrenar una NN. Los parámetros a evaluar en la red neuronal anterior son los W^i, queremos ver hacia que direccion variar cada elemento de cada vector W tal que cuando se entrene la red, la perdida sea mínima.\n",
    "\n",
    "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? (2 puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "- Se aplica un vector de entrada $\\vec{x}$ a la red y se propaga en esta para encontrar todas las activaciones de todas las unidades ocultas y de salida. \n",
    "- Se calcula el delta de la ultima capa, de la cual se pueden obtener sus valores directamente de la función de Loss.\n",
    "- Se utiliza la siguiente ecuación para calcular las derivadas requeridas de la ultima capa.\n",
    "\n",
    " $$ \\frac{\\partial L}{\\partial W^l_{[i,j]}} = \\vec{\\delta^l_{[j]}} x \\vec{z^{l-1}_{[i]}} $$\n",
    " \n",
    "- Se propaga hacia atras el delta calculado, calculando el delta anterior de cada unidad oculta. Se recorre de las capas superiores a inferiores en la red usando la siguiente ecuación.\n",
    "\n",
    " $$\\vec{\\delta^l_{[j]}} = g^l(\\vec{h^l}_{[j]}) x \\sum\\limits_{k}(\\vec{\\delta^{l+1}_{[j]}} x W^{l+1}_{[j,k]})$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocS_vQhR1gcU"
   },
   "source": [
    "## Pregunta Práctica:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ol82nJ0FnmcP"
   },
   "source": [
    "### Parte 3: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgmeSFqKLpFL"
   },
   "source": [
    "En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de diálogos de los Simpson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecCvnryeQiG7"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZgN06q4QPi3"
   },
   "source": [
    "Utilizando el dataset adjunto con la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eY3kmg4onnsu"
   },
   "outputs": [],
   "source": [
    "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
    "df = pd.read_csv(data_file)\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAg5a5bmWk3T"
   },
   "source": [
    "**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. (4 puntos) (Hint, le puede servir explorar un poco los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWw2fXFRXe5Y"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procesamiento de dataset y tokenizado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         raw_character_text                                   spoken_words\n",
      "94070           Cookie Kwan   Wipe your tears with a monogrammed napkin...\n",
      "99998   Thought Bubble Lisa  Mr. Szyslak, you want to buy some band candy?\n",
      "1116          Marge Simpson                   Oh my God! Mother was right.\n",
      "128435         Bart Simpson                                 Piece of cake.\n",
      "82672         Homer Simpson                     Marge, the coast is clear.\n"
     ]
    }
   ],
   "source": [
    "sample = df.sample(5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YILUICGtYJo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:55:32,837 : INFO : NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "content = df['raw_character_text'] + ' ' + df['spoken_words'] # Unimos la persona con su oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Miss Hoover No, actually, it was a little of b...\n",
      "1                  Lisa Simpson Where's Mr. Bergstrom?\n",
      "2    Miss Hoover I don't know. Although I'd sure li...\n",
      "3              Lisa Simpson That life is worth living.\n",
      "4    Edna Krabappel-Flanders The polls will be open...\n",
      "5    Martin Prince I don't think there's anything l...\n",
      "6                        Edna Krabappel-Flanders Bart?\n",
      "7          Bart Simpson Victory party under the slide!\n",
      "8           Lisa Simpson Mr. Bergstrom! Mr. Bergstrom!\n",
      "9    Landlady Hey, hey, he Moved out this morning. ...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample = content[0:10]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "\n",
    "def simple_tokenizer(doc, lower=False): # Tokenizer usado en el aux.\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "\n",
    "    tokenized_doc = [\n",
    "        token for token in tokenized_doc if token.lower() not in stopwords\n",
    "    ]\n",
    "    return tokenized_doc\n",
    "\n",
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogo de ejemplo tokenizado: [['Hoover', 'disease', 'magazines', 'news', 'natural'], ['Lisa', 'Simpson', 'Bergstrom'], ['Hoover', 'talk', 'touch', 'lesson', 'plan', 'teach'], ['Lisa', 'Simpson', 'life', 'worth', 'living'], ['Edna', 'KrabappelFlanders', 'polls', 'recess', 'decided', 'final', 'statements', 'Martin'], ['Martin', 'Prince', 'left'], ['Edna', 'KrabappelFlanders', 'Bart'], ['Bart', 'Simpson', 'Victory', 'party', 'slide'], ['Lisa', 'Simpson', 'Bergstrom', 'Bergstrom'], ['Landlady', 'Hey', 'hey', 'Moved', 'morning', 'job', 'Copernicus', 'costume'], ['Lisa', 'Simpson'], ['Landlady', 'train', 'Capital', 'City'], ['Lisa', 'Simpson', 'train', 'traditional', 'environmentally', 'sound'], ['Landlady', 'backbone', 'country', 'Leland', 'Stanford', 'drove', 'golden', 'spike', 'Promontory'], ['Lisa', 'Simpson', 'touched'], ['Bart', 'Simpson', 'Hey', 'vote'], ['Nelson', 'Muntz', 'vote', 'Votings', 'geeks'], ['Bart', 'Simpson', 'vote', 'girls'], ['Terrisherri', 'forgot'], ['Bart', 'Simpson', 'sweat', 'couple', 'people', 'Milhouse'], ['Milhouse', 'Van', 'Houten', 'Uh'], ['Bart', 'Simpson', 'Lewis'], ['Bart', 'Simpson', 'voted'], ['Milhouse', 'Van', 'Houten', 'Bart', 'vote'], ['Bart', 'Simpson', 'Uh'], ['Wendell', 'Borton', 'Yayyyyyyyyyyyyyy'], ['Bart', 'Simpson', 'demand', 'recount'], ['Edna', 'KrabappelFlanders', 'Martin', 'Martin', 'recount'], ['Bart', 'Simpson'], ['Edna', 'KrabappelFlanders', 'Martin', 'Martin'], ['Kid', 'Reporter', 'Mister', 'President'], ['Conductor', 'boarding', 'track', '5', 'afternoon', 'delight', 'coming', 'Shelbyville', 'Parkville', 'andâ€¦'], ['Lisa', 'Simpson', 'Bergstrom', 'Hey', 'Bergstrom'], ['BERGSTROM', 'Hey', 'Lisa'], ['Lisa', 'Simpson', 'Hey', 'Lisa'], ['BERGSTROM'], ['Lisa', 'Simpson', 'leave'], ['BERGSTROM', 'Lisa', 'life', 'substitute', 'teacher', 'fraud', 'wearing', 'gym', 'shorts', 'tomorrow', 'speaking', 'French', 'pretending', 'band', 'God'], ['Lisa', 'Simpson', 'teacher'], ['BERGSTROM', 'true', 'teachers'], ['Lisa', 'Simpson'], ['BERGSTROM', 'lie', 'projects', 'Capital', 'City'], ['Lisa', 'Simpson'], ['BERGSTROM', 'middle', 'class', 'cares', 'abandon'], ['Lisa', 'Simpson', 'understand', 'Bergstrom'], ['BERGSTROM'], ['BERGSTROM', 'feel', 'rely'], ['Lisa', 'Simpson', 'Bergstrom'], ['Conductor', 'aboard'], ['Lisa', 'Simpson', 'guess', 'mind', 'train', 'speeds', 'life']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dialogo de ejemplo tokenizado: {}\".format(cleaned_content[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento de Word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiremos primero el embedding Word2vec y lo entrenaremos.\n",
    "\n",
    "simpsons_w2v = Word2Vec(min_count=10,\n",
    "                      window=4,\n",
    "                      size=200,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:55:34,369 : INFO : collecting all words and their counts\n",
      "2020-06-09 18:55:34,371 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-09 18:55:34,409 : INFO : PROGRESS: at sentence #10000, processed 52242 words, keeping 10997 word types\n",
      "2020-06-09 18:55:34,430 : INFO : PROGRESS: at sentence #20000, processed 105199 words, keeping 17683 word types\n",
      "2020-06-09 18:55:34,453 : INFO : PROGRESS: at sentence #30000, processed 161865 words, keeping 23783 word types\n",
      "2020-06-09 18:55:34,484 : INFO : PROGRESS: at sentence #40000, processed 214884 words, keeping 28073 word types\n",
      "2020-06-09 18:55:34,509 : INFO : PROGRESS: at sentence #50000, processed 266154 words, keeping 32109 word types\n",
      "2020-06-09 18:55:34,537 : INFO : PROGRESS: at sentence #60000, processed 315182 words, keeping 35628 word types\n",
      "2020-06-09 18:55:34,569 : INFO : PROGRESS: at sentence #70000, processed 367905 words, keeping 39375 word types\n",
      "2020-06-09 18:55:34,597 : INFO : PROGRESS: at sentence #80000, processed 422831 words, keeping 43048 word types\n",
      "2020-06-09 18:55:34,623 : INFO : PROGRESS: at sentence #90000, processed 476619 words, keeping 46442 word types\n",
      "2020-06-09 18:55:34,653 : INFO : PROGRESS: at sentence #100000, processed 529769 words, keeping 49508 word types\n",
      "2020-06-09 18:55:34,681 : INFO : PROGRESS: at sentence #110000, processed 583593 words, keeping 52883 word types\n",
      "2020-06-09 18:55:34,706 : INFO : PROGRESS: at sentence #120000, processed 636374 words, keeping 55810 word types\n",
      "2020-06-09 18:55:34,736 : INFO : PROGRESS: at sentence #130000, processed 688116 words, keeping 58010 word types\n",
      "2020-06-09 18:55:34,743 : INFO : collected 58375 word types from a corpus of 697543 raw words and 131853 sentences\n",
      "2020-06-09 18:55:34,745 : INFO : Loading a fresh vocabulary\n",
      "2020-06-09 18:55:34,804 : INFO : effective_min_count=10 retains 7476 unique words (12% of original 58375, drops 50899)\n",
      "2020-06-09 18:55:34,805 : INFO : effective_min_count=10 leaves 588500 word corpus (84% of original 697543, drops 109043)\n",
      "2020-06-09 18:55:34,851 : INFO : deleting the raw counts dictionary of 58375 items\n",
      "2020-06-09 18:55:34,853 : INFO : sample=6e-05 downsamples 866 most-common words\n",
      "2020-06-09 18:55:34,855 : INFO : downsampling leaves estimated 287702 word corpus (48.9% of prior 588500)\n",
      "2020-06-09 18:55:34,887 : INFO : estimated required memory for 7476 words and 200 dimensions: 15699600 bytes\n",
      "2020-06-09 18:55:34,888 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v.build_vocab(cleaned_content, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:55:36,775 : INFO : training model with 4 workers on 7476 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
      "2020-06-09 18:55:37,738 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:37,750 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:37,758 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:37,767 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:37,768 : INFO : EPOCH - 1 : training on 697543 raw words (287734 effective words) took 1.0s, 294248 effective words/s\n",
      "2020-06-09 18:55:38,769 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:38,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:38,785 : INFO : EPOCH 2 - PROGRESS: at 98.85% examples, 282916 words/s, in_qsize 1, out_qsize 1\n",
      "2020-06-09 18:55:38,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:38,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:38,806 : INFO : EPOCH - 2 : training on 697543 raw words (287147 effective words) took 1.0s, 280131 effective words/s\n",
      "2020-06-09 18:55:39,774 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:39,790 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:39,791 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:39,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:39,805 : INFO : EPOCH - 3 : training on 697543 raw words (287886 effective words) took 1.0s, 302225 effective words/s\n",
      "2020-06-09 18:55:40,739 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:40,760 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:40,767 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:40,769 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:40,771 : INFO : EPOCH - 4 : training on 697543 raw words (287653 effective words) took 1.0s, 302548 effective words/s\n",
      "2020-06-09 18:55:41,730 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:41,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:41,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:41,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:41,758 : INFO : EPOCH - 5 : training on 697543 raw words (287384 effective words) took 0.9s, 306976 effective words/s\n",
      "2020-06-09 18:55:42,680 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:42,697 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:42,700 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:42,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:42,714 : INFO : EPOCH - 6 : training on 697543 raw words (287352 effective words) took 0.9s, 303987 effective words/s\n",
      "2020-06-09 18:55:43,734 : INFO : EPOCH 7 - PROGRESS: at 87.24% examples, 249322 words/s, in_qsize 8, out_qsize 0\n",
      "2020-06-09 18:55:43,809 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:43,830 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:43,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:43,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:43,849 : INFO : EPOCH - 7 : training on 697543 raw words (287404 effective words) took 1.1s, 255993 effective words/s\n",
      "2020-06-09 18:55:44,793 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:44,807 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:44,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:44,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:44,823 : INFO : EPOCH - 8 : training on 697543 raw words (287840 effective words) took 1.0s, 301540 effective words/s\n",
      "2020-06-09 18:55:45,751 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:45,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:45,776 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:45,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:45,779 : INFO : EPOCH - 9 : training on 697543 raw words (287845 effective words) took 0.9s, 304880 effective words/s\n",
      "2020-06-09 18:55:46,721 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:46,743 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:46,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:46,763 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:46,764 : INFO : EPOCH - 10 : training on 697543 raw words (288024 effective words) took 1.0s, 296300 effective words/s\n",
      "2020-06-09 18:55:47,705 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:47,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:47,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:47,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:47,744 : INFO : EPOCH - 11 : training on 697543 raw words (287688 effective words) took 1.0s, 296530 effective words/s\n",
      "2020-06-09 18:55:48,752 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:48,754 : INFO : EPOCH 12 - PROGRESS: at 97.37% examples, 280276 words/s, in_qsize 2, out_qsize 1\n",
      "2020-06-09 18:55:48,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:48,766 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:48,767 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:48,769 : INFO : EPOCH - 12 : training on 697543 raw words (287399 effective words) took 1.0s, 283098 effective words/s\n",
      "2020-06-09 18:55:49,715 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:49,722 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:49,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:49,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:49,756 : INFO : EPOCH - 13 : training on 697543 raw words (287989 effective words) took 1.0s, 294596 effective words/s\n",
      "2020-06-09 18:55:50,697 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:50,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:50,727 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:50,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:50,734 : INFO : EPOCH - 14 : training on 697543 raw words (287406 effective words) took 1.0s, 298658 effective words/s\n",
      "2020-06-09 18:55:51,680 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:55:51,682 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:55:51,687 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:55:51,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:55:51,706 : INFO : EPOCH - 15 : training on 697543 raw words (287989 effective words) took 1.0s, 300500 effective words/s\n",
      "2020-06-09 18:55:51,707 : INFO : training on a 10463145 raw words (4314740 effective words) took 14.9s, 289030 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4314740, 10463145)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.train(cleaned_content, total_examples=simpsons_w2v.corpus_count, epochs=15, report_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:55:51,736 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v.init_sims(replace=True)\n",
    "# Dejamos de entrenar Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:55:51,762 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_FT = FastText(min_count=10,\n",
    "                      window=4,\n",
    "                      size=200,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:56:36,355 : INFO : collecting all words and their counts\n",
      "2020-06-09 18:56:36,394 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-09 18:56:36,473 : INFO : PROGRESS: at sentence #10000, processed 52242 words, keeping 10997 word types\n",
      "2020-06-09 18:56:36,508 : INFO : PROGRESS: at sentence #20000, processed 105199 words, keeping 17683 word types\n",
      "2020-06-09 18:56:36,554 : INFO : PROGRESS: at sentence #30000, processed 161865 words, keeping 23783 word types\n",
      "2020-06-09 18:56:36,610 : INFO : PROGRESS: at sentence #40000, processed 214884 words, keeping 28073 word types\n",
      "2020-06-09 18:56:36,656 : INFO : PROGRESS: at sentence #50000, processed 266154 words, keeping 32109 word types\n",
      "2020-06-09 18:56:36,696 : INFO : PROGRESS: at sentence #60000, processed 315182 words, keeping 35628 word types\n",
      "2020-06-09 18:56:36,739 : INFO : PROGRESS: at sentence #70000, processed 367905 words, keeping 39375 word types\n",
      "2020-06-09 18:56:36,782 : INFO : PROGRESS: at sentence #80000, processed 422831 words, keeping 43048 word types\n",
      "2020-06-09 18:56:36,831 : INFO : PROGRESS: at sentence #90000, processed 476619 words, keeping 46442 word types\n",
      "2020-06-09 18:56:36,871 : INFO : PROGRESS: at sentence #100000, processed 529769 words, keeping 49508 word types\n",
      "2020-06-09 18:56:36,914 : INFO : PROGRESS: at sentence #110000, processed 583593 words, keeping 52883 word types\n",
      "2020-06-09 18:56:36,957 : INFO : PROGRESS: at sentence #120000, processed 636374 words, keeping 55810 word types\n",
      "2020-06-09 18:56:36,994 : INFO : PROGRESS: at sentence #130000, processed 688116 words, keeping 58010 word types\n",
      "2020-06-09 18:56:37,003 : INFO : collected 58375 word types from a corpus of 697543 raw words and 131853 sentences\n",
      "2020-06-09 18:56:37,006 : INFO : Loading a fresh vocabulary\n",
      "2020-06-09 18:56:37,076 : INFO : effective_min_count=10 retains 7476 unique words (12% of original 58375, drops 50899)\n",
      "2020-06-09 18:56:37,077 : INFO : effective_min_count=10 leaves 588500 word corpus (84% of original 697543, drops 109043)\n",
      "2020-06-09 18:56:37,117 : INFO : deleting the raw counts dictionary of 58375 items\n",
      "2020-06-09 18:56:37,119 : INFO : sample=6e-05 downsamples 866 most-common words\n",
      "2020-06-09 18:56:37,122 : INFO : downsampling leaves estimated 287702 word corpus (48.9% of prior 588500)\n",
      "2020-06-09 18:56:37,261 : INFO : estimated required memory for 7476 words, 56279 buckets and 200 dimensions: 62169160 bytes\n",
      "2020-06-09 18:56:37,264 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_FT.build_vocab(cleaned_content, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:56:57,453 : INFO : training model with 4 workers on 7476 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
      "2020-06-09 18:56:58,558 : INFO : EPOCH 1 - PROGRESS: at 29.72% examples, 82249 words/s, in_qsize 6, out_qsize 3\n",
      "2020-06-09 18:57:00,064 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:00,069 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:00,136 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:00,145 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:00,146 : INFO : EPOCH - 1 : training on 697543 raw words (287962 effective words) took 2.6s, 109618 effective words/s\n",
      "2020-06-09 18:57:01,202 : INFO : EPOCH 2 - PROGRESS: at 41.55% examples, 113192 words/s, in_qsize 8, out_qsize 0\n",
      "2020-06-09 18:57:02,440 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:02,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:02,494 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:02,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:02,505 : INFO : EPOCH - 2 : training on 697543 raw words (288055 effective words) took 2.3s, 122743 effective words/s\n",
      "2020-06-09 18:57:03,553 : INFO : EPOCH 3 - PROGRESS: at 47.75% examples, 130938 words/s, in_qsize 7, out_qsize 0\n",
      "2020-06-09 18:57:04,759 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:04,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:04,826 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:04,839 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:04,840 : INFO : EPOCH - 3 : training on 697543 raw words (287588 effective words) took 2.3s, 124402 effective words/s\n",
      "2020-06-09 18:57:05,896 : INFO : EPOCH 4 - PROGRESS: at 44.62% examples, 120733 words/s, in_qsize 5, out_qsize 2\n",
      "2020-06-09 18:57:06,955 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:07,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:07,021 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:07,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:07,034 : INFO : EPOCH - 4 : training on 697543 raw words (287674 effective words) took 2.2s, 131764 effective words/s\n",
      "2020-06-09 18:57:08,114 : INFO : EPOCH 5 - PROGRESS: at 47.75% examples, 126837 words/s, in_qsize 7, out_qsize 0\n",
      "2020-06-09 18:57:09,150 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:09,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:09,203 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:09,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:09,230 : INFO : EPOCH - 5 : training on 697543 raw words (287496 effective words) took 2.2s, 132279 effective words/s\n",
      "2020-06-09 18:57:10,262 : INFO : EPOCH 6 - PROGRESS: at 32.54% examples, 92834 words/s, in_qsize 7, out_qsize 0\n",
      "2020-06-09 18:57:11,652 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:11,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:11,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:11,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:11,672 : INFO : EPOCH - 6 : training on 697543 raw words (287694 effective words) took 2.4s, 118898 effective words/s\n",
      "2020-06-09 18:57:12,706 : INFO : EPOCH 7 - PROGRESS: at 44.70% examples, 123584 words/s, in_qsize 8, out_qsize 1\n",
      "2020-06-09 18:57:13,794 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:13,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:13,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:13,825 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:13,826 : INFO : EPOCH - 7 : training on 697543 raw words (287874 effective words) took 2.1s, 134475 effective words/s\n",
      "2020-06-09 18:57:14,892 : INFO : EPOCH 8 - PROGRESS: at 47.75% examples, 127537 words/s, in_qsize 7, out_qsize 0\n",
      "2020-06-09 18:57:15,894 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:15,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:15,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:15,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:15,992 : INFO : EPOCH - 8 : training on 697543 raw words (287576 effective words) took 2.2s, 133581 effective words/s\n",
      "2020-06-09 18:57:17,013 : INFO : EPOCH 9 - PROGRESS: at 46.24% examples, 129836 words/s, in_qsize 8, out_qsize 0\n",
      "2020-06-09 18:57:18,033 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:18,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:18,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:18,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:18,118 : INFO : EPOCH - 9 : training on 697543 raw words (287445 effective words) took 2.1s, 136451 effective words/s\n",
      "2020-06-09 18:57:19,155 : INFO : EPOCH 10 - PROGRESS: at 47.71% examples, 134548 words/s, in_qsize 8, out_qsize 0\n",
      "2020-06-09 18:57:20,173 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:20,234 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:20,258 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:20,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:20,262 : INFO : EPOCH - 10 : training on 697543 raw words (288297 effective words) took 2.1s, 136824 effective words/s\n",
      "2020-06-09 18:57:21,295 : INFO : EPOCH 11 - PROGRESS: at 44.70% examples, 124095 words/s, in_qsize 7, out_qsize 0\n",
      "2020-06-09 18:57:22,376 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:22,451 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:22,458 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:22,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:22,460 : INFO : EPOCH - 11 : training on 697543 raw words (287697 effective words) took 2.2s, 131792 effective words/s\n",
      "2020-06-09 18:57:23,482 : INFO : EPOCH 12 - PROGRESS: at 44.70% examples, 124573 words/s, in_qsize 6, out_qsize 1\n",
      "2020-06-09 18:57:24,550 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:24,583 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:24,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:24,619 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:24,620 : INFO : EPOCH - 12 : training on 697543 raw words (287393 effective words) took 2.1s, 133775 effective words/s\n",
      "2020-06-09 18:57:25,647 : INFO : EPOCH 13 - PROGRESS: at 46.24% examples, 129534 words/s, in_qsize 7, out_qsize 0\n",
      "2020-06-09 18:57:26,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:26,749 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:26,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:26,797 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:26,798 : INFO : EPOCH - 13 : training on 697543 raw words (287656 effective words) took 2.2s, 133305 effective words/s\n",
      "2020-06-09 18:57:27,837 : INFO : EPOCH 14 - PROGRESS: at 46.24% examples, 126294 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:57:28,875 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:28,917 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:28,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:28,967 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:28,968 : INFO : EPOCH - 14 : training on 697543 raw words (287641 effective words) took 2.2s, 133086 effective words/s\n",
      "2020-06-09 18:57:30,007 : INFO : EPOCH 15 - PROGRESS: at 47.75% examples, 132387 words/s, in_qsize 7, out_qsize 0\n",
      "2020-06-09 18:57:31,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 18:57:31,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 18:57:31,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 18:57:31,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 18:57:31,078 : INFO : EPOCH - 15 : training on 697543 raw words (287883 effective words) took 2.1s, 138026 effective words/s\n",
      "2020-06-09 18:57:31,079 : INFO : training on a 10463145 raw words (4315931 effective words) took 33.6s, 128427 effective words/s\n"
     ]
    }
   ],
   "source": [
    "simpsons_FT.train(cleaned_content, total_examples=simpsons_w2v.corpus_count, epochs=15, report_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 18:57:31,517 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-06-09 18:57:31,528 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    }
   ],
   "source": [
    "# Finalizamos entrenando FastText\n",
    "simpsons_FT.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oos6kUF5tYJr"
   },
   "source": [
    "**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. Cúal es la diferencia entre ambos resultados? Por qué ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escogería uno vs el otro? (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wqX03jStYJr"
   },
   "source": [
    "**Respuesta**:\n",
    "\n",
    "Compararemos cada palabra con su respectivo Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lisas', 0.8793889284133911),\n",
       " ('Mom', 0.8765089511871338),\n",
       " ('Dad', 0.8709357976913452),\n",
       " ('Barts', 0.8547607064247131),\n",
       " ('Simpson', 0.8331567049026489),\n",
       " ('Bart', 0.829723596572876),\n",
       " ('homework', 0.8154399394989014),\n",
       " ('Homie', 0.8150808215141296),\n",
       " ('hope', 0.8108934760093689),\n",
       " ('age', 0.8105964064598083)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"Lisa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lisas', 0.959214985370636),\n",
       " ('Lis', 0.89859539270401),\n",
       " ('fault', 0.8012110590934753),\n",
       " ('Dads', 0.7965435981750488),\n",
       " ('girls', 0.777320384979248),\n",
       " ('grownup', 0.7684344053268433),\n",
       " ('adult', 0.7659843564033508),\n",
       " ('Lifeways', 0.7644272446632385),\n",
       " ('Adult', 0.762630045413971),\n",
       " ('grownups', 0.7606004476547241)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_FT.wv.most_similar(positive=[\"Lisa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una primera instancia podemos observar que w2v busca las palabras más cercanas en lo que es el vector de palabras. Por otra parte, FT, en este caso tiene pocas palabras coincidentes con w2v, y varias de las palabras parecen ser cercanas entre sí en términos de caracteres. Lisa, Lisas, Lis, indican lo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mom', 0.8941462635993958),\n",
       " ('Dad', 0.8727694749832153),\n",
       " ('Barts', 0.8678514957427979),\n",
       " ('Lis', 0.8489663600921631),\n",
       " ('dad', 0.8474310040473938),\n",
       " ('Moms', 0.8462323546409607),\n",
       " ('mom', 0.8337964415550232),\n",
       " ('Lisas', 0.8330223560333252),\n",
       " ('Lisa', 0.8297237157821655),\n",
       " ('homework', 0.8164154291152954)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"Bart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Barts', 0.893882155418396),\n",
       " ('Bartholomew', 0.7546021342277527),\n",
       " ('Bartdude', 0.7417711615562439),\n",
       " ('smart', 0.7299052476882935),\n",
       " ('Barbara', 0.6822119951248169),\n",
       " ('Barn', 0.6781874299049377),\n",
       " ('fart', 0.6675987243652344),\n",
       " ('Barneys', 0.6454269886016846),\n",
       " ('Mozart', 0.6368935704231262),\n",
       " ('Adult', 0.6320449113845825)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_FT.wv.most_similar(positive=[\"Bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de Bart, w2v lo asocia principalmente a dialogos que tiene con otras personas, Mom, dad, o acciones tipicas, como homework, lo que indica más contexto. Por otra parte, FT lo asocia principalmente a las variaciones que posee la palabra, Bart-s, Bart-holomew, sm-art, Bar-n, f-art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Simpson', 0.8876999616622925),\n",
       " ('Grampa', 0.8207604289054871),\n",
       " ('sounds', 0.8164547085762024),\n",
       " ('Homie', 0.8082134127616882),\n",
       " ('Maggie', 0.8056645393371582),\n",
       " ('Honey', 0.8012152314186096),\n",
       " ('Marge', 0.7997177839279175),\n",
       " ('realized', 0.7922268509864807),\n",
       " ('drinking', 0.7883081436157227),\n",
       " ('HERB', 0.7876418828964233)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"Homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customer', 0.7505385875701904),\n",
       " ('Customer', 0.7472519278526306),\n",
       " ('beer', 0.7286218404769897),\n",
       " ('bartender', 0.7088319063186646),\n",
       " ('reindeer', 0.6962839365005493),\n",
       " ('happier', 0.6891319155693054),\n",
       " ('sweeter', 0.6889042854309082),\n",
       " ('babysitter', 0.6872116327285767),\n",
       " ('farmer', 0.6795423030853271),\n",
       " ('hammer', 0.6792935729026794)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_FT.wv.most_similar(positive=[\"Homer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para Homer, w2v de partida le da más probabilidad a Simpson, porque siempre al inicio de los dialogos aparece Homer Simpson, sin embargo, eso no ocurre en el caso de Bart, lo cual es curioso.\n",
    "Luego, las siguientes palabras más cercanas son Honey, Homie, y Grampa, lo que indica que lo asocia a referirse a otros personajes.\n",
    "En su contraparte, FT, en este caso muestra más contexto respecto a las cosas y acciones a la que está relacionada Homer. Lo cual discrepa con lo expuesto en el caso de Bart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Homie', 0.9043211340904236),\n",
       " ('honey', 0.8638327121734619),\n",
       " ('glad', 0.8630102872848511),\n",
       " ('Simpson', 0.8626872301101685),\n",
       " ('marriage', 0.8572517037391663),\n",
       " ('sweetie', 0.8384672403335571),\n",
       " ('Honey', 0.8355017900466919),\n",
       " ('feel', 0.8354851603507996),\n",
       " ('husband', 0.8348455429077148),\n",
       " ('guess', 0.8274424076080322)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"Marge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marges', 0.9029541611671448),\n",
       " ('Marriage', 0.8588688373565674),\n",
       " ('Marbles', 0.7858073711395264),\n",
       " ('marriage', 0.7624776363372803),\n",
       " ('Marlowe', 0.746626615524292),\n",
       " ('Marjorie', 0.7430392503738403),\n",
       " ('Marco', 0.7326650023460388),\n",
       " ('Margarine', 0.7267853021621704),\n",
       " ('Maggie', 0.7209067940711975),\n",
       " ('Maggies', 0.7065613269805908)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_FT.wv.most_similar(positive=[\"Marge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de Marge se observa algo similar que con Bart, w2v asocia al token con acciones o personas o caracteristicas, y FT la asocia a variaciones de la palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Liisa' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2107305043f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimpsons_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Liisa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Liisa' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"Liisa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lisa', 0.7327530384063721),\n",
       " ('Lisas', 0.7055224180221558),\n",
       " ('Thomasina', 0.6392861604690552),\n",
       " ('equals', 0.638843297958374),\n",
       " ('fault', 0.6362926363945007),\n",
       " ('subway', 0.634484589099884),\n",
       " ('wisdom', 0.6324256062507629),\n",
       " ('ads', 0.6275869011878967),\n",
       " ('suddenly', 0.6269004344940186),\n",
       " ('odds', 0.6254395842552185)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_FT.wv.most_similar(positive=[\"Liisa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liisa es una palabra inexistente en el vocabulario, lo que indica que w2v no puede reconocer palabras externas al corpus, y FT puede generalizar utilizando los caracteres y encontrando las variaciones de este. Por lo anterior, se podria decir con cierto sesgo que w2v funciona mejor para otorgar contexto en un conjunto definido de tokens que se encuentran en el corpus mientras que FT generaliza mejor en la busqueda de una palabra similar, pero no otorga tanto contexto como lo hace w2v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAbldwNFtYJu"
   },
   "source": [
    "### Parte 4: Aplicar embeddings para clasificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5DlKSestYJu"
   },
   "source": [
    "Ahora utilizaremos estos embeddings para clasificar palabras basadas en su polaridad (positivas o negativas). Para esto ocuparemos el lexicón AFINN incluido en la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCDFYm_ytYJu"
   },
   "outputs": [],
   "source": [
    "AFINN = 'AFINN_full.csv'\n",
    "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>refuse</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>glory</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>adorable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>loom</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>resolution</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>ranter</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>misfortune</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>sullen</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>cheery</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>corrupted</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0  1\n",
       "2466      refuse -1\n",
       "2976       glory  1\n",
       "1005    adorable  1\n",
       "692         loom -1\n",
       "1319  resolution  1\n",
       "470       ranter -1\n",
       "629   misfortune -1\n",
       "2560      sullen -1\n",
       "97        cheery  1\n",
       "2452   corrupted -1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_afinn.sample(10) # Visualizamos como estan escritas clasificadas algunas palabras de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-icASpvytYJw"
   },
   "source": [
    "Hint: Para w2v son esperables KeyErrors, para eso pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar), para luego aplicar los embeddings en toda la columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8d8qtIgtYJy"
   },
   "source": [
    "**Pregunta 1**: Una vez que tengan un dataframe del estilo [embedding, sentimiento] para ambos modelos, separarlo utilizando la siguiente función, donde X es su columna de embeddings e y es la columna de los valores. (3 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_E_array(af, model):\n",
    "    doc_embeddings = []\n",
    "    for token in af[0]:\n",
    "        if token in model.wv.vocab:\n",
    "            doc_embedding = np.array(model.wv[token])\n",
    "            doc_embeddings.append(doc_embedding)\n",
    "        else: \n",
    "            try: \n",
    "                doc_embedding = np.array(model.wv[token])\n",
    "                doc_embeddings.append(doc_embedding)\n",
    "            except KeyError:\n",
    "                doc_embeddings.append(np.zeros(model.wv.vector_size))\n",
    "    return np.array(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emb_w2v = pd.DataFrame(create_E_array(df_afinn, simpsons_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.013957</td>\n",
       "      <td>-0.009268</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.148234</td>\n",
       "      <td>-0.051251</td>\n",
       "      <td>-0.031032</td>\n",
       "      <td>-0.020761</td>\n",
       "      <td>0.132998</td>\n",
       "      <td>0.024785</td>\n",
       "      <td>0.045445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009537</td>\n",
       "      <td>0.077303</td>\n",
       "      <td>-0.103042</td>\n",
       "      <td>0.035528</td>\n",
       "      <td>0.103283</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>0.051297</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>-0.049812</td>\n",
       "      <td>0.143792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>-0.001683</td>\n",
       "      <td>-0.031512</td>\n",
       "      <td>0.030853</td>\n",
       "      <td>0.111482</td>\n",
       "      <td>-0.079600</td>\n",
       "      <td>-0.023971</td>\n",
       "      <td>-0.001491</td>\n",
       "      <td>0.128574</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>0.065458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020276</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>-0.118502</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>0.079020</td>\n",
       "      <td>0.024174</td>\n",
       "      <td>0.060604</td>\n",
       "      <td>0.025250</td>\n",
       "      <td>-0.044383</td>\n",
       "      <td>0.137037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3382 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.013957 -0.009268  0.040161  0.148234 -0.051251 -0.031032 -0.020761   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3377  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3378 -0.001683 -0.031512  0.030853  0.111482 -0.079600 -0.023971 -0.001491   \n",
       "3379  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3380  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3381  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           7         8         9    ...       190       191       192  \\\n",
       "0     0.132998  0.024785  0.045445  ... -0.009537  0.077303 -0.103042   \n",
       "1     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3377  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3378  0.128574  0.012997  0.065458  ... -0.020276  0.043397 -0.118502   \n",
       "3379  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3380  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3381  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "           193       194       195       196       197       198       199  \n",
       "0     0.035528  0.103283  0.006075  0.051297  0.031431 -0.049812  0.143792  \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3377  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3378  0.006143  0.079020  0.024174  0.060604  0.025250 -0.044383  0.137037  \n",
       "3379  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3380  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3381  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[3382 rows x 200 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emb_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emb_FT = pd.DataFrame(create_E_array(df_afinn, simpsons_FT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093947</td>\n",
       "      <td>0.171375</td>\n",
       "      <td>0.059314</td>\n",
       "      <td>0.034651</td>\n",
       "      <td>-0.108250</td>\n",
       "      <td>0.128284</td>\n",
       "      <td>-0.015587</td>\n",
       "      <td>0.102042</td>\n",
       "      <td>-0.188611</td>\n",
       "      <td>0.101413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064615</td>\n",
       "      <td>-0.029970</td>\n",
       "      <td>0.010281</td>\n",
       "      <td>0.140455</td>\n",
       "      <td>-0.049327</td>\n",
       "      <td>0.048961</td>\n",
       "      <td>0.029860</td>\n",
       "      <td>-0.162166</td>\n",
       "      <td>-0.009611</td>\n",
       "      <td>-0.005940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.015569</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>-0.024428</td>\n",
       "      <td>-0.041312</td>\n",
       "      <td>0.047462</td>\n",
       "      <td>-0.001430</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>-0.023307</td>\n",
       "      <td>0.029131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.036343</td>\n",
       "      <td>-0.046236</td>\n",
       "      <td>0.038289</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>-0.043448</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.003297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087908</td>\n",
       "      <td>0.069316</td>\n",
       "      <td>-0.043301</td>\n",
       "      <td>-0.075194</td>\n",
       "      <td>-0.091728</td>\n",
       "      <td>0.054214</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>-0.064718</td>\n",
       "      <td>-0.104128</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011381</td>\n",
       "      <td>-0.022925</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.088560</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.016239</td>\n",
       "      <td>0.053263</td>\n",
       "      <td>-0.087721</td>\n",
       "      <td>0.059161</td>\n",
       "      <td>-0.029186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.039382</td>\n",
       "      <td>-0.020032</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>-0.026689</td>\n",
       "      <td>0.043453</td>\n",
       "      <td>0.009043</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>-0.014738</td>\n",
       "      <td>0.015387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>0.008437</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>-0.025665</td>\n",
       "      <td>0.026459</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>-0.027686</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>0.002854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048480</td>\n",
       "      <td>0.095455</td>\n",
       "      <td>-0.013646</td>\n",
       "      <td>0.009087</td>\n",
       "      <td>-0.052692</td>\n",
       "      <td>0.056348</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.030795</td>\n",
       "      <td>-0.035868</td>\n",
       "      <td>0.011293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013692</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>0.021325</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.026462</td>\n",
       "      <td>0.021857</td>\n",
       "      <td>-0.055610</td>\n",
       "      <td>-0.013071</td>\n",
       "      <td>0.018997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.063305</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0.012923</td>\n",
       "      <td>-0.041803</td>\n",
       "      <td>0.029519</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016816</td>\n",
       "      <td>0.008585</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>-0.008078</td>\n",
       "      <td>-0.006308</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>-0.037248</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>0.004822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>0.070521</td>\n",
       "      <td>0.119743</td>\n",
       "      <td>0.058447</td>\n",
       "      <td>-0.035089</td>\n",
       "      <td>-0.104484</td>\n",
       "      <td>0.058912</td>\n",
       "      <td>-0.061795</td>\n",
       "      <td>0.132333</td>\n",
       "      <td>-0.156985</td>\n",
       "      <td>0.031258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060591</td>\n",
       "      <td>-0.048079</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>-0.048934</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>0.180756</td>\n",
       "      <td>-0.202525</td>\n",
       "      <td>-0.012638</td>\n",
       "      <td>-0.046520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>0.044325</td>\n",
       "      <td>0.033758</td>\n",
       "      <td>-0.020595</td>\n",
       "      <td>0.021001</td>\n",
       "      <td>-0.049363</td>\n",
       "      <td>0.026758</td>\n",
       "      <td>-0.042578</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>-0.050447</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013753</td>\n",
       "      <td>-0.007783</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>-0.027747</td>\n",
       "      <td>0.020374</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>-0.034541</td>\n",
       "      <td>-0.034446</td>\n",
       "      <td>0.028150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>0.008692</td>\n",
       "      <td>0.072673</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>-0.048866</td>\n",
       "      <td>0.077361</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.028484</td>\n",
       "      <td>-0.045707</td>\n",
       "      <td>0.029233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032462</td>\n",
       "      <td>-0.026527</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.045988</td>\n",
       "      <td>-0.040410</td>\n",
       "      <td>0.026837</td>\n",
       "      <td>0.023180</td>\n",
       "      <td>-0.093692</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.010183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>0.011233</td>\n",
       "      <td>0.036489</td>\n",
       "      <td>-0.008838</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>-0.050047</td>\n",
       "      <td>0.051371</td>\n",
       "      <td>0.020843</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>-0.045950</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013528</td>\n",
       "      <td>-0.026065</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.049156</td>\n",
       "      <td>-0.028779</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0.014191</td>\n",
       "      <td>-0.057451</td>\n",
       "      <td>0.016897</td>\n",
       "      <td>-0.016573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3382 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.093947  0.171375  0.059314  0.034651 -0.108250  0.128284 -0.015587   \n",
       "1     0.015198  0.015569  0.006683 -0.024428 -0.041312  0.047462 -0.001430   \n",
       "2     0.087908  0.069316 -0.043301 -0.075194 -0.091728  0.054214  0.005978   \n",
       "3     0.010106  0.039382 -0.020032  0.005292 -0.026689  0.043453  0.009043   \n",
       "4     0.048480  0.095455 -0.013646  0.009087 -0.052692  0.056348  0.000153   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3377  0.015618  0.063305 -0.000216  0.012923 -0.041803  0.029519  0.002636   \n",
       "3378  0.070521  0.119743  0.058447 -0.035089 -0.104484  0.058912 -0.061795   \n",
       "3379  0.044325  0.033758 -0.020595  0.021001 -0.049363  0.026758 -0.042578   \n",
       "3380  0.008692  0.072673  0.008956  0.005345 -0.048866  0.077361  0.006187   \n",
       "3381  0.011233  0.036489 -0.008838  0.007369 -0.050047  0.051371  0.020843   \n",
       "\n",
       "           7         8         9    ...       190       191       192  \\\n",
       "0     0.102042 -0.188611  0.101413  ... -0.064615 -0.029970  0.010281   \n",
       "1     0.025797 -0.023307  0.029131  ...  0.003688 -0.013403  0.004108   \n",
       "2    -0.064718 -0.104128  0.034180  ... -0.011381 -0.022925  0.002057   \n",
       "3     0.000963 -0.014738  0.015387  ...  0.012205  0.008437  0.006898   \n",
       "4     0.030795 -0.035868  0.011293  ...  0.013692  0.003157  0.007872   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3377  0.012076  0.000272  0.011350  ...  0.016816  0.008585  0.007823   \n",
       "3378  0.132333 -0.156985  0.031258  ... -0.060591 -0.048079  0.056600   \n",
       "3379  0.019571 -0.050447 -0.015869  ...  0.013753 -0.007783  0.008845   \n",
       "3380  0.028484 -0.045707  0.029233  ...  0.032462 -0.026527  0.019343   \n",
       "3381  0.016613 -0.045950  0.038571  ... -0.013528 -0.026065  0.001310   \n",
       "\n",
       "           193       194       195       196       197       198       199  \n",
       "0     0.140455 -0.049327  0.048961  0.029860 -0.162166 -0.009611 -0.005940  \n",
       "1     0.036343 -0.046236  0.038289  0.015433 -0.043448  0.005752  0.003297  \n",
       "2     0.088560 -0.003368  0.016239  0.053263 -0.087721  0.059161 -0.029186  \n",
       "3     0.022677 -0.025665  0.026459  0.006344 -0.027686 -0.019531  0.002854  \n",
       "4     0.021325  0.000549  0.026462  0.021857 -0.055610 -0.013071  0.018997  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3377  0.030300 -0.008078 -0.006308  0.019287 -0.037248  0.004346  0.004822  \n",
       "3378  0.130000 -0.048934  0.037935  0.180756 -0.202525 -0.012638 -0.046520  \n",
       "3379  0.019297 -0.027747  0.020374  0.006581 -0.034541 -0.034446  0.028150  \n",
       "3380  0.045988 -0.040410  0.026837  0.023180 -0.093692  0.002916  0.010183  \n",
       "3381  0.049156 -0.028779  0.059160  0.014191 -0.057451  0.016897 -0.016573  \n",
       "\n",
       "[3382 rows x 200 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emb_FT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u33__LaNtYJ3"
   },
   "source": [
    "**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? (3 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_frb5aDatYJz"
   },
   "outputs": [],
   "source": [
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(Emb_w2v, df_afinn[1], random_state=0, test_size=0.1, stratify=df_afinn[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "clf1 = LogisticRegression(max_iter=1000000)\n",
    "clf2 = LogisticRegression(max_iter=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_w2v = Pipeline([('clf', clf1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_w2v.fit(X_train_w2v,y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_FT, X_test_FT, y_train_FT, y_test_FT = train_test_split(Emb_FT, df_afinn[1], random_state=0, test_size=0.1, stratify=df_afinn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_FT = Pipeline([('clf', clf2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_FT.fit(X_train_FT,y_train_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_w2v = pipeline_w2v.predict(X_test_w2v)\n",
    "y_pred_FT = pipeline_FT.predict(X_test_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados Word2Vec\n",
      "[[218   3]\n",
      " [102  16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.68      0.99      0.81       221\n",
      "           1       0.84      0.14      0.23       118\n",
      "\n",
      "    accuracy                           0.69       339\n",
      "   macro avg       0.76      0.56      0.52       339\n",
      "weighted avg       0.74      0.69      0.61       339\n",
      "\n",
      "Resultados FastText\n",
      "[[219   2]\n",
      " [101  17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.68      0.99      0.81       221\n",
      "           1       0.89      0.14      0.25       118\n",
      "\n",
      "    accuracy                           0.70       339\n",
      "   macro avg       0.79      0.57      0.53       339\n",
      "weighted avg       0.76      0.70      0.61       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Resultados Word2Vec')\n",
    "conf_matrix_w2v = confusion_matrix(y_test_w2v, y_pred_w2v)\n",
    "print(conf_matrix_w2v)\n",
    "print(classification_report(y_test_w2v, y_pred_w2v))\n",
    "print('Resultados FastText')\n",
    "conf_matrix_FT = confusion_matrix(y_test_FT, y_pred_FT)\n",
    "print(conf_matrix_FT)\n",
    "print(classification_report(y_test_FT, y_pred_FT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dP5zYH3qtYJ3"
   },
   "source": [
    "**Respuesta**: Estos resultados se obtienen porque el conjunto de entrenamiento es el dialogo de Los Simpsons, el cual posee 54000 palabras, pero de las cuales, solo se tiene en cuenta alrededor de 7000 de ellas para entrenar el Embedding, no solo eso, tienen un sesgo considerable a elementos de la misma serie. \n",
    "\n",
    "Por lo tanto, como es posible ver en el DataFrame de w2v, la mayor parte del conjunto AFINN no coincidia con el vocabulario del Embedding, por lo que poseian un vector cero. Eso establece que la cantidad de palabras con las que se entreno cada Embedding es muy baja y un dataset mas grande sería beneficioso a la hora de clasificar el AFINN.\n",
    "\n",
    "Por otra parte, el dataset de AFINN esta ligeramente desbalanceado, por lo que el clasificador decidió elegir mas veces -1.\n",
    "Como resultados preliminares, puesto que esto es una muestra del conjunto, se podría decir que FastText hace un mejor trabajo que Word2Vec, principalmente porque asocia palabras similares a traves de bi y trigramas, por lo que el DataFrame no tiene filas nulas, como ocurre con Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOt6N4nftYJ4"
   },
   "source": [
    "# Bonus: 2 puntos en cualquier pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnsV0J6StYJ4"
   },
   "source": [
    "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset más grande y obtener mejores resultados. Les puede servir [ésta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim (1 punto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lYudsa_tYJ4"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd4psA7LtYJ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrZxZZfutYJ7"
   },
   "source": [
    "**Pregunta 2**: Utilizar wefe para ver si el modelo w2v entrenado con los dialogos de los Simpson tienen algun bias entre los personajes hombres y la cerveza (1 punto):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Subk47EatYJ7"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgThotyEtYJ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Minitarea3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
