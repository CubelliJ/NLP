{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LMgKjfYC_Go-",
        "BehSou6rCvwg",
        "uMud7YGMBZvg",
        "xCKTJOdgC5eC",
        "2l05KYy5FSUy",
        "qrYvF3X0sjWL",
        "y4wPiydnaSGs"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# Tarea 2 - Named Entity Recognition\n",
        "\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T15:21:15.416464Z",
          "start_time": "2020-06-22T15:21:15.411478Z"
        },
        "colab_type": "text",
        "id": "X3QUWWoWaSE3"
      },
      "source": [
        "- **Nombre: Joaquin Cubelli - Tomás de la Sotta**\n",
        "\n",
        "- **Usuario o nombre de equipo en Codalab: Team NSOUBN** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W_dxGEs3iiau"
      },
      "source": [
        "\n",
        "## Introducción a la tarea\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "\n",
        "El objetivo de esta tarea es resolver una de las tasks mas importantes de Sequence Labelling: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf). \n",
        "\n",
        "En particular, deberán participar, al igual que en la tarea anterior, en una competencia en donde deberán crear distintos modelos que apunten a resolver NER en español. Para esto, les proveeremos un dataset de NER de noticias etiquetadas en español mas este baseline en donde podrán comenzar a trabajar. \n",
        "\n",
        "Esperamos que (por lo menos) utilizen Redes Neuronales Recurrentes (RNN) para resolverla. Nuevamente, hay total libertad para utilizar software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados (como el caso de spacy).\n",
        "\n",
        "\n",
        "**¿Qué es Sequence Labelling?** \n",
        "\n",
        "En breves palabras, dada una secuencia de tokens (frase u oración) sequence labelling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia.\n",
        "\n",
        "**Named Entity Recognition (NER)**\n",
        "\n",
        "Esta tarea consiste en localizar y clasificar los tokens de una oración que representen entidades nombradas. Es decir, tokens que simbolicen (1) **personas**, (2) **organizaciones**, (3) **lugares** y (4) **adjetivos, eventos y otras entidades que no entren en las categorías anteriores** deberán ser taggeados como (1) **PER**, (2) **ORG**, (3) **LOC** y (4) **MISC** respectivamente. Adicionalmente, dado que existen entidades representadas en más de un token (como La Serena), se utiliza la notación BIO como prefijo al tag: Beginning, Inside, Outside. Es decir, si encuentro una entidad, el primer token etiquetado será precedido por B, el segundo por I y los n restantes por I. Por otra parte, si el token no representa ninguna entidad nombrada, se representa por O. Un ejemplo de esto es:\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "```\n",
        "Felipe B-PER\n",
        "Bravo I-PER\n",
        "es O\n",
        "el O\n",
        "profesor O\n",
        "de O\n",
        "PLN B-MISC\n",
        "de O\n",
        "la O\n",
        "Universidad B-ORG\n",
        "de I-ORG\n",
        "Chile I-ORG\n",
        ". O\n",
        "```\n",
        "\n",
        "Estos links son los más indicados para comenzar:\n",
        "\n",
        "-  [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf) | [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)\n",
        "\n",
        "\n",
        "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWlfabmkaSE7"
      },
      "source": [
        "### Reglas de la tarea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T15:33:49.808401Z",
          "start_time": "2020-06-22T15:33:49.798428Z"
        },
        "colab_type": "text",
        "id": "3w9Dw4CSaSE8"
      },
      "source": [
        "Algunos detalles de la competencia:\n",
        "\n",
        "- Para que su tarea sea evaluada, deben participar en la competencia como también, enviar este notebook con su informe.\n",
        "- Para participar, deben registrarse en la competencia en Codalab en grupos de máximo 2 alumnos. Cada grupo debe tener un nombre de equipo. (¡Y deben reportarlo en su informe!)\n",
        "- Las métricas usadas serán Precisión, Recall y F1.\n",
        "- En esta tarea se recomienda usar GPU. Pueden ejecutar su tarea en colab (lo cual trae todo instalado) o pueden intentar correrlo en su computador. en este caso, deberá ser compatible con cuda y deberán instalar todo por su cuenta.\n",
        "- En total pueden hacer un **máximo de 4 envíos**.\n",
        "- Por favor, todas sus dudas haganlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente serán remitidos a ese medio. Recuerden el ánimo colaborativo del curso!!\n",
        "- Estar top 5 en alguna métrica equivale a 1 punto extra en la nota final.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5BBxJWQaSE-"
      },
      "source": [
        "**Link a la competencia:  https://competitions.codalab.org/competitions/25302?secret_key=690406c7-b3b0-4092-8694-d08d7991ca94**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9spX-Hkh8YJg"
      },
      "source": [
        "### Modelos\n",
        "\n",
        "La RNN del baseline adjunto a este notebook está programado en [`pytorch`](https://pytorch.org/) y contiene:\n",
        "\n",
        "- La carga los datasets, creación de batches de texto y padding. En resumen, carga los datos y los deja listo para entrenar la red.\n",
        "- La implementación básica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad. \n",
        "- La construcción un output para que lo puedan probar en la tarea en codelab.\n",
        "\n",
        "\n",
        "\n",
        "roponer algunos experimentos a hacer:\n",
        "(cambiar el batch size, dimensiones de las capas, cambiar el tipo de\n",
        "RNN, cambiar el optimizer, usar una CRF loss, usar embeddings\n",
        "pre-entrenados, usar BERT??). Quizás podemos sugerir usar algo como\n",
        "https://github.com/flairNLP/flair\n",
        "\n",
        "Se espera que ustedes experimenten con el baseline utilizando (pero no limitándose) estas sugerencias:\n",
        "\n",
        "*   Probar Early stopping\n",
        "*   Variar la cantidad de parámetros de la capa de embeddings.\n",
        "*   Variar la cantidad de capas RNN.\n",
        "*   Variar la cantidad de parámetros de las capas de RNN.\n",
        "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...).[Guía breve aquí](https://github.com/dccuchile/spanish-word-embeddings), [Embeddings en español aquí](https://github.com/dccuchile/spanish-word-embeddings).\n",
        "*   Variar la cantidad de épocas de entrenamiento.\n",
        "*   Variar el optimizador, learning rate, batch size, usar CRF loss, etc...\n",
        "*   Probar bi-direccionalidad.\n",
        "*   Probar teacher forcing.\n",
        "*   Incluir dropout.\n",
        "*   Probar modelos de tipo GRU\n",
        "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
        "*   Probar modelos de transformers en español usando [Huggingface](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4HfZqQ-_aSFE"
      },
      "source": [
        "### Reporte\n",
        "\n",
        "Este debe cumplir la siguiente estructura:\n",
        "\n",
        "1.\t**Introducción**: Presentar brevemente el problema a resolver, los modelos utilizados en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
        "\n",
        "2.\t**Modelos**: Describir brevemente los modelos, métodos y hiperparámetros utilizados. (1.0 puntos)\n",
        "\n",
        "4.\t**Métricas de evaluación**: Describir las métricas utilizadas en la evaluación indicando que miden y cuál es su interpretación en este problema en particular. (0.5 puntos)\n",
        "\n",
        "5.\t**Experimentos**: Reportar todos sus experimentos y código en esta sección. Comparar los resultados obtenidos utilizando diferentes modelos. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (3.0 puntos)\n",
        "\n",
        "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1.0 punto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGM-hn87aSFF"
      },
      "source": [
        "(Pueden eliminar cualquier celda con instrucciones...)\n",
        "\n",
        "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO serán evaluados Notebooks sin nombre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f7X2FruyaSFG"
      },
      "source": [
        "\n",
        "-----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Dentro de los estudios de gran interés en el área de comunicación y comprensión del lenguaje, ser capaz de reconocer entidades es vital para la clasificación. El procesamiento de lenguaje natural tiene tres áreas centrales: \"semantics\", \"syntaxis\" y \"speech\", de esto nace la idea del NER, que busca resolver el problema de reconocimiento, tomando un lugar en el área de \"semantics\", extendiéndose hacia lo que es la interpretabilidad contextual de los documentos.\n",
        "\n",
        "El siguiente informe desarrolla distintos experimentos para determinar técnicas útiles para hacer este procedimiento. Dentro del consiguiente, se muestra el estudio de 10 modelos diferentes, desde el baseline hasta BERT, pasando por variados modelos de RNN y regresiones, los cuales se tratarán con mayor detalle en la sección definida para ello.\n",
        "\n",
        "Haciendo un acercamiento hacia el final de la investigación, se concluye que, con los parámetros utilizados y el dataset predefinido, se obtiene un mejor resultado con modelos LSTM por sobre GRU, Redes Bidireccionales sobre Unidireccionales, Profundidad de las RNN con resultados inconsistentes y la regresión logística muy por debajo de todos los modelos evaluados. \n",
        "Con respecto a los optimizadores utilizados para las RNN, todos se comportan similarmente, sin embargo destacan Adam y RMSprop. \n",
        "\n",
        "Finalmente se menciona que no todos los modelos desarrollados presentan el funcionamiento esperado y algunos no fueron completamente implementables durante el desarrollo de la investigación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## Modelos \n",
        "En esta tarea, se definieron 4 clases de modelos principales:\n",
        "\n",
        "- Regresión Logistica\n",
        "- RNN-LTSM\n",
        "- RNN-GRU\n",
        "- Bert\n",
        "\n",
        "De los cuales se iteró modificando en una primera instancia los Métodos n_layers y bidireccional, dando origen a los siguientes submodelos:\n",
        "\n",
        "- biLTSM \n",
        "- biGRU\n",
        "- deepLTSM\n",
        "- deepGRU\n",
        "\n",
        "Hiperparámetros a considerar:\n",
        "\n",
        "Para la Regresión Logistica: Embedding_dim # Dimensiones de embeddings\n",
        "\n",
        "Para las RNN: \n",
        "\n",
        "- Embedding_dim # Dimensiones de embeddings.\n",
        "- hidden_dim # Dimensiones de la capa oculta.\n",
        "- n_layers # Cantidad de capas tanto de LTSM como de GRU.\n",
        "- Dropout: Proporcion de neuronas cuyos pesos son llevadas a cero para evitar overfitting.\n",
        "\n",
        "Para todos los modelos a entrenar, también se iteran los hiperparámetros Batch_size y n_epochs, que corresponden a la cantidad de veces que el train_dataset utiliza en la red para entrenar por época, y la cantidad de épocas en las que se entrena respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## Métricas de evaluación\n",
        "\n",
        "- **Precision:** La precision es la razon entre las observaciones correctamente predecidas con el total de las observaciones predictas positivas. Una precision alta se relaciona con pocos falsos positivos. Precision = VP / (VP + FP). Dado un precision de 0.3, por ejemplo, esto significa que el 30% de las veces que prediga positivamente, estará en lo correcto y 70% de las veces, no.\n",
        "\n",
        "- **Recall:** Recall es la razon entre observaciones correctamente clasificadas como positivas respecto al total de las observaciones en la clase actual. Recall = VP / (VP + FN). El recall o exhaustividad nos informa de la cantidad que es capaz el modelo de identificar. Utilizando un ejemplo similar, con un recall de 0.25, el modelo sería capaz de identificar solo un 25% de los verdaderos positivos.\n",
        "\n",
        "- **F1 score:** El score F1 es la media ponderada entre Precision y Recall. Bajo la idea de que la presición y la exhaustividad tienen igual importancia dentro del modelo, el score F1 es una medida que combina ambas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T15:44:52.175773Z",
          "start_time": "2020-06-22T15:44:52.172782Z"
        },
        "colab_type": "text",
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## Experimentos\n",
        "\n",
        "\n",
        "El código que les entregaremos servirá de baseline para luego implementar mejores modelos. \n",
        "En general, el código asociado a la carga de los datos, las funciones de entrenamiento, de evaluación y la predicción de los datos de la competencia no deberían cambiar. \n",
        "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperparámetros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  Carga de datos y Preprocesamiento\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librería [`torchtext`](https://github.com/pytorch/text).\n",
        "En particular usaremos su módulo `data`, el cual según su documentación original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso será el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:24:52.392908Z",
          "start_time": "2020-06-23T22:24:50.641641Z"
        },
        "colab_type": "code",
        "id": "27csY87GaSFO",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "b527ae14-a43f-4fc4-c979-07f07e42bbb6"
      },
      "source": [
        "# Instalar torchtext (en codalab) - Descomentar.\n",
        "!pip3 install --upgrade torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 7.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2gFwxudx75k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip freeze | grep torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMkfXGWEx9zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall --yes torch torchvision && pip install torch torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:24:53.086354Z",
          "start_time": "2020-06-23T22:24:52.394902Z"
        },
        "colab_type": "code",
        "id": "ng7wRGEyawjM",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad \n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-V0w-5X0Ptk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9bd9b84-e66d-496b-a1fb-ee26ab603801"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### Obtener datos\n",
        "\n",
        "Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:49.789218Z",
          "start_time": "2020-06-23T22:24:53.088871Z"
        },
        "colab_type": "code",
        "id": "lbT0g_kC18Jb",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/Data/train_NER_esp.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/Data/val_NER_esp.txt -nc    # Dataset de Validación (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/Data/test_NER_esp.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¡¡SON LOS QUE DEBEN SER PREDICHOS!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  Fields\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros parámetros relacionados con la forma en que se debe numericalizar un tipo de datos, como un método de tokenización y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "Abogado B-PER\n",
        "General I-PER\n",
        "del I-PER\n",
        "Estado I-PER\n",
        ", O\n",
        "Daryl B-PER\n",
        "Williams I-PER\n",
        "```\n",
        "\n",
        "Cada linea contiene una palabra y su clase. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y los NER_TAGS (`clase`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:49.795126Z",
          "start_time": "2020-06-23T22:25:49.791108Z"
        },
        "colab_type": "code",
        "id": "3DcM_IjgCdzz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "45f1fcef-02c8-4033-c237-a1076ffe2504"
      },
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = data.Field(unk_token=None)\n",
        "\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  SequenceTaggingDataset\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext diseñada para contener datasets de sequence labelling. \n",
        "Los ejemplos que se guarden en una instancia de estos serán arreglos de palabras pareados con sus respectivos tags.\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estará pareado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase cómo cargar los datasets de prueba, validación y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.294370Z",
          "start_time": "2020-06-23T22:25:49.797092Z"
        },
        "colab_type": "code",
        "id": "HsHdGml62J21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5dfd0fb8-721b-4f37-f086-655c208db68f"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train_NER_esp.txt\",\n",
        "    validation=\"val_NER_esp.txt\",\n",
        "    test=\"test_NER_esp.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"iso-8859-1\",\n",
        "    separator=\" \"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.301354Z",
          "start_time": "2020-06-23T22:25:50.296368Z"
        },
        "colab_type": "code",
        "id": "Hu7q3HCliia5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "662391a1-ffa9-4fc8-92cb-cbedc7752d26"
      },
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Número de ejemplos de validación: {len(valid_data)}\")\n",
        "print(f\"Número de ejemplos de test (competencia): {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero de ejemplos de entrenamiento: 8323\n",
            "Número de ejemplos de validación: 1915\n",
            "Número de ejemplos de test (competencia): 1517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.317313Z",
          "start_time": "2020-06-23T22:25:50.303361Z"
        },
        "colab_type": "code",
        "id": "T023Ld4RaSF4",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "32b14424-661d-4c95-e8c3-932f90cb8757"
      },
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Cáceres', 'B-LOC'),\n",
              " (',', 'O'),\n",
              " ('23', 'O'),\n",
              " ('may', 'O'),\n",
              " ('(', 'O'),\n",
              " ('EFE', 'B-ORG'),\n",
              " (')', 'O'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### Construir los vocabularios para el texto y las etiquetas\n",
        "\n",
        "Los vocabularios son los obbjetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields.\n",
        "El siguiente paso consiste en construirlos. Para esto, hacemos uso del método `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.445968Z",
          "start_time": "2020-06-23T22:25:50.320305Z"
        },
        "colab_type": "code",
        "id": "PBhp7WICiibL",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.453960Z",
          "start_time": "2020-06-23T22:25:50.448987Z"
        },
        "colab_type": "code",
        "id": "M4OgUKM_iibO",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "401a016c-ce60-4da0-c7bc-6c1a01857f6d"
      },
      "source": [
        "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens únicos en TEXT: 26101\n",
            "Tokens únicos en NER_TAGS: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.460965Z",
          "start_time": "2020-06-23T22:25:50.455942Z"
        },
        "colab_type": "code",
        "id": "d4FeyL9nFnId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "73e24e8e-ac2a-4f25-ee82-c7fbb83330a9"
      },
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'B-ORG',\n",
              " 'I-ORG',\n",
              " 'B-LOC',\n",
              " 'B-PER',\n",
              " 'I-PER',\n",
              " 'I-MISC',\n",
              " 'B-MISC',\n",
              " 'I-LOC']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oración.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.473893Z",
          "start_time": "2020-06-23T22:25:50.462923Z"
        },
        "colab_type": "code",
        "id": "m5eSLm4diibR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "6d370b9e-0443-4000-ea8d-76e77db36f7c"
      },
      "source": [
        "# Tokens mas frecuentes\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 17657),\n",
              " (',', 14716),\n",
              " ('la', 9571),\n",
              " ('que', 7516),\n",
              " ('.', 7263),\n",
              " ('el', 6905),\n",
              " ('en', 6484),\n",
              " ('\"', 5691),\n",
              " ('y', 5336),\n",
              " ('a', 4304)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.479897Z",
          "start_time": "2020-06-23T22:25:50.475889Z"
        },
        "id": "31HIXgbktWtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seteamos algunas variables que nos serán de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### Frecuencia de los Tags\n",
        "\n",
        "Visualizemos rápidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.490885Z",
          "start_time": "2020-06-23T22:25:50.481873Z"
        },
        "colab_type": "code",
        "id": "tuXOsbJUiibh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "88c2a9a7-7110-41ba-85b2-6bf64a6a63ce"
      },
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t231920\t87.6%\n",
            "B-ORG\t7390\t 2.8%\n",
            "I-ORG\t4992\t 1.9%\n",
            "B-LOC\t4913\t 1.9%\n",
            "B-PER\t4321\t 1.6%\n",
            "I-PER\t3903\t 1.5%\n",
            "I-MISC\t3212\t 1.2%\n",
            "B-MISC\t2173\t 0.8%\n",
            "I-LOC\t1891\t 0.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T21:44:17.730460Z",
          "start_time": "2020-06-22T21:44:17.724482Z"
        },
        "colab_type": "text",
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### Configuramos pytorch y dividimos los datos.\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tamaño de los batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.101455Z",
          "start_time": "2020-06-23T22:25:50.492843Z"
        },
        "colab_type": "code",
        "id": "uB7cwLWpaSGs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a5608198-f388-4dc3-fb57-0b78f7e10d93"
      },
      "source": [
        "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que está disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### Métricas de evaluación\n",
        "\n",
        "Además, definiremos las métricas que serán usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `f1`.\n",
        "**Importante**: Noten que la evaluación solo se hace para las Named Entities (sin contar 'O')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.654826Z",
          "start_time": "2020-06-23T22:25:51.103450Z"
        },
        "colab_type": "code",
        "id": "9mUOOLEWiicU",
        "colab": {}
      },
      "source": [
        "# Definimos las métricas\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import warnings\n",
        "import sklearn.exceptions\n",
        "warnings.filterwarnings(\"ignore\",\n",
        "                        category=sklearn.exceptions.UndefinedMetricWarning)\n",
        "\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "    # Obtenemos los indices distintos de 0.\n",
        "\n",
        "    # filtramos <pad> y O para calcular los scores.\n",
        "    mask = [(y_true != o_idx) & (y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu')\n",
        "    y_true = y_true.to('cpu')\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### Modelo Baseline\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrá una capa de embedding, unas cuantas LSTM y una capa de salida y usará dropout en el entrenamiento.\n",
        "\n",
        "Este constará de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendrá la red.\n",
        "2. Definir los hiperparámetros e inicializar la red. \n",
        "3. Definir la época de entrenamiento\n",
        "3. Definir la función de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.666751Z",
          "start_time": "2020-06-23T22:25:51.656778Z"
        },
        "colab_type": "code",
        "id": "rMPL08XqaSG3",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definimos la Regresión Logistica https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, output_dim, pad_idx):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                          embedding_dim,\n",
        "                                          padding_idx=pad_idx)\n",
        "        \n",
        "        self.linear = torch.nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs = self.linear(embedded)\n",
        "        return outputs\n",
        "\n",
        "    \n",
        "# Definir la red\n",
        "class RNN_LSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx) :\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Agregamos una capa con GRU a modo de comparación\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions\n",
        "\n",
        "class RNN_GRU(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx) :\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Agregamos una capa con GRU a modo de comparación\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        outputs, hidden = self.gru(embedded) \n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T21:43:02.333880Z",
          "start_time": "2020-06-22T21:43:02.329861Z"
        },
        "colab_type": "text",
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### Hiperparámetros de la red\n",
        "\n",
        "Definimos los hiperparámetros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.705684Z",
          "start_time": "2020-06-23T22:25:51.668746Z"
        },
        "colab_type": "code",
        "id": "EHdi3QdOaSG8",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100 #200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128 #120  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 1  # número de capas.\n",
        "DROPOUT = 0.25\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = RNN_LSTM(INPUT_DIM, 100, 128, OUTPUT_DIM,\n",
        "                         1, False, 0.25, PAD_IDX) # Baseline con parametros de baseline.\n",
        "\n",
        "baseline_model_name = 'baseline_LSTM'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.710633Z",
          "start_time": "2020-06-23T22:25:51.706649Z"
        },
        "colab_type": "code",
        "id": "jlF1DhJeaSHA",
        "colab": {}
      },
      "source": [
        "baseline_n_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la función de loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.715637Z",
          "start_time": "2020-06-23T22:25:51.712628Z"
        },
        "colab_type": "code",
        "id": "6G_4k99_aSHG",
        "colab": {}
      },
      "source": [
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "\n",
        "# Loss: Cross Entropy\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "NegLogLikelihood = nn.NLLLoss(ignore_index = TAG_PAD_IDX)\n",
        "L1_Loss = nn.L1Loss()\n",
        "MSE_Loss = nn.MSELoss()\n",
        "PoissonNNL = nn.PoissonNLLLoss()\n",
        "MLSoftMarg = nn.MultiLabelSoftMarginLoss()\n",
        "\n",
        "criterion = cross_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1\n",
        "\n",
        "LSTM bidireccional, de una capa, con capa de embeddings, y dropout de 0.25.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.722604Z",
          "start_time": "2020-06-23T22:25:51.717615Z"
        },
        "colab_type": "code",
        "id": "c81f8ki5aSHL",
        "colab": {}
      },
      "source": [
        "model_1 = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         1, True, DROPOUT, PAD_IDX)\n",
        "model_name_1 = 'biLSTM'\n",
        "n_epochs_1 = 20\n",
        "loss_1 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2\n",
        "\n",
        "Queremos ver si hay alguna diferencia entre LSTM y GRU y cómo afecta la ausencia del atributo 'cell' en GRU en la clasificación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.728587Z",
          "start_time": "2020-06-23T22:25:51.724596Z"
        },
        "colab_type": "code",
        "id": "KWPzETaNaSHP",
        "colab": {}
      },
      "source": [
        "# Modelo 2 no funciona bien:\n",
        "\n",
        "# ValueError: not enough values to unpack (expected 2, got 1)\n",
        "# --> 110         outputs, (hidden, cell) = self.gru(embedded)\n",
        "\n",
        "# Sorprendentemente funciona bien el 3.\n",
        "\n",
        "model_2 = RNN_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         1, False, DROPOUT, PAD_IDX)\n",
        "model_name_2 = 'GRU'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T16:07:45.755561Z",
          "start_time": "2020-06-22T16:07:45.751571Z"
        },
        "colab_type": "text",
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 3\n",
        "\n",
        "Se replica el modelo 2 pero con GRU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.733572Z",
          "start_time": "2020-06-23T22:25:51.730580Z"
        },
        "colab_type": "code",
        "id": "_w0CFjA8aSHU",
        "colab": {}
      },
      "source": [
        "model_3 = RNN_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         1, True, DROPOUT, PAD_IDX)\n",
        "model_name_3 = 'biGRU'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNygonTz1dmr",
        "colab_type": "text"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 4\n",
        "\n",
        "Se usa un modelo que generalmente no es tan bueno en NER respecto a cualquier RNN a modo de comparación, la Regresión Lineal. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onuI3tbW1fqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_4 = LogisticRegression(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "model_name_4 = 'LogisticRegression'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668_JI-jBvyq",
        "colab_type": "text"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 5 y 6\n",
        "\n",
        "Dado que hasta ahora la red que ha dado mejores resultados ha sido una biLSTM, probaremos haciendo una biLSTM profunda. Suponemos que usar muchas capas va a ser contraproducente, por lo que crearemos una con 2 y 3 capas respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caaLx0H2BvVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_5 = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         2, True, DROPOUT, PAD_IDX)\n",
        "model_name_5 = 'Deep_biLSTM(2)'\n",
        "\n",
        "model_6 = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         3, True, DROPOUT, PAD_IDX)\n",
        "model_name_6 = 'Deep_biLSTM(3)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osS1d3P4EzN5",
        "colab_type": "text"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 7 y 8\n",
        "\n",
        "Veremos asimismo que ocurre cuando el dropout cambia en una biLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STBhhhBFEojG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_7 = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         1, True, 0, PAD_IDX)\n",
        "\n",
        "model_name_7 = 'Drop_biLSTM(0)'\n",
        "\n",
        "model_8 = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         1, True, 0.5, PAD_IDX)\n",
        "\n",
        "model_name_8 = 'Drop_biLSTM(0.5)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_GSKAhBSZH3",
        "colab_type": "text"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 9\n",
        "\n",
        "Aunque hasta ahora todos los modelos han funcionado correctamente, se encuentra que es de importancia el hacer una mención a un modelo que no pudo ser completamente implementado, con resultados prometedores. Este modelo corresponde a BERT, en lo que se adjunta el código desarrollado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lyf_wuoSXUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from torch.utils.data import Dataset\n",
        "#import pandas as pd\n",
        "\n",
        "#class SSTDataset(Dataset):\n",
        "#    # Inicializacion de la clase\n",
        "#    def __init__(self, filename, maxlen):\n",
        "#        #Guardar los contenidos del dataframe\n",
        "#        self.df = pd.read_csv(filename, delimiter = '\\t', quoting=csv.QUOTE_NONE, encoding=\"iso-8859-1\")\n",
        "#        #Initialize the BERT tokenizer\n",
        "#        self.tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "#        # Establecer el largo máximo\n",
        "#        self.maxlen = maxlen\n",
        "#\n",
        "#    # Funcion auxiliar que retorna el largo del dataframe\n",
        "#    def __len__(self):\n",
        "#        return len(self.df)\n",
        "#\n",
        "#    def __getitem__(self, index):\n",
        "#        sentence = self.df.loc[index, 'sentence']\n",
        "#        label = self.df.loc[index, 'label']\n",
        "#        tokens = self.tokenizer.tokenize(sentence)\n",
        "#        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "#        if len(tokens) < self.maxlen:\n",
        "#            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n",
        "#        else:\n",
        "#            tokens = tokens[:self.maxlen-1] + ['[SEP]']\n",
        "#        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "#        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "#        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "#        return tokens_ids_tensor, attn_mask, label\n",
        "#\n",
        "#from torch.utils.data import DataLoader\n",
        "#\n",
        "#train_set = SSTDataset(filename = 'train_NER_esp.txt', maxlen = 30)\n",
        "#val_set = SSTDataset(filename = 'val_NER_esp.txt', maxlen = 30)\n",
        "#\n",
        "#train_loader = DataLoader(train_set, batch_size = 64, num_workers = 5)\n",
        "#val_loader = DataLoader(val_set, batch_size = 64, num_workers = 5)\n",
        "#\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "#import torch\n",
        "#\n",
        "#tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "#model = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "#\n",
        "#import torch.nn as nn\n",
        "#class Classifier(nn.Module):\n",
        "#    def __init__(self, freeze_bert = True):\n",
        "#        super(Classifier, self).__init__()\n",
        "#        self.bert_layer = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased').cuda()\n",
        "#        if freeze_bert:\n",
        "#            for p in self.bert_layer.parameters():\n",
        "#                p.requires_grad = False\n",
        "#        self.cls_layer = nn.Linear(768, 1).cuda()\n",
        "#\n",
        "#    def forward(self, seq, attn_masks):\n",
        "#        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "#        cls_rep = cont_reps[:, 0]\n",
        "#        logits = self.cls_layer(cls_rep)\n",
        "#        return logits\n",
        "\n",
        "#def get_accuracy_from_logits(logits, labels):\n",
        "#    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "#    soft_probs = (probs > 0.5).long()\n",
        "#    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "#    return acc\n",
        "    \n",
        "#def evaluate(net, criterion, dataloader):\n",
        "#    net.eval()\n",
        "#    mean_acc, mean_loss = 0, 0\n",
        "#    count = 0\n",
        "#    with torch.no_grad():\n",
        "#        for seq, attn_masks, labels in dataloader:\n",
        "#            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "#            logits = net(seq, attn_masks)\n",
        "#            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "#            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "#            count += 1\n",
        "#\n",
        "#    return mean_acc / count, mean_loss / count\n",
        "#\n",
        "#def BertTrain(net, criterion, opti, train_loader, val_loader, epochs):\n",
        "#    for ep in range(epochs):\n",
        "#        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "#            opti.zero_grad()  \n",
        "#            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "#            logits = net(seq, attn_masks)\n",
        "#            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "#            loss.backward()\n",
        "#            opti.step()\n",
        "#            if (it + 1) % 100 == 0:\n",
        "#                acc = get_accuracy_from_logits(logits, labels)\n",
        "#                print(\"Iteration {} of epoch {} complete. Loss : {} Train Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
        "#        val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
        "#        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(ep+1, val_acc, val_loss))\n",
        "\n",
        "#import torch.optim as optim\n",
        "#net_freezed = SentimentClassifier(freeze_bert = True)\n",
        "#net_not_freezed = SentimentClassifier(freeze_bert = False)\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "#opti_freezed = optim.Adam(net_freezed.parameters(), lr = 2e-5)\n",
        "#opti_not_freezed = optim.Adam(net_not_freezed.parameters(), lr = 2e-5)\n",
        "\n",
        "#epochs = 5\n",
        "\n",
        "#try:\n",
        "#  BertTrain(net_freezed, criterion, opti_freezed, train_loader, val_loader, epochs)\n",
        "#except:\n",
        "#  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr9ik88s1Lhy",
        "colab_type": "text"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Conjunto de modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTXlt4A_xeMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### Entrenamos y evaluamos\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el número de épocas de entrenamiento, la loss y el optimizador que usarán para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.748571Z",
          "start_time": "2020-06-23T22:25:51.739556Z"
        },
        "colab_type": "code",
        "id": "r8YlGnjxaSHZ",
        "colab": {}
      },
      "source": [
        "\n",
        "criterion = loss_1\n",
        "n_epochs = n_epochs_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "#### Inicializamos la red\n",
        "\n",
        "iniciamos los pesos de la red de forma aleatoria (Usando una distribución normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.805380Z",
          "start_time": "2020-06-23T22:25:51.751524Z"
        },
        "colab_type": "code",
        "id": "Q-G_NWFcaSHe",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Por último, definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "#### Definimos el optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.819370Z",
          "start_time": "2020-06-23T22:25:51.814357Z"
        },
        "colab_type": "code",
        "id": "AH6o8_cTaSHq",
        "colab": {}
      },
      "source": [
        "# Lista de optimizadores de pytorch. \n",
        "#optimizerList = [optim.Adam(model.parameters()), optim.SGD(model.parameters()), \n",
        "#                 optim.Adadelta(model.parameters()), optim.Adagrad(model.parameters()), \n",
        "#                 optim.AdamW(model.parameters()), optim.Adamax(model.parameters()), \n",
        "#                 optim.ASDG(model.parameters()), optim.LBFGS(model.parameters()),\n",
        "#                 optim.RMSprop(model.parameters()), optim.Rprop(model.parameters())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "#### Definimos el entrenamiento de la red\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracción de la época. Se utilizan para entrenar mas rápidamente la red. (mas eficiente pasar n datos que uno en cada ejecución del backpropagation)\n",
        "\n",
        "Esta función está encargada de entrenar la red en una época. Para esto, por cada batch de la época actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\"\n",
        "\n",
        "Observación: En algunos comentarios aparecerá el tamaño de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:54.515194Z",
          "start_time": "2020-06-23T22:25:54.505221Z"
        },
        "colab_type": "code",
        "id": "DV6YLt0oiicW",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la época:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los parámetros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las métricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "#### `Definimos la función de evaluación`\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validación. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las métricas asociadas al conjunto de validación. \n",
        "Ya que las métricas son calculadas por cada batch, estas son retornadas promediadas por el número de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:54.527162Z",
          "start_time": "2020-06-23T22:25:54.518186Z"
        },
        "colab_type": "code",
        "id": "WsRuiUuHiicY",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las métricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:54.535141Z",
          "start_time": "2020-06-23T22:25:54.529158Z"
        },
        "colab_type": "code",
        "id": "Xs-n9Y5yiica",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### Entrenamiento de la red\n",
        "\n",
        "En este cuadro de código ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el número de épocas y luego por cada época, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez. \n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la función `init_weights`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T21:49:02.524817Z",
          "start_time": "2020-06-23T21:47:09.863026Z"
        },
        "colab_type": "code",
        "id": "iK5lQqpviicf",
        "colab": {}
      },
      "source": [
        "def run(model, model_n, optimizer, t_iterator, v_iterator, criterion, n_epoch, early_stopping):\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "  best_valid_loss = float('inf')\n",
        "  model.apply(init_weights)\n",
        "  print(model_n)\n",
        "  print('*******************************************')\n",
        "  num = 0\n",
        "  for epoch in range(n_epoch):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, t_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validación)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, v_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        num = 0 \n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no seguimos disminuyendo el loss de validación, terminamos de entrenar.\n",
        "    if best_valid_loss < valid_loss and early_stopping:\n",
        "      num += 1\n",
        "      if num == 3:\n",
        "        print('Early Stopped')\n",
        "        break\n",
        "      if num == 2: \n",
        "        print('Running one more iteration:')\n",
        "      else:\n",
        "        print('Running two more iterations:') \n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5EeVcf1t2Fs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79156896-8fc8-4aa4-97f1-71ab4d78d27c"
      },
      "source": [
        "Early_Stopping = True\n",
        "\n",
        "models = [baseline_model, \n",
        "          model_1, \n",
        "          model_2, \n",
        "          model_3, \n",
        "          model_4, # Descartado por bajo rendimiento Min F1.\n",
        "          model_5,\n",
        "          model_6,\n",
        "          model_7,\n",
        "          model_8\n",
        "          ]\n",
        "model_name = [baseline_model_name, \n",
        "              model_name_1, \n",
        "              model_name_2, \n",
        "              model_name_3, \n",
        "              model_name_4, # Descartado por bajo rendimiento Min F1.\n",
        "              model_name_5, \n",
        "              model_name_6, \n",
        "              model_name_7, \n",
        "              model_name_8\n",
        "              ]\n",
        "\n",
        "LossList = [cross_entropy, \n",
        "            #NegLogLikelihood ## Loss negativos.\n",
        "            #L1_Loss, # Error: Dimensionalidad\n",
        "            #MSE_Loss, # The size of tensor a (560) must match \n",
        "            #PoissonNNL, # the size of tensor b (10) at \n",
        "            #MLSoftMarg # non-singleton dimension 1.\n",
        "            ] \n",
        "\n",
        "for i in range(len(models)):\n",
        "  model = models[i]   \n",
        "                                                      # LSTM BLSTM BGRU LR : Puntajes F1\n",
        "  optimizerList = [optim.Adam(model.parameters()),     # 0.55 0.62 0.58 0.39  Best F1 = 0.62  Mejor hasta ahora con biLSTM\n",
        "                  #optim.SGD(model.parameters()),      # Falla\n",
        "                  #optim.Adadelta(model.parameters()), # 0.54 0.53 0.54 0.34  Best F1 = 0.54  Descartado\n",
        "                  #optim.Adagrad(model.parameters()),  # Falla\n",
        "                  #optim.AdamW(model.parameters()),     # 0.57 0.58 0.57 0.39  Best F1 = 0.58\n",
        "                  #optim.Adamax(model.parameters()),   # 0.52 0.57 0.56 0.38  Best F1 = 0.57  Descartado\n",
        "                  #optim.ASGD(model.parameters()),     # Falla\n",
        "                  #optim.LBFGS(model.parameters()),    # Falla\n",
        "                  optim.RMSprop(model.parameters())   # 0.56 0.60 0.55 0.37  Best F1 = 0.60\n",
        "                  #optim.Rprop(model.parameters())     # 0.50 0.54 0.53 0.28  Best F1 = 0.54  Descartado\n",
        "  ]\n",
        "   # Elegimos los mejores optimizadores para ahorrar tiempo de computo y poder iterar en otras categorías.\n",
        "   # Asimismo tambien damos de baja a la regresión lineal como modelo.\n",
        "                                    \n",
        "  for optimizer in optimizerList:\n",
        "    print(optimizer)\n",
        "    \n",
        "    for loss in LossList:\n",
        "      #print('*******************************************')\n",
        "      #print(loss)\n",
        "      run(model, model_name[i], optimizer, train_iterator, valid_iterator, loss, n_epochs, Early_Stopping)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "baseline_LSTM\n",
            "*******************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.434 | Train f1: 0.17 | Train precision: 0.22 | Train recall: 0.16\n",
            "\t Val. Loss: 0.307 |  Val. f1: 0.33 |  Val. precision: 0.41 | Val. recall: 0.32\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.170 | Train f1: 0.50 | Train precision: 0.56 | Train recall: 0.50\n",
            "\t Val. Loss: 0.246 |  Val. f1: 0.50 |  Val. precision: 0.60 | Val. recall: 0.49\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.101 | Train f1: 0.67 | Train precision: 0.71 | Train recall: 0.67\n",
            "\t Val. Loss: 0.208 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.55\n",
            "Running two more iterations:\n",
            "Epoch: 04 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.069 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
            "\t Val. Loss: 0.218 |  Val. f1: 0.57 |  Val. precision: 0.64 | Val. recall: 0.57\n",
            "Epoch: 05 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.051 | Train f1: 0.80 | Train precision: 0.83 | Train recall: 0.80\n",
            "\t Val. Loss: 0.206 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.59\n",
            "Running two more iterations:\n",
            "Epoch: 06 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.040 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
            "\t Val. Loss: 0.245 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.57\n",
            "Running one more iteration:\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.032 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
            "\t Val. Loss: 0.241 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.58\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "baseline_LSTM\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.224 | Train f1: 0.49 | Train precision: 0.57 | Train recall: 0.47\n",
            "\t Val. Loss: 0.202 |  Val. f1: 0.57 |  Val. precision: 0.66 | Val. recall: 0.54\n",
            "Running two more iterations:\n",
            "Epoch: 02 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.084 | Train f1: 0.73 | Train precision: 0.76 | Train recall: 0.73\n",
            "\t Val. Loss: 0.204 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
            "Running one more iteration:\n",
            "Epoch: 03 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.057 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
            "\t Val. Loss: 0.221 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "biLSTM\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.373 | Train f1: 0.25 | Train precision: 0.31 | Train recall: 0.24\n",
            "\t Val. Loss: 0.240 |  Val. f1: 0.44 |  Val. precision: 0.52 | Val. recall: 0.43\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.127 | Train f1: 0.60 | Train precision: 0.66 | Train recall: 0.60\n",
            "\t Val. Loss: 0.203 |  Val. f1: 0.55 |  Val. precision: 0.63 | Val. recall: 0.55\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.070 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
            "\t Val. Loss: 0.195 |  Val. f1: 0.59 |  Val. precision: 0.67 | Val. recall: 0.57\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.044 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.82\n",
            "\t Val. Loss: 0.188 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
            "Running two more iterations:\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.031 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
            "\t Val. Loss: 0.203 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
            "Running one more iteration:\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.023 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.219 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "biLSTM\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.318 | Train f1: 0.32 | Train precision: 0.38 | Train recall: 0.31\n",
            "\t Val. Loss: 0.283 |  Val. f1: 0.43 |  Val. precision: 0.53 | Val. recall: 0.42\n",
            "Running two more iterations:\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.121 | Train f1: 0.62 | Train precision: 0.66 | Train recall: 0.63\n",
            "\t Val. Loss: 0.321 |  Val. f1: 0.43 |  Val. precision: 0.51 | Val. recall: 0.45\n",
            "Running one more iteration:\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.100 | Train f1: 0.68 | Train precision: 0.72 | Train recall: 0.69\n",
            "\t Val. Loss: 0.289 |  Val. f1: 0.49 |  Val. precision: 0.58 | Val. recall: 0.49\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.068 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.76\n",
            "\t Val. Loss: 0.266 |  Val. f1: 0.53 |  Val. precision: 0.61 | Val. recall: 0.52\n",
            "Running two more iterations:\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.055 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
            "\t Val. Loss: 0.284 |  Val. f1: 0.54 |  Val. precision: 0.60 | Val. recall: 0.55\n",
            "Running one more iteration:\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.047 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.82\n",
            "\t Val. Loss: 0.304 |  Val. f1: 0.54 |  Val. precision: 0.60 | Val. recall: 0.54\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "GRU\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.399 | Train f1: 0.23 | Train precision: 0.30 | Train recall: 0.22\n",
            "\t Val. Loss: 0.275 |  Val. f1: 0.43 |  Val. precision: 0.53 | Val. recall: 0.41\n",
            "Epoch: 02 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.149 | Train f1: 0.57 | Train precision: 0.64 | Train recall: 0.57\n",
            "\t Val. Loss: 0.219 |  Val. f1: 0.53 |  Val. precision: 0.62 | Val. recall: 0.51\n",
            "Running two more iterations:\n",
            "Epoch: 03 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.089 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.70\n",
            "\t Val. Loss: 0.221 |  Val. f1: 0.56 |  Val. precision: 0.65 | Val. recall: 0.55\n",
            "Epoch: 04 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.061 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
            "\t Val. Loss: 0.210 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.59\n",
            "Running two more iterations:\n",
            "Epoch: 05 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.045 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.218 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.59\n",
            "Running one more iteration:\n",
            "Epoch: 06 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.035 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
            "\t Val. Loss: 0.245 |  Val. f1: 0.60 |  Val. precision: 0.68 | Val. recall: 0.58\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "GRU\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.234 | Train f1: 0.51 | Train precision: 0.60 | Train recall: 0.49\n",
            "\t Val. Loss: 0.217 |  Val. f1: 0.56 |  Val. precision: 0.66 | Val. recall: 0.53\n",
            "Running two more iterations:\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.080 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.74\n",
            "\t Val. Loss: 0.219 |  Val. f1: 0.59 |  Val. precision: 0.67 | Val. recall: 0.57\n",
            "Running one more iteration:\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.059 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
            "\t Val. Loss: 0.232 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.57\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "biGRU\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.342 | Train f1: 0.29 | Train precision: 0.36 | Train recall: 0.27\n",
            "\t Val. Loss: 0.233 |  Val. f1: 0.48 |  Val. precision: 0.58 | Val. recall: 0.46\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.118 | Train f1: 0.63 | Train precision: 0.69 | Train recall: 0.62\n",
            "\t Val. Loss: 0.188 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.56\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.77 | Train precision: 0.79 | Train recall: 0.76\n",
            "\t Val. Loss: 0.183 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.59\n",
            "Running two more iterations:\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.040 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n",
            "\t Val. Loss: 0.191 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
            "Running one more iteration:\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.026 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
            "\t Val. Loss: 0.197 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "biGRU\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.318 | Train f1: 0.44 | Train precision: 0.52 | Train recall: 0.43\n",
            "\t Val. Loss: 0.212 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.56\n",
            "Running two more iterations:\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.092 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.72\n",
            "\t Val. Loss: 0.301 |  Val. f1: 0.56 |  Val. precision: 0.67 | Val. recall: 0.53\n",
            "Running one more iteration:\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.059 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
            "\t Val. Loss: 0.287 |  Val. f1: 0.59 |  Val. precision: 0.67 | Val. recall: 0.57\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "LogisticRegression\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.850 | Train f1: 0.26 | Train precision: 0.34 | Train recall: 0.26\n",
            "\t Val. Loss: 0.512 |  Val. f1: 0.33 |  Val. precision: 0.43 | Val. recall: 0.32\n",
            "Epoch: 02 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.294 | Train f1: 0.48 | Train precision: 0.57 | Train recall: 0.47\n",
            "\t Val. Loss: 0.398 |  Val. f1: 0.39 |  Val. precision: 0.51 | Val. recall: 0.38\n",
            "Epoch: 03 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.194 | Train f1: 0.58 | Train precision: 0.66 | Train recall: 0.57\n",
            "\t Val. Loss: 0.373 |  Val. f1: 0.41 |  Val. precision: 0.52 | Val. recall: 0.39\n",
            "Epoch: 04 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.159 | Train f1: 0.60 | Train precision: 0.68 | Train recall: 0.59\n",
            "\t Val. Loss: 0.368 |  Val. f1: 0.41 |  Val. precision: 0.52 | Val. recall: 0.39\n",
            "Running two more iterations:\n",
            "Epoch: 05 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.146 | Train f1: 0.60 | Train precision: 0.68 | Train recall: 0.59\n",
            "\t Val. Loss: 0.369 |  Val. f1: 0.41 |  Val. precision: 0.52 | Val. recall: 0.39\n",
            "Running one more iteration:\n",
            "Epoch: 06 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.139 | Train f1: 0.60 | Train precision: 0.67 | Train recall: 0.59\n",
            "\t Val. Loss: 0.371 |  Val. f1: 0.41 |  Val. precision: 0.52 | Val. recall: 0.39\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "LogisticRegression\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.308 | Train f1: 0.42 | Train precision: 0.54 | Train recall: 0.38\n",
            "\t Val. Loss: 0.343 |  Val. f1: 0.39 |  Val. precision: 0.50 | Val. recall: 0.38\n",
            "Epoch: 02 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.166 | Train f1: 0.58 | Train precision: 0.66 | Train recall: 0.57\n",
            "\t Val. Loss: 0.340 |  Val. f1: 0.40 |  Val. precision: 0.51 | Val. recall: 0.38\n",
            "Running two more iterations:\n",
            "Epoch: 03 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.151 | Train f1: 0.59 | Train precision: 0.66 | Train recall: 0.58\n",
            "\t Val. Loss: 0.346 |  Val. f1: 0.40 |  Val. precision: 0.52 | Val. recall: 0.38\n",
            "Running one more iteration:\n",
            "Epoch: 04 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.147 | Train f1: 0.59 | Train precision: 0.66 | Train recall: 0.58\n",
            "\t Val. Loss: 0.351 |  Val. f1: 0.40 |  Val. precision: 0.51 | Val. recall: 0.38\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "Deep_biLSTM(2)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 27s\n",
            "\tTrain Loss: 0.365 | Train f1: 0.25 | Train precision: 0.31 | Train recall: 0.24\n",
            "\t Val. Loss: 0.244 |  Val. f1: 0.44 |  Val. precision: 0.51 | Val. recall: 0.45\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.126 | Train f1: 0.60 | Train precision: 0.65 | Train recall: 0.60\n",
            "\t Val. Loss: 0.185 |  Val. f1: 0.57 |  Val. precision: 0.64 | Val. recall: 0.57\n",
            "Running two more iterations:\n",
            "Epoch: 03 | Epoch Time: 0m 24s\n",
            "\tTrain Loss: 0.068 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
            "\t Val. Loss: 0.190 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.59\n",
            "Running one more iteration:\n",
            "Epoch: 04 | Epoch Time: 0m 24s\n",
            "\tTrain Loss: 0.043 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.82\n",
            "\t Val. Loss: 0.199 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "Deep_biLSTM(2)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.400 | Train f1: 0.24 | Train precision: 0.28 | Train recall: 0.25\n",
            "\t Val. Loss: 0.366 |  Val. f1: 0.31 |  Val. precision: 0.37 | Val. recall: 0.36\n",
            "Epoch: 02 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.131 | Train f1: 0.58 | Train precision: 0.63 | Train recall: 0.59\n",
            "\t Val. Loss: 0.201 |  Val. f1: 0.56 |  Val. precision: 0.63 | Val. recall: 0.57\n",
            "Running two more iterations:\n",
            "Epoch: 03 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.087 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.73\n",
            "\t Val. Loss: 0.219 |  Val. f1: 0.57 |  Val. precision: 0.63 | Val. recall: 0.57\n",
            "Running one more iteration:\n",
            "Epoch: 04 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.78 | Train precision: 0.80 | Train recall: 0.78\n",
            "\t Val. Loss: 0.221 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.60\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "Deep_biLSTM(3)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 45s\n",
            "\tTrain Loss: 0.382 | Train f1: 0.23 | Train precision: 0.28 | Train recall: 0.22\n",
            "\t Val. Loss: 0.241 |  Val. f1: 0.44 |  Val. precision: 0.53 | Val. recall: 0.44\n",
            "Epoch: 02 | Epoch Time: 0m 42s\n",
            "\tTrain Loss: 0.133 | Train f1: 0.58 | Train precision: 0.64 | Train recall: 0.58\n",
            "\t Val. Loss: 0.190 |  Val. f1: 0.56 |  Val. precision: 0.63 | Val. recall: 0.55\n",
            "Running two more iterations:\n",
            "Epoch: 03 | Epoch Time: 0m 43s\n",
            "\tTrain Loss: 0.074 | Train f1: 0.73 | Train precision: 0.76 | Train recall: 0.74\n",
            "\t Val. Loss: 0.207 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.57\n",
            "Running one more iteration:\n",
            "Epoch: 04 | Epoch Time: 0m 44s\n",
            "\tTrain Loss: 0.049 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.215 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "Deep_biLSTM(3)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 43s\n",
            "\tTrain Loss: 0.752 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: 0.766 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 02 | Epoch Time: 0m 45s\n",
            "\tTrain Loss: 0.436 | Train f1: 0.13 | Train precision: 0.16 | Train recall: 0.14\n",
            "\t Val. Loss: 0.491 |  Val. f1: 0.33 |  Val. precision: 0.39 | Val. recall: 0.34\n",
            "Epoch: 03 | Epoch Time: 0m 40s\n",
            "\tTrain Loss: 0.170 | Train f1: 0.47 | Train precision: 0.52 | Train recall: 0.48\n",
            "\t Val. Loss: 0.230 |  Val. f1: 0.49 |  Val. precision: 0.58 | Val. recall: 0.48\n",
            "Epoch: 04 | Epoch Time: 0m 44s\n",
            "\tTrain Loss: 0.111 | Train f1: 0.64 | Train precision: 0.67 | Train recall: 0.64\n",
            "\t Val. Loss: 0.221 |  Val. f1: 0.54 |  Val. precision: 0.59 | Val. recall: 0.57\n",
            "Running two more iterations:\n",
            "Epoch: 05 | Epoch Time: 0m 44s\n",
            "\tTrain Loss: 0.086 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.72\n",
            "\t Val. Loss: 0.224 |  Val. f1: 0.54 |  Val. precision: 0.60 | Val. recall: 0.55\n",
            "Running one more iteration:\n",
            "Epoch: 06 | Epoch Time: 0m 41s\n",
            "\tTrain Loss: 0.073 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
            "\t Val. Loss: 0.233 |  Val. f1: 0.58 |  Val. precision: 0.63 | Val. recall: 0.60\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "Drop_biLSTM(0)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.331 | Train f1: 0.29 | Train precision: 0.35 | Train recall: 0.28\n",
            "\t Val. Loss: 0.240 |  Val. f1: 0.46 |  Val. precision: 0.55 | Val. recall: 0.46\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.102 | Train f1: 0.66 | Train precision: 0.71 | Train recall: 0.66\n",
            "\t Val. Loss: 0.190 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.58\n",
            "Running two more iterations:\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.049 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.190 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
            "Running one more iteration:\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.028 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.207 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.59\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "Drop_biLSTM(0)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.246 | Train f1: 0.46 | Train precision: 0.54 | Train recall: 0.45\n",
            "\t Val. Loss: 0.218 |  Val. f1: 0.54 |  Val. precision: 0.62 | Val. recall: 0.53\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.066 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
            "\t Val. Loss: 0.197 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.59\n",
            "Running two more iterations:\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.037 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
            "\t Val. Loss: 0.215 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
            "Running one more iteration:\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.025 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
            "\t Val. Loss: 0.228 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.59\n",
            "Early Stopped\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "Drop_biLSTM(0.5)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.422 | Train f1: 0.18 | Train precision: 0.24 | Train recall: 0.16\n",
            "\t Val. Loss: 0.291 |  Val. f1: 0.35 |  Val. precision: 0.45 | Val. recall: 0.35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.179 | Train f1: nan | Train precision: nan | Train recall: nan\n",
            "\t Val. Loss: 0.209 |  Val. f1: 0.51 |  Val. precision: 0.59 | Val. recall: 0.51\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.114 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.63\n",
            "\t Val. Loss: 0.201 |  Val. f1: 0.55 |  Val. precision: 0.63 | Val. recall: 0.54\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.081 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.72\n",
            "\t Val. Loss: 0.183 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.58\n",
            "Running two more iterations:\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.061 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
            "\t Val. Loss: 0.191 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.59\n",
            "Running one more iteration:\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.191 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.62\n",
            "Early Stopped\n",
            "RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    momentum: 0\n",
            "    weight_decay: 0\n",
            ")\n",
            "Drop_biLSTM(0.5)\n",
            "*******************************************\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.357 | Train f1: 0.28 | Train precision: 0.35 | Train recall: 0.26\n",
            "\t Val. Loss: 0.298 |  Val. f1: 0.39 |  Val. precision: 0.48 | Val. recall: 0.39\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.49 | Train precision: 0.54 | Train recall: 0.49\n",
            "\t Val. Loss: 0.290 |  Val. f1: 0.44 |  Val. precision: 0.53 | Val. recall: 0.45\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.127 | Train f1: 0.60 | Train precision: 0.64 | Train recall: 0.60\n",
            "\t Val. Loss: 0.244 |  Val. f1: 0.52 |  Val. precision: 0.60 | Val. recall: 0.52\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.093 | Train f1: 0.70 | Train precision: 0.73 | Train recall: 0.70\n",
            "\t Val. Loss: 0.243 |  Val. f1: 0.57 |  Val. precision: 0.64 | Val. recall: 0.58\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.069 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
            "\t Val. Loss: 0.211 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.60\n",
            "Running two more iterations:\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.057 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
            "\t Val. Loss: 0.218 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
            "Running one more iteration:\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.223 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
            "Early Stopped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkArXL5ftWud",
        "colab_type": "text"
      },
      "source": [
        "**Importante**: Recuerden que el último modelo entrenado no es el mejor (probablemente esté *overfitteado*), si no el que guardamos con la menor loss del conjunto de validación.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
        "\n",
        "Este problema lo pueden solucionar con *early stopping*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:58.680706Z",
          "start_time": "2020-06-23T22:25:58.663725Z"
        },
        "colab_type": "code",
        "id": "y27CNYfrjtQ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "9a8b4d96-ad90-4edf-8bec-66ef1e752fbd"
      },
      "source": [
        "# Modelo seleccionado para la competencia:\n",
        "\n",
        "model_comp = RNN_LSTM(INPUT_DIM, 200, 200, OUTPUT_DIM,\n",
        "                         1, True, 0.25, PAD_IDX)\n",
        "model_comp_name = 'model_Comp'\n",
        "\n",
        "optim_comp = optim.Adam(model_comp.parameters())\n",
        "\n",
        "run(model_comp, model_comp_name, optim_comp, train_iterator, valid_iterator, cross_entropy, 5, False)\n",
        "\n",
        "# cargar el mejor modelo entrenado.\n",
        "#model.load_state_dict(torch.load('{}.pt'.format(model_comp_name)))\n",
        "# Lo vamos a ejecutar diretamente en usando el modelo entrenado en el epoch 5."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Comp\n",
            "*******************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.306 | Train f1: 0.34 | Train precision: 0.42 | Train recall: 0.32\n",
            "\t Val. Loss: 0.232 |  Val. f1: 0.50 |  Val. precision: 0.59 | Val. recall: 0.49\n",
            "Epoch: 02 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.099 | Train f1: 0.67 | Train precision: 0.72 | Train recall: 0.67\n",
            "\t Val. Loss: 0.231 |  Val. f1: 0.55 |  Val. precision: 0.63 | Val. recall: 0.55\n",
            "Epoch: 03 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.051 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
            "\t Val. Loss: 0.245 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
            "Epoch: 04 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.030 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.235 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
            "Epoch: 05 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.020 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
            "\t Val. Loss: 0.249 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T21:52:04.077979Z",
          "start_time": "2020-06-23T21:52:04.072991Z"
        },
        "id": "BL8VtRnStWuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kBctQHTh0lxD"
      },
      "source": [
        "#### Evaluamos el set de validación con el modelo final\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluación con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:26:01.788742Z",
          "start_time": "2020-06-23T22:26:00.558829Z"
        },
        "colab_type": "code",
        "id": "s0gVbP8yiicj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1001c039-bc5b-46f4-c350-6068062b81b6"
      },
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model_comp, valid_iterator, criterion) # Se evalua el modelo de competencia \n",
        "                                           # especificamente, cargando la ultima \n",
        "                                           # epoch a propósito.\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val. Loss: 0.249 |  Val. f1: 0.61 | Val. precision: 0.66 | Val. recall: 0.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "\n",
        "### Predecir datos para la competencia\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, predeciremos las etiquetas que serán evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:31:56.776563Z",
          "start_time": "2020-06-23T22:31:39.654525Z"
        },
        "colab_type": "code",
        "id": "1RBs3UU4wLk3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "92bc9b5f-3a23-4f27-e6ec-8cee54e1b92b"
      },
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oración predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model_comp, test_iterator, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "### Generar el archivo para la submission\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:33:41.845955Z",
          "start_time": "2020-06-23T22:33:41.731717Z"
        },
        "colab_type": "code",
        "id": "RPfZkjJGkWyq",
        "colab": {}
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for word, tag in predictions:\n",
        "    f.write(word + ' ' + tag + '\\n')\n",
        "f.write('\\n')\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T21:49:19.575711Z",
          "start_time": "2020-06-23T21:49:19.100486Z"
        },
        "colab_type": "code",
        "id": "k2PqvJAmTFWR",
        "colab": {}
      },
      "source": [
        "# A veces no funciona a la primera. Ejecutar mas de una vez para obtener el archivo...\n",
        "#from google.colab import files\n",
        "#files.download('predictions.zip')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "Como se puede observar en los resultados obtenidos, se concluye que, con los parámetros utilizados y el dataset predefinido, destacan los modelos basados en RNN, en específico los LSTM bidireccionales.\n",
        "Siguiendo la línea de los procedimientos, lamentablemente las regresiones logísticas no tuvieron lugar. Los resultados obtenidos en F1 con estas corresponden a valores por bajo del 60% de los valores logrados con cualquier RNN. \n",
        "\n",
        "Dentro de lo obtenido con RNN se observan mejores resultados con el modelo LSTM por sobre GRU, en conjunto con estructuras Bidireccionales sobre Unidireccionales. Con respecto a la profundidad de las RNN, se obtienen resultados inconsistentes, variando mucho entre entrenamiento y entrenamiento, por lo cual no es confiable dentro de lo esperado.\n",
        "\n",
        "Con respecto a los optimizadores utilizados para las RNN, todos se comportan similarmente, sin embargo, destacan en rendimiento los optimizadores Adam y RMSprop.\n",
        "\n",
        "Finalmente, es de importancia mencionar que los mejores resultados se obtuvieron con una mayor cantidad neuronas respecto al baseline, siendo el mejor de los casos probados en orden de las 200 en ambas capas (hidden y embeddings), sin embargo, estos valores altos de neuronas requieren un mayor dropout para evitar overfitting.\n",
        "\n",
        "Con esto se evalua el uso del dropout variándolo entre 0 y 0.5, mostrando que usando 0.5 recien se observa overfitting en epochs muy altas, como 20-30, pero tampoco convergencia, mientras que en 0 se observa gran tendencia a overfitting en las primeras 5 epochs. \n",
        "\n",
        "Cuando se evalúa el uso del Early stopping se puede observar que no necesariamente muestra mejores resultados en términos de F1, sino que arroja el epoch con mínimo loss de validación, por lo que se implementó un early stopping retardado, en donde se corren dos iteraciones adicionales para ver cómo evoluciona el loss y F1 de validación. \n",
        "\n",
        "A trabajos futuros, nos gustaría poder implementar flair para poder compararlo con los valores obtenidos, al igual que terminar la implementacióón de BERT. Aprendimos mucho de RNN y de diversos optimizadores que son utilizables en general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_XICVU2Pe6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}