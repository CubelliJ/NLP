{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea1Returns.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rf9ZEugQOR5U",
        "PkMKhSLzOR5e",
        "9EJ0v5e9OR5o",
        "3lQLmd8WOR5u",
        "Rdgxo2VCOR6J",
        "YrsqpUWCOR6U",
        "IBOSYWxxOR6o",
        "QkcqLzhZOR62",
        "pYcqr1xSOR7k"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hJ-Y8iPlOR5L",
        "toc": true
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objetivo-e-Instrucciones:\" data-toc-modified-id=\"Objetivo-e-Instrucciones:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objetivo e Instrucciones:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objetivo\" data-toc-modified-id=\"Objetivo-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objetivo</a></span></li><li><span><a href=\"#Fecha-de-Entrega:\" data-toc-modified-id=\"Fecha-de-Entrega:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Fecha de Entrega:</a></span></li><li><span><a href=\"#Detalles-e-instrucciones-de-la-competencia:\" data-toc-modified-id=\"Detalles-e-instrucciones-de-la-competencia:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Detalles e instrucciones de la competencia:</a></span></li><li><span><a href=\"#Reporte\" data-toc-modified-id=\"Reporte-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Reporte</a></span></li><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Baseline</a></span></li></ul></li><li><span><a href=\"#1.-Introducción\" data-toc-modified-id=\"1.-Introducción-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>1. Introducción</a></span></li><li><span><a href=\"#2.-Representaciones\" data-toc-modified-id=\"2.-Representaciones-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>2. Representaciones</a></span></li><li><span><a href=\"#3.-Algoritmos\" data-toc-modified-id=\"3.-Algoritmos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>3. Algoritmos</a></span></li><li><span><a href=\"#4.-Métricas-de-Evaluación\" data-toc-modified-id=\"4.-Métricas-de-Evaluación-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>4. Métricas de Evaluación</a></span></li><li><span><a href=\"#5.-Experimentos\" data-toc-modified-id=\"5.-Experimentos-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>5. Experimentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importar-librerías-y-utiles\" data-toc-modified-id=\"Importar-librerías-y-utiles-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Importar librerías y utiles</a></span></li><li><span><a href=\"#Definir-métodos-de-evaluación\" data-toc-modified-id=\"Definir-métodos-de-evaluación-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Definir métodos de evaluación</a></span></li><li><span><a href=\"#Datos\" data-toc-modified-id=\"Datos-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Datos</a></span></li><li><span><a href=\"#Analizar-los-datos\" data-toc-modified-id=\"Analizar-los-datos-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Analizar los datos</a></span></li><li><span><a href=\"#Custom-Features\" data-toc-modified-id=\"Custom-Features-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Custom Features</a></span></li><li><span><a href=\"#Definir-la-representación-y-el-clasificador\" data-toc-modified-id=\"Definir-la-representación-y-el-clasificador-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Definir la representación y el clasificador</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-para-algún-dataset\" data-toc-modified-id=\"Ejecutar-el-pipeline-para-algún-dataset-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Ejecutar el pipeline para algún dataset</a></span></li><li><span><a href=\"#Ejecutar-el-sistema-creado-por-cada-train-set\" data-toc-modified-id=\"Ejecutar-el-sistema-creado-por-cada-train-set-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Ejecutar el sistema creado por cada train set</a></span></li><li><span><a href=\"#Predecir-los-target-set-y-crear-la-submission\" data-toc-modified-id=\"Predecir-los-target-set-y-crear-la-submission-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Predecir los target set y crear la submission</a></span></li></ul></li><li><span><a href=\"#6.-Conclusiones\" data-toc-modified-id=\"6.-Conclusiones-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>6. Conclusiones</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "colab_type": "text",
        "id": "MFxQBdRhOR5O"
      },
      "source": [
        "# Tarea 1 NLP : Competencia de Clasificación de Texto\n",
        "-------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4vEPE71lOR5R"
      },
      "source": [
        "- **Nombres: Joaquin Cubelli de León, Tomás de la Sotta Krause**\n",
        "\n",
        "- **Usuario o nombre de equipo en Codalab: Team NSNOUBN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rf9ZEugQOR5U"
      },
      "source": [
        "## Objetivo e Instrucciones:\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "Esta tarea consiste en participar en una competencia cuyo objetivo es la clasificación de tweets según su intensidad de emoción. Específicamente: \n",
        "\n",
        "Tendrán 4 datasets de tweets de distintas emociones: `anger`, `fear`, `sadness` y `joy`. Para cada uno de estos datasets, deberán crear un clasificador que indique la intensidad de dicha emoción en sus tweets (`low`, `medium`, `high`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zs5tUUY-OR5V"
      },
      "source": [
        "###  Fecha de Entrega: \n",
        "\n",
        "Por ser anunciada una vez termine el paro. Se publicará la fecha en ucursos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T14:34:38.796217Z",
          "start_time": "2020-04-07T14:34:38.782255Z"
        },
        "colab_type": "text",
        "id": "IgNcSnajOR5X"
      },
      "source": [
        "### Detalles e instrucciones de la competencia:\n",
        "\n",
        "- La competencia consiste en resolver 4 problemas de clasificación distintos, cada uno de tres clases. Por cada problema deberán crear un clasificador distinto. La evaluación de la competencia se realiza en base a 4 métricas: AUC, Kappa y Accuracy. Los mejores puntajes en cada ítem serán los que ganen.\n",
        "\n",
        "- Para comenzar se les entregará en este notebook el baseline y la estructura del reporte. El baseline es el código que realiza creación de features y clasificación básica. Los puntajes de este serán ocupados como base para la competencia: deben superar sus resultados para ser bien evaluados.  \n",
        "\n",
        "- Para participar, deben registrarse en Codalab y luego ingresar a la competencia usando el siguiente [link]( https://competitions.codalab.org/competitions/24121?secret_key=f5eb2d95-b36e-4aad-8fc5-4d9d77f4e4dc). \n",
        "\n",
        "- **Es requisito entregar el reporte con el código y haber participado en la competencia para ser evaluado.**\n",
        "\n",
        "- Pueden hacer grupos de máximo 2 alumnos. Cada grupo debe tener un nombre de equipo (En codalab, ir a settings y después cambiar Team Name). Solo una persona debe administrar la cuenta del grupo.\n",
        "\n",
        "- En total pueden hacer un **máximo de 4 envíos/submissions** (tanto para equipos como para envíos indivuales).\n",
        "\n",
        "- Hagan varios experimentos haciendo cross-validation o evaluación sobre una sub-partición antes de enviar sus predicciones a Codalab. Asegúrense que la distribución de las clases sea balanceada en las particiones de training y testing. Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les será evaluado incorrectamente.\n",
        "\n",
        "- Estar top 5 en alguna métrica equivale a 1 punto extra en la nota final.\n",
        "\n",
        "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. ¡Usen todo su conocimiento e ingenio en mejorar sus sistemas! \n",
        "\n",
        "- Todas las dudas escríbanlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente serán remitidos a ese medio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:18:43.301002Z",
          "start_time": "2019-08-21T19:18:43.298037Z"
        },
        "colab_type": "text",
        "id": "EUYuMRm5OR5Z"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "Por último, el baseline contiene un código básico que:\n",
        "\n",
        "- Obtiene los dataset.\n",
        "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
        "- Crea un Pipeline que: \n",
        "    - Crea features personalizadas.\n",
        "    - Transforma los dataset a bag of words (BoW).  \n",
        "    - Entrena un clasificador usando cada train set.\n",
        "- Clasifica y evalua el sistema creado usando el test set.\n",
        "- Clasifica el target set.\n",
        "- Genera una submission con el target en formato zip en el directorio en donde se está ejecutando el notebook. \n",
        "\n",
        "\n",
        "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendrá mas sentido cuando vean el código)\n",
        "\n",
        "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. También, el parámetro `ngram_range` (Ojo que el clf naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia). Además, implementar los atributos que crean útiles desde el listado del el enunciado. Investigar también el vectorizador tf-idf.\n",
        "\n",
        "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la función `predict_proba`).\n",
        "\n",
        "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aquí les adjuntamos algunos ejemplos:\n",
        "    -\tWord n-grams.\n",
        "    -\tCharacter n-grams. \n",
        "    -\tPart-of-speech tags.\n",
        "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
        "        - Count the number of positive and negative words within a sentence.\n",
        "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
        "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
        "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
        "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
        "    -\tThe number of words with all characters in uppercase.\n",
        "    -\tThe presence and the number of positive or negative emoticons.\n",
        "    -\tThe number of individual negations.\n",
        "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
        "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
        "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
        "\n",
        "- **Reducción de dimensionalidad**: También puede serles de ayuda. Referencias [aquí](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
        "\n",
        "- Por último, pueden encontrar mas referencias de cómo mejorar sus features, el vectorizador y el clasificador [aquí](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:25:19.677190Z",
          "start_time": "2020-04-07T15:25:19.671206Z"
        },
        "colab_type": "text",
        "id": "lNHzN8pIOR5a"
      },
      "source": [
        "(Pueden eliminar cualquier celda con instrucciones...)\n",
        "\n",
        "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO serán evaluados Notebooks sin nombre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jMbkpWWmOR5c"
      },
      "source": [
        "----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "colab_type": "text",
        "id": "PkMKhSLzOR5e"
      },
      "source": [
        "## 1. Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M13d7dmvOR5f"
      },
      "source": [
        "**Presentar brevemente el problema a resolver, los métodos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)**\n",
        "\n",
        "En esta Tarea, se busca clasificar la intensidad de sentimientos de 4 sentimientos distintos, basado en un dataset de tweets ya categorizados en cada sentimiento. \n",
        "Para ello, se utiliza el siguiente algoritmo:\n",
        "- Se dividen los datos de entrenamiento en dos conjuntos, un conjunto para entrenar al clasificador, y un conjunto para probar la efectividad del clasificador. \n",
        "- Se crea un pipeline, que consiste en un preprocesador, que arregla fallas ortograficas, y agrupa parte de la información, para luego, generar un bag of words usando CountVectorizer, y usando un FeatureUnion se le agregan al bag of words conjuntos de emoji que representan ciertas emociones, y caracteres especiales que permiten agregar contexto al sistema, el resultado de esta union entre las palabras vectorizadas, los emoji, y los caracteres pasan a un clasificador, en este caso Regresión Logistica.\n",
        "- Se entrena el clasificador utilizando el train data y luego se valida con el test data, obteniendo diversos resultados. \n",
        "- Finalmente se clasifica el target set y se compara con los datos Gold en la competencia obteniendo el puntaje final.\n",
        "\n",
        "Durante el transcurso de la tarea, se iteró con distintos tipos de Vectorizadores, Lemmatizadores y Stemmers, y Clasificadores. \n",
        "\n",
        "Los Features que se mencionaron en el pipeline fueron los que terminaron siendo usados en la entrega final. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "colab_type": "text",
        "id": "AzIxVVxgOR5g"
      },
      "source": [
        "## 2. Representaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:17.719268Z",
          "start_time": "2020-03-31T13:47:17.709207Z"
        },
        "colab_type": "text",
        "id": "pFK240L5OR5i"
      },
      "source": [
        "**Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano. Mas abajo encontrarán una lista útil de estos que les podrá ser de utilidad. (1.5 puntos)** \n",
        "\n",
        "Se utilizó en conjunto al bag of words, dos contadores de caracteres, uno de emoji, en donde se clasifican distintos tipos de emoji por categoría, y por otra parte, un contador de simbolos. \n",
        "Ambos contadores entregan vectores para cada tweet y son agregadas a través de un Feature Union al Bag of Words.\n",
        "\n",
        "En tanto, en las pruebas se utilizaron diversos Vectorizadores:\n",
        "- CountVectorizer\n",
        "- TfIdfVectorizer\n",
        "- TweetTokenizer\n",
        "\n",
        "Todos estos vectorizadores fueron utilizados con sus versiones de uni, bi y trigramas y se aplicaron sobre ellos stopwords de nltk en diversos rangosjajaja.\n",
        "\n",
        "Respecto a la normalización de tokens, se utilizaron:\n",
        "- Porter Stemmer\n",
        "- Snowball Stemmer\n",
        "- WordNetLemmatizer\n",
        "- noLemmatizer\n",
        "\n",
        "También se probó usar un autocorrector, SpellChecker con nulos resultados y no se alcanzó a implementar el Mark_Negation.\n",
        "\n",
        "Por último, se agregó al pipeline previo al vectorizador, un preprocesador de palabras que se utilizó principalmente para agrupar algunos types raros por tipo, por ejemplo, los nombres de usuario, reemplazandolos por \"@USER\", y otorgando a ciertos numeros, distintos significados, a modo de agregar contexto, como lo son el tiempo, el dinero, entre otros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zS9PFJmoOR5j"
      },
      "source": [
        "## 3. Algoritmos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jly0lKzGOR5n"
      },
      "source": [
        "Describir brevemente los algoritmos de clasificación usados. (0.5 puntos)\n",
        "Dentro de los algoritmos evaluados para la realización de esta competencia, se utilizaron:\n",
        "\n",
        "*   Multinomial Naive Bayes: Teorema de Bayes aplicado al BoW, creando un modelo probabilístico según el uso de elementos del vocabulario.\n",
        "*   Logistic Regression: Modela la probabilidad de que un evento ocurra sobre otro dadas ciertas condiciones, utilizando un sigmoide. Este modelo itera sobre si mismo hasta converger en los ponderadores de cada vector. Puede aplicarse a casos multivariables.\n",
        "*   K Neighbours Classifier: Se tienen n conjuntos, con vectores iniciales aleatorios, y se asocian a distintas palabras a través de distancia euclidiana, luego, se calcula el centro de los vectores asociados de cada conjunto, reemplazando al vector inicial. Esto se itera hasta converger.\n",
        "*   Support Vector Machines: Este algoritmo busca un hiperplano que clasifica distintivamente los puntos. Se soporta en vecores \n",
        "*   Decision Tree Classifier: Estructurado en base a diferentes evaluaciones, la raíz (vector original) es dividido en ramas (subvectores) según los resultados obtenidos en estas pruebas. Es un algoritmo que busca dividir el conjunto de la mejor manera, por lo que busca parámetros que dividan de mejor forma el conjunto.\n",
        "*   Random Forest Classifier: En base a vectores aleatorios de elementos del conjunto construye varios árboles los cuales son luego promediados.\n",
        "*   Quadratic Cost Classifier: Utilizando la función de costo cuadrático, este algoritmo optimiza según un método establecido (por ejemplo gradiente).\n",
        "*   Multilayer Perceptron Classifier: Basado en redes neuronales, utiliza neuronas tipo perceptrón en capas y cantidades predefinidas para calcular los pesos de las variables. Suelen ser en dos o más capas, habiendo correlación entre las combinaciones de elementos. Los perceptrones se estructuran en base a una función de costo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "colab_type": "text",
        "id": "9EJ0v5e9OR5o"
      },
      "source": [
        "## 4. Métricas de Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RjPjp7F7OR5q"
      },
      "source": [
        "**Describir brevemente las métricas utilizadas en la evaluación indicando que miden y su interpretación. (0.5 puntos)**\n",
        "\n",
        "Primero definamos lo que es la curva ROC, ésta es una representacion grafica de la sensibilidad de un clasificador binario según se varia el umbral de discriminación, es decir, es el ratio entre verdaderos positivos frente al ratio de falsos positivos. \n",
        "\n",
        "- AUC: Del inglés 'Area Under Curve', representa el área bajo la curva ROC. Dado que la curva ROC se utiliza para calificar sistemas binarios, no es muy buena en sistemas multivariables. Se tiene que para un clasificador aleatorio, el AUC es de 0.5 sin importar la distribuición de las clases, por lo que un puntaje igual o inferior a esto significaría que el sistema no funciona correctamente.\n",
        "\n",
        "- Kappa: Es la clasificación de Accuracy pero con la excepcion de que está normalizada en la base de que se clasificara el problema aleatoriamente.\n",
        "Además, es una métrica útil para problemas con clases desbalanceadas. Soporta muy bien la multivariabilidad. Dada que esta normalizada en base a si el sistema se clasificara aleatoriamente, el minimo Kappa es de 0.\n",
        "Existe un criterio de clasificacion creado por Landis and Koch (1977), que muestra correlacion en base a cierto puntaje Kappa. < 0, indica que no existe concordancia, 0-0.2, muy poca; 0.2-0.4 adecuada, 0.4-0.6 moderada, 0.6-0.8 substancial, 0.8-1 casi perfecta a perfecta.\n",
        "\n",
        "- Accuracy: Es el porcentaje de clasificaciones correctas en todas las instancias respecto al total de instancias. Es más útil en clasificacion binaria, que en clasificacion mulivariable, pues puede volverse menos claro cómo se divide la exactitud en cada variable. Para obtener esa información, se puede usar una matriz de confusión. Esta depende de la distribuición de las palabras, por lo que puede tener un cesgo importante en caso de que exista una clase con probabilidad mayor, dado que si se elige un clasificador que solo clasifique todo el dataset en sola esa clase, se tendría una puntuación artificialmente alta. Por otra parte, en un dataset equilibrado, si se usa un clasificador aleatorio, se tiene que el Accurary es del orden del 0.5.\n",
        "\n",
        "En esta tarea, la metrica a la que le prestaremos mayor interés es el kappa, pues al ser un problema multinomial, representa el nivel de correlación entre los resultados, y nos apoyaremos en la matriz de confusión, para observar que tan alejados estan los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxnZdUfnOR5r"
      },
      "source": [
        "## 5. Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q-8g2R-1OR5s"
      },
      "source": [
        "Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partición de evaluación que deben crear (o pueden usar cross-validation). Incluyan todo el código de sus experimentos aquí. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3lQLmd8WOR5u"
      },
      "source": [
        "###Lista de Emojis\n",
        "\n",
        "(Abrir solo para inspeccionar, idealmente dejar cerrada, baja la velocidad de scrolling)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KJJPVAnPOR5v",
        "colab": {}
      },
      "source": [
        "emojiClassificationList = [\n",
        "    ('🐶','🐱','🐭','🐹','🐰','🦊','🐻','🐼','🐨','🐯','🦁','🐮','🐷','🐽','🐸','🐒','🐔','🐧','🐦','🐤',\n",
        "       '🐣','🐥','🦆','🦩','🦅','🦉','🦇','🐺','🐗','🐴','🦄','🐝','🐛','🦋','🐌','🐚','🦗','🐞','🐜','🕷',\n",
        "       '🕸','🐢','🐍','🦎','🦂','🦀','🦑','🐙','🦐','🐠','🐟','🐡','🐬','🦈','🐳','🐋','🐊','🐆','🐅','🐃',\n",
        "       '🐂','🐄','🦌','🐪','🐫','🐘','🦏','🦍','🐎','🐖','🐐','🐏','🐑','🐕','🐩','🦮','🦺','🐈','🐓',\n",
        "       '🦃','🕊','🐇','🐁','🐀','🐿','🦓','🦒','🦔','🦧','🦥','🦦','🦨','🦕','🦖','🐉','🐲'),\n",
        "    ('🐵','🙊','🙉','🙊'),\n",
        "    ('🌵', '🎄', '🌲', '🌳', '🌴', '🌱', '🌿', '☘️', '🍀', '🎍', '🎋', '🍃', '🍂', '🍁', '🍄', '🌾', '💐',\n",
        "        '🌷', '🌹', '🥀', '🌻', '🌼', '🌸', '🌺'),\n",
        "    ('😀', '😃', '😄', '😁', '😆', '🤩', '😊', '😇', '🙂', '☺️', '😉', '🤗', '🤓', '😎', '🤠', '🤙', '💪',\n",
        "             '🤞', '✌️', '🤘', '✊', '🤛', '🤜', '😺', '😸'),\n",
        "    ('😂', '🤣', '😄', '😁', '😆', '😹'),\n",
        "    ('😍', '😘', '😗', '😙', '😚', '🥰', '😻', '😽', '❤️', '🧡', '💛', '💚', '💙', '💜', '🖤', '❣️', '💕',\n",
        "        '💞', '💓', '💗', '💖', '💘', '💝', '💟', '💌'),\n",
        "    ('😌', '😴'),\n",
        "    ('🤢', '🤮', '🤧', '😷', '🤒', '🤕'),\n",
        "    ('😈', '👿', '👹', '👺', '😼'),\n",
        "    ('😤', '😠', '😡', '🤬'),\n",
        "    ('😒','😐', '😑', '🙃', '😾'),\n",
        "    ('😞', '😔', '😟', '😕', '🙁', '☹️', '😣', '😖', '😫', '😩', '😯', '😦', '😧', '😮', '😲', '😵', '😭', '😓',\n",
        "        '😪', '😾', '😿'),\n",
        "    ('💔',),\n",
        "    ('😳', '😨', '😰', '😶'),\n",
        "    ('😋', '🤪', '😜', '😝', '😛'),\n",
        "    ('🤑', '💸', '💵', '💴', '💶', '💷', '💰', '💳', '💎'),\n",
        "    ('🤨', '🧐', '🤔', '🤯', '👀', '🤫', '🤭', '🤥', '🤐', '🙄'),\n",
        "    ('🙀', '😱'),\n",
        "    ('😏', '😼', '🤤', '🍆', '🍑', '🍌', '🔩', '💦', '👅'),\n",
        "    ('🙏',),\n",
        "    ('🔥',),\n",
        "    ('🌥', '🌦', '☁️', '🌧', '⛈', '🌩', '🌨', '☃️', '⛄', '❄️', '🌬', '💨', '🌪', '🌫', '☔'),\n",
        "    ('👐', '🙌', '👏', '🤲', '🤝', '👍', '👎', '👊', '👌', '👈', '👉', '👆', '👇', '☝️', '✋', '🤚', '🖐',\n",
        "        '🖖', '👋', '🤟', '✍️', '🤳', '💅', '🖖', '👂', '\\U0001f9bb', '👃', '🦵', '🦶', '💄', '💋', '👄'),\n",
        "    ('👷', '💂',  '🕵️', '🤶', '🎅', '👸', '🤴', '👰', '🤵', '👼', '🤰', '🤱', '🙇', '💁', '🙅', '🙆', '🙋',\n",
        "        '🙎', '🙍', '💆','🕴', '👶', '👦', '👧', '🧒', '👨', '👩', '🧑','👱', '🧔', '👴', '👵', '🧓', '👲', '👳',\n",
        "        '🧕', '🚶', '🏃', '👫', '👭', '👬', '💑', '💏', '👪'),\n",
        "    ('👚', '👕', '👖', '👔', '👗', '👙', '👘', '👠', '👡', '👢', '👞', '👟', '👒', '🎩', '🎓', '👑', '⛑',\n",
        "        '🎒', '👝', '👛', '👜', '💼', '👓', '🕶', '\\U0001f93f', '🌂', '☂️', '🧣', '🧤', '🧥', '\\U0001f9ba',\n",
        "        '\\U0001f97b', '\\U0001fa71', '\\U0001fa72', '\\U0001fa73', '\\U0001fa70', '🧦', '🧢'),\n",
        "    ('\\U0001f9be', '\\U0001f9bf', '👣', '👁', '🗣', '👤', '👥', '💃', '🕺', '👯', '🥵', '🥶', '🥳', '🥴',\n",
        "        '🥺', '🦸', '🦹', '🎗', '🎫', '🎟', '🎪'),\n",
        "    ('👻', '💀', '☠️', '👽', '👾', '🤖', '🎃', '🤡'),\n",
        "    ('⚽', '🏀', '🏈', '⚾', '🎾', '🏐', '🏉', '🎱', '🏓', '🏸', '🥅', '🏒', '🏑', '🏏', '⛳', '🏹', '🎣',\n",
        "        '🥊', '🥋', '⛸', '🎿', '⛷', '🏂', '🏋️', '🤺',  '⛹️', '🏌️', '🏄', '🏊', '🚣', '🏇', '🚴', '🚵',\n",
        "        '\\U0001fa82', '🎽', '🏅', '🎖', '🥇', '🥈', '🥉', '🏆'),\n",
        "    ('🎭', '🎨', '🎬', '🎤', '🎧', '🎼', '🎹', '🥁', '🎷', '🎺', '🎸', '🎻', '\\U0001fa95'),\n",
        "    ('🍏', '🍎', '🍐', '🍊', '🍋', '🍉', '🍇', '🍓', '🍈', '🍒', '🍍', '🥝', '🥑', '🍅', '🥒', '🥕', '🌽',\n",
        "        '🌶', '\\U0001f9c4', '\\U0001f9c5', '🥔', '🍠', '🌰', '🥜', '🍯', '🥐', '🍞', '🥖', '🧀', '🥚', '🍳',\n",
        "        '🥓', '🥞', '🍤', '🍗', '🍖', '🍕', '🌭', '🍔', '🍟', '🥙', '🌮', '🌯', '🥗', '🥘', '🍝', '🍜', '🍲',\n",
        "        '🍥', '🍣', '🍱', '🍛', '🍚', '🍙', '🍘', '🍢', '🍡', '🍧', '🍨', '🍦', '🍰', '🎂', '🍮', '🍭', '🍬',\n",
        "        '🍫', '🍿', '🍩', '🍪', '🥛', '🍼', '☕', '🍵', '🍶', '🥄', '🍴', '🍽', '🥥', '🥨', '🥩', '🥪', '🥣',\n",
        "        '🥫', '\\U0001f9c7', '\\U0001f9c6', '\\U0001f9c8', '🥟', '🥠', '🥡', '🥧', '🥤', '🥢', '\\U0001f9c3',\n",
        "        '\\U0001f9c9'),\n",
        "    ('♈', '♉', '♊', '♋', '♌', '♍', '♎', '♏', '♐', '♑', '♒', '♓', '☮️', '✝️', '☪️', '🕉', '☸️', '✡️', '🔯',\n",
        "        '🕎', '☯️', '☦️', '🛐', '⛎', '🆔', '⚛️'),\n",
        "    ('🚗', '🚕', '🚙', '🚌', '🚎', '🏎', '🚓', '🚑', '🚒', '🚐', '🚚', '🚛', '🚜', '🛴', '🚲', '🛵',\n",
        "        '\\U0001f6fa', '🏍', '\\U0001f9bd', '\\U0001f9bc', '🚨', '🚔', '🚍', '🚘', '🚖', '🚡', '🚠', '🚟',\n",
        "        '🚃', '🚋', '🚞', '🚝', '🚄', '🚅', '🚈', '🚂', '🚆', '🚇', '🚊', '🚉', '🚁', '🛩', '✈️', '🛫', '🛬',\n",
        "        '🚀', '🛰', '💺', '🛶', '⛵', '🛥', '🚤', '🛳', '⛴', '🚢', '⚓', '🚧', '⛽', '🚏', '🚦', '🚥',\n",
        "        '🗺', '🗿', '🗽', '⛲', '🗼', '🏰', '🏯', '🏟', '🎡', '🎢', '🎠', '⛱', '🏖', '🏝', '⛰', '🏔',\n",
        "        '🗻', '🌋', '🏜', '🏕', '⛺', '🛤', '🛣', '🏗', '🏭', '🏠', '🏡', '🏘', '🏚', '🏢', '🏬', '🏣',\n",
        "        '🏤', '🏥', '🏦', '🏨', '🏪', '🏫', '🏩', '💒', '🏛', '⛪', '🕌', '🕍', '🕋', '\\U0001f6d5', '⛩',\n",
        "        '🗾', '🎑', '🏞', '🌅', '🌄', '🌠', '🎇', '🎆', '🌇', '🌆', '🏙', '🌃', '🌌', '🌉', '🌁', '🛸'),\n",
        "    ('🎁', '🎈', '🎏', '🎀', '🎊', '🎉', '🎎', '🏮'),\n",
        "    ('🕐', '🕑', '🕒', '🕓', '🕔', '🕕', '🕖', '🕗', '🕘', '🕙', '🕚', '🕛', '🕜', '🕝', '🕞', '🕟', '🕠',\n",
        "        '🕡', '🕢', '🕣', '🕤', '🕥', '🕦', '🕧', '⏱', '⏲', '⏰', '🗓', '📆', '📅'),\n",
        "    ('♠️', '♣️', '♥️', '♦️', '🃏', '🎴', '🀄', '🎲', '🎯', '🎳', '🎮', '🎰', '🛷', '🥌', '\\U0001fa80',\n",
        "        '\\U0001fa81'),\n",
        "    ('🔫', '💣', '🔪', '🗡', '⚔️', '\\U0001fa93', '⌚', '\\U0001fa78', '⚰️', '⚱️', '🏺', '🔧', '🔨', '⚒',\n",
        "        '🛠', '⛏', '⌛', '⏳', '✂️'),\n",
        "    ('🉑', '☢️', '☣️', '📴', '📳', '🈶', '🈚', '🈸', '🈺', '🈷️', '✴️', '🆚', '💮', '🉐', '㊙️', '㊗️', '🈴',\n",
        "        '🈵', '🈹', '🈲', '🅰️', '🅱️', '🆎', '🆑', '🅾️', '🆘', '❌', '⭕', '🛑', '⛔', '📛', '🚫', '💯', '💢',\n",
        "        '♨️', '🚷', '🚯', '🚳', '🚱', '🔞', '📵', '🚭', '❗', '❕', '❓', '❔', '‼️', '⁉️', '🔅', '🔆', '〽️',\n",
        "        '⚠️', '🚸', '🔱', '⚜️', '🔰', '♻️', '✅', '🈯', '💹', '❇️', '✳️', '❎', '🌐', '💠', 'Ⓜ️', '🌀', '💤', '🏧',\n",
        "        '🚾', '♿', '🅿️', '🈳', '🈂️', '🛂', '🛃', '🛄', '🛅', '🚹', '🚺', '🚼', '🚻', '🚮', '🎦', '📶', '🈁',\n",
        "        '🔣', 'ℹ️', '🔤', '🔡', '🔠', '🆖', '🆗', '🆙', '🆒', '🆕', '🆓', '0️⃣', '1️⃣', '2️⃣', '3️⃣', '4️⃣', '5️⃣',\n",
        "        '6️⃣', '7️⃣', '8️⃣', '9️⃣', '🔟', '🔢', '#️⃣', '*️⃣', '▶️', '⏸', '⏯', '⏹', '⏺', '⏭', '⏮', '⏩', '⏪', '⏫',\n",
        "        '⏬', '◀️', '🔼', '🔽', '➡️', '⬅️', '⬆️', '⬇️', '↗️', '↘️', '↙️', '↖️', '↕️', '↔️', '↪️', '↩️', '⤴️', '⤵️',\n",
        "        '🔀', '🔁', '🔂', '🔄', '🔃', '🎵', '🎶', '➕', '➖', '➗', '✖️', '💲', '💱', '™️', '©️', '®️', '〰️',\n",
        "        '➰', '➿', '🔚', '🔙', '🔛', '🔝', '✔️', '☑️', '🔘', '🔴', '\\U0001f7e0', '\\U0001f7e1', '\\U0001f7e2',\n",
        "        '🔵', '\\U0001f7e3', '⚫', '⚪', '\\U0001f7e4', '🔺', '🔻', '🔸', '🔹', '🔶', '🔷', '🔳', '🔲', '▪️',\n",
        "        '▫️', '◾', '◽', '◼️', '◻️', '⬛', '⬜', '\\U0001f7e5', '\\U0001f7e7', '\\U0001f7e8', '\\U0001f7e9',\n",
        "        '\\U0001f7e6', '\\U0001f7ea', '\\U0001f7eb', '🔈', '🔇', '🔉', '🔊', '🔔', '🔕', '📣', '📢', '💬',\n",
        "        '💭', '🗯', '⏏', '♀', '♂', '⚕', '♾️'),\n",
        "    ('🍺', '🍻', '🥂', '🍷', '🥃', '🍸', '🍹', '🍾', '🧊', '💊', '💉', '🚬'),\n",
        "    ('🏴', '🇦🇫', '🇦🇽', '🇦🇱', '🇩🇿', '🇦🇸', '🇦🇩', '🇦🇴', '🇦🇮', '🇦🇶', '🇦🇬', '🇦🇷', '🇦🇲', '🇦🇼', '🇦🇨', '🇦🇺',\n",
        "        '🇦🇹', '🇦🇿', '🇧🇸', '🇧🇭', '🇧🇩', '🇧🇧', '🇧🇾', '🇧🇪', '🇧🇿', '🇧🇯', '🇧🇲', '🇧🇹', '🇧🇴', '🇧🇦', '🇧🇼', '🇧🇻',\n",
        "        '🇧🇷', '🇮🇴', '🇻🇬', '🇧🇳', '🇧🇬', '🇧🇫', '🇧🇮', '🇰🇭', '🇨🇲', '🇨🇦', '🇮🇨', '🇨🇻', '🇧🇶', '🇰🇾', '🇨🇫', '🇪🇦',\n",
        "        '🇹🇩', '🇨🇱', '🇨🇳', '🇨🇽', '🇨🇵', '🇨🇨', '🇨🇴', '🇰🇲', '🇨🇬', '🇨🇩', '🇨🇰', '🇨🇷', '🇨🇮', '🇭🇷', '🇨🇺', '🇨🇼',\n",
        "        '🇨🇾', '🇨🇿', '🇩🇰', '🇩🇬', '🇩🇯', '🇩🇲', '🇩🇴', '🇪🇨', '🇪🇬', '🇸🇻', '🇬🇶', '🇪🇷', '🇪🇪', '🇪🇹', '🇪🇺', '🇫🇰',\n",
        "        '🇫🇴', '🇫🇯', '🇫🇮', '🇫🇷', '🇬🇫', '🇵🇫', '🇹🇫', '🇬🇦', '🇬🇲', '🇬🇪', '🇩🇪', '🇬🇭', '🇬🇮', '🇬🇷', '🇬🇱', '🇬🇩',\n",
        "        '🇬🇵', '🇬🇺', '🇬🇹', '🇬🇬', '🇬🇳', '🇬🇼', '🇬🇾', '🇭🇹', '🇭🇲', '🇭🇳', '🇭🇰', '🇭🇺', '🇮🇸', '🇮🇳', '🇮🇩', '🇮🇷',\n",
        "        '🇮🇶', '🇮🇪', '🇮🇲', '🇮🇱', '🇮🇹', '🇯🇲', '🇯🇵', '🇯🇪', '🇯🇴', '🇰🇿', '🇰🇪', '🇰🇮', '🇽🇰', '🇰🇼', '🇰🇬', '🇱🇦',\n",
        "        '🇱🇻', '🇱🇧', '🇱🇸', '🇱🇷', '🇱🇾', '🇱🇮', '🇱🇹', '🇱🇺', '🇲🇴', '🇲🇰', '🇲🇬', '🇲🇼', '🇲🇾', '🇲🇻', '🇲🇱', '🇲🇹',\n",
        "        '🇲🇭', '🇲🇶', '🇲🇷', '🇲🇺', '🇾🇹', '🇲🇽', '🇫🇲', '🇲🇩', '🇲🇨', '🇲🇳', '🇲🇪', '🇲🇸', '🇲🇦', '🇲🇿', '🇲🇲', '🇳🇦',\n",
        "        '🇳🇷', '🇳🇵', '🇳🇱', '🇳🇨', '🇳🇿', '🇳🇮', '🇳🇪', '🇳🇬', '🇳🇺', '🇳🇫', '🇲🇵', '🇰🇵', '🇳🇴', '🇴🇲', '🇵🇰', '🇵🇼',\n",
        "        '🇵🇸', '🇵🇦', '🇵🇬', '🇵🇾', '🇵🇪', '🇵🇭', '🇵🇳', '🇵🇱', '🇵🇹', '🇵🇷', '🇶🇦', '🇷🇪', '🇷🇴', '🇷🇺', '🇷🇼', '🇼🇸',\n",
        "        '🇸🇲', '🇸🇹', '🇸🇦', '🇸🇳', '🇷🇸', '🇸🇨', '🇸🇱', '🇸🇬', '🇸🇽', '🇸🇰', '🇸🇮', '🇸🇧', '🇸🇴', '🇿🇦', '🇬🇸', '🇰🇷',\n",
        "        '🇸🇸', '🇪🇸', '🇱🇰', '🇧🇱', '🇸🇭', '🇰🇳', '🇱🇨', '🇲🇫', '🇵🇲', '🇻🇨', '🇸🇩', '🇸🇷', '🇸🇯', '🇸🇿', '🇸🇪', '🇨🇭',\n",
        "        '🇸🇾', '🇹🇼', '🇹🇯', '🇹🇿', '🇹🇭', '🇹🇱', '🇹🇬', '🇹🇰', '🇹🇴', '🇹🇹', '🇹🇦', '🇹🇳', '🇹🇷', '🇹🇲', '🇹🇨', '🇹🇻',\n",
        "        '🇺🇬', '🇺🇦', '🇦🇪', '🇬🇧', '🇺🇸', '🇺🇾', '🇺🇲', '🇺🇳', '🇻🇮', '🇺🇿', '🇻🇺', '🇻🇦', '🇻🇪', '🇻🇳', '🇼🇫', '🇪🇭',\n",
        "        '🇾🇪', '🇿🇲', '🇿🇼', '🏴‍☠️'),\n",
        "    ('📱', '📲', '💻', '⌨️', '🖥', '🖨', '🖱', '🖲', '🕹', '🗜', '💽', '💾', '💿', '📀', '📼', '📷',\n",
        "        '📸', '📹', '🎥', '📽', '🎞', '📞', '☎️', '📟', '📠', '📺', '📻', '🎙', '🎚', '🎛', '🕰', '📡',\n",
        "        '🔋', '🔌', '💡', '🔦', '🕯', '🗑', '🛢', '⚖️', '⚙️', '⛓', '\\U0001f9af', '🛡', '🔮', '📿', '💈',\n",
        "        '⚗️', '🔭', '🔬', '🕳', '\\U0001fa79', '\\U0001fa7a', '🌡', '\\U0001fa92', '🚽', '🚰', '🚿', '🛁',\n",
        "        '🛀', '🛎', '🔑', '🗝', '🚪', '\\U0001fa91', '🛋', '🛏', '🛌', '🖼', '🛍', '🛒', '🎐', '✉️',\n",
        "        '📩', '📨', '📧', '📥', '📤', '📦', '🏷', '📪', '📫', '📬', '📭', '📮', '📯', '📜', '📃', '📄',\n",
        "        '📑', '📊', '📈', '📉', '🗒', '📇', '🗃', '🗳', '🗄', '📋', '📁', '📂', '🗂', '🗞', '📰', '📓',\n",
        "        '📔', '📒', '📕', '📗', '📘', '📙', '📚', '📖', '🔖', '🔗', '📎', '🖇', '📐', '📏', '📌', '📍',\n",
        "        '📌', '🎌', '🏳️', '🏴', '🏁', '\\U0001fa94', '🖊', '🖋', '✒️', '🖌', '🖍', '📝', '✏️', '🔍', '🔎',\n",
        "        '🔏', '🔐', '🔒', '🔓'),\n",
        "    ('🌈',),\n",
        "    ('🌎', '🌍', '🌏', '🌕', '🌖', '🌗', '🌘', '🌑', '🌒', '🌓', '🌔', '🌞', '🌛', '🌜', '🌙',\n",
        "        '\\U0001fa90', '💫', '⭐', '🌟', '✨', '⚡', '💥', '☄️', '☀️', '🌤', '⛅', '🌊', '💧')\n",
        "                  \n",
        "    ]\n",
        "\n",
        "# Los deje al inicio asi cuando hago scroll no se laguea mi pantalla jajaj."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:31:40.023344Z",
          "start_time": "2020-03-31T13:31:40.003541Z"
        },
        "colab_type": "text",
        "id": "WtJr2bdNOR52"
      },
      "source": [
        "### Importar librerías y utiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "houUEut6OR53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c3582af2-1efa-40a4-b15d-00868f59f83b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sVU8g6BrZ465",
        "colab": {}
      },
      "source": [
        "def noLemmatizer():\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.587160Z",
          "start_time": "2020-04-07T15:44:19.319386Z"
        },
        "colab_type": "code",
        "id": "FWoBKYn6OR6B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "87ecdf26-58b0-466a-920d-87a85f95d85d"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "# Tokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk import TweetTokenizer\n",
        "\n",
        "# StopWords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Lemmatizer\n",
        "from nltk.stem import  WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Functions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# BEGIN_QUOTE (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# END_QUOTE\n",
        "\n",
        "\n",
        "# Mark Negation\n",
        "from nltk.sentiment.util import mark_negation\n",
        "\n",
        "# Other\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "Porter = nltk.stem.PorterStemmer()\n",
        "Snowball = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "stp_wrds = list(stopwords.words('english')) # nltk stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wuQLUX6kiJi1",
        "colab": {}
      },
      "source": [
        "# Funciones Base:\n",
        "\n",
        "def isLemmatizer(Lemma):\n",
        "    if hasattr(Lemma, 'lemmatize'):\n",
        "        return True\n",
        "\n",
        "\n",
        "def isStemmer(Stemm):\n",
        "    if hasattr(Stemm, 'stem'):\n",
        "        return True\n",
        "\n",
        "\n",
        "def noNormalizer(): # Simplemente devuelve la misma palabra, sin normalizar los token.\n",
        "    pass\n",
        "\n",
        "Porter = nltk.stem.PorterStemmer()\n",
        "Snowball = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "#Tknzr = nltk.tokenize.TweetTokenizer() # No funciona, problemas de compatibilidad entre nltk y sklearn."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvf3mJ__jGfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Vectorizers = [CountVectorizer(),CountVectorizer(ngram_range=(1, 2)),CountVectorizer(ngram_range=(1, 3)),TfidfVectorizer(),TfidfVectorizer(ngram_range=(1, 2)),TfidfVectorizer(ngram_range=(1, 3)),\n",
        "               CountVectorizer(stop_words=stp_wrds),CountVectorizer(ngram_range=(1, 2),stop_words=stp_wrds),CountVectorizer(ngram_range=(1, 3),stop_words=stp_wrds),TfidfVectorizer(stop_words=stp_wrds),\n",
        "               TfidfVectorizer(ngram_range=(1, 2),stop_words=stp_wrds),TfidfVectorizer(ngram_range=(1, 3),stop_words=stp_wrds)]\n",
        "\n",
        "# En orden de vectorizadores, unigramas, uni y bigramas, uni, bi y trigramas. \n",
        "# Luego lo mismo pero con las stopwords de nltk.\n",
        "\n",
        "FClassifiers = [\n",
        "                    LogisticRegression(n_jobs=-1), \n",
        "                    MultinomialNB(), # Baseline.\n",
        "# BEGIN_QUOTE (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
        "                    KNeighborsClassifier(6),\n",
        "                    SVC(kernel=\"linear\", C=0.025, probability=True),\n",
        "                    SVC(gamma=2, C=1, probability=True),\n",
        "                    DecisionTreeClassifier(max_depth=5),\n",
        "                    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "                    AdaBoostClassifier(),\n",
        "                    MLPClassifier(hidden_layer_sizes=(50),early_stopping=True,max_iter=1000),\n",
        "                    MLPClassifier(hidden_layer_sizes=(100),early_stopping=True,max_iter=1000)\n",
        "# END_QUOTE\n",
        "] \n",
        "\n",
        "Normalizers = [WordNetLemmatizer(), Porter, Snowball, noNormalizer()] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rdgxo2VCOR6J"
      },
      "source": [
        "### Definir métodos de evaluación\n",
        "\n",
        "Estas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "colab_type": "code",
        "id": "4xl1bWBmOR6K",
        "colab": {}
      },
      "source": [
        "def auc_score(test_set, predicted_set):\n",
        "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
        "    medium_predicted = np.array(\n",
        "        [prediction[1] for prediction in predicted_set])\n",
        "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
        "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
        "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
        "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
        "    auc_high = roc_auc_score(high_test, high_predicted)\n",
        "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
        "    auc_low = roc_auc_score(low_test, low_predicted)\n",
        "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
        "             high_test.sum() * auc_high) / (\n",
        "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
        "    return auc_w\n",
        "\n",
        "def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    print(\n",
        "        confusion_matrix(y_test,\n",
        "                         predicted_labels,\n",
        "                         labels=['low', 'medium', 'high']))\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(\n",
        "        classification_report(y_test,\n",
        "                              predicted_labels,\n",
        "                              labels=['low', 'medium', 'high']))\n",
        "#     Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])\n",
        "\n",
        "# Copiamos la funcion anterior y comentamos todos los print, para hacer las pruebas\n",
        "# masivas obteniendo los resultados más relevantes.\n",
        "\n",
        "def evaulate_class(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "    #print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    #print(\n",
        "    #    confusion_matrix(y_test,\n",
        "    #                     predicted_labels,\n",
        "    #                     labels=['low', 'medium', 'high']))\n",
        "\n",
        "    #print('\\nClassification Report:\\n')\n",
        "    #print(\n",
        "    #    classification_report(y_test,\n",
        "    #                          predicted_labels,\n",
        "    #                          labels=['low', 'medium', 'high']))\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    #print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    #print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    #print(\"Accuracy:\", accuracy)\n",
        "    #print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YrsqpUWCOR6U"
      },
      "source": [
        "### Datos\n",
        "\n",
        "Obtener los datasets desde el github del curso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "colab_type": "code",
        "id": "7-zqcg8YOR6X",
        "colab": {}
      },
      "source": [
        "# Datasets de entrenamiento.\n",
        "train = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
        "}\n",
        "# Datasets que deberán predecir para la competencia.\n",
        "target = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "colab_type": "code",
        "id": "v4SUr4g0OR6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "14ddedf7-7373-4d66-f623-4b1dc4e47996"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias:\n",
        "train['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>10152</td>\n",
              "      <td>@RobertTaitWHU against Chelsea anything is pos...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>10350</td>\n",
              "      <td>#disgracefulesin I resent all men in some way;...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>707</th>\n",
              "      <td>10707</td>\n",
              "      <td>@l1ght__eyes u tried boiling em takes years too</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>10443</td>\n",
              "      <td>Indignation: [whispers to date during that ter...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>10197</td>\n",
              "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... sentiment_intensity\n",
              "152  10152  ...              medium\n",
              "350  10350  ...              medium\n",
              "707  10707  ...              medium\n",
              "443  10443  ...              medium\n",
              "197  10197  ...              medium\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IBOSYWxxOR6o"
      },
      "source": [
        "### Analizar los datos \n",
        "\n",
        "Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento. Noten que las clases están desbalanceadas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.117633Z",
          "start_time": "2020-04-07T15:44:21.090703Z"
        },
        "colab_type": "code",
        "id": "W4nQkKsjOR6t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "fb5e5cd8-2664-4700-8f73-4f58b483f6ed"
      },
      "source": [
        "def get_group_dist(group_name, train):\n",
        "    print(group_name, \"\\n\",\n",
        "          train[group_name].groupby('sentiment_intensity').count(),\n",
        "          '\\n---------------------------------------\\n')\n",
        "for dataset_name in train:\n",
        "    get_group_dist(dataset_name, train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anger \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 163    163    163\n",
            "low                  161    161    161\n",
            "medium               617    617    617 \n",
            "---------------------------------------\n",
            "\n",
            "fear \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 270    270    270\n",
            "low                  288    288    288\n",
            "medium               699    699    699 \n",
            "---------------------------------------\n",
            "\n",
            "joy \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 195    195    195\n",
            "low                  219    219    219\n",
            "medium               488    488    488 \n",
            "---------------------------------------\n",
            "\n",
            "sadness \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 197    197    197\n",
            "low                  210    210    210\n",
            "medium               453    453    453 \n",
            "---------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qakI2e_EUcBq"
      },
      "source": [
        "###Preprocesador: \n",
        "Se nos ocurrió la idea de preparar el texto previo a clasificar, para unificar ciertas fuentes de información, como algunos simbolos, si es algo relacionado con el tiempo, con el dinero, si es un usuario, o si es un numero.\n",
        "\n",
        "Se intentó arreglar sin exito el hecho que existian saltos de linea (\\n).\n",
        "\n",
        "Se usó en un momento el pyspellchecker, pero no dió buenos resultados.\n",
        "\n",
        "Por ultimo, se utilizó un Stemmer sobre las palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SxOSEeaeUbIH",
        "colab": {}
      },
      "source": [
        "def ch_user(tweet):\n",
        "    words = tweet.split()\n",
        "    n = len(words)\n",
        "    symb = ['@','#','!','?','.',',','*', '\\'', '\\\"', 'k', 'M', 'K', 'm', '-', '+']\n",
        "    money = ['$', '€', '£', '¥', 'USD', 'US$', 'k', 'M', 'K', 'm', 'c', '¢', '.', ',', '!', '?', 'th', '\\'']\n",
        "    time = ['am', 'pm', 'yrs', 'y', 'h', 'm', 'hrs', 'min', 's', '\\'', '/', ':']\n",
        "    for i in range(n):\n",
        "        words[i] = ' '.join(words[i].split('\\n'))\n",
        "        if words[i] == '&amp;':\n",
        "            words[i] = '&'\n",
        "        if words[i][0] == '@':\n",
        "            words[i] = '@'\n",
        "        for w in symb:\n",
        "            numb = words[i].replace(w, '')\n",
        "        if (numb.isdigit()):\n",
        "            words[i] = 'ª'\n",
        "        for m in money:\n",
        "            mon = words[i].replace(m, '')\n",
        "        if (mon.isdigit()):\n",
        "            words[i] = '$'\n",
        "        for t in time:\n",
        "            tim = words[i].replace(t, '')\n",
        "        if (tim.isdigit()):\n",
        "            words[i] = 'TIME'\n",
        "        if isStemmer(normal):\n",
        "            words[i] = normal.stem(words[i])\n",
        "        if isLemmatizer(normal):\n",
        "            words[i] = normal.lemmatize(words[i])\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Preprocesador:\n",
        "\n",
        "class Preproci(BaseEstimator, TransformerMixin):\n",
        "    # Se cambian todos los username por 'username', con el propósito de disminuir la sparseness.\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(ch_user(tweet))\n",
        "        return pd.Series(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QkcqLzhZOR62"
      },
      "source": [
        "### Custom Features \n",
        "\n",
        "Se modificó el CharsCountTransformer inicial y se agregaron más simbolos\n",
        "\n",
        "Por otra parte, fue creada una lista con distintos subtipos de emoji, la cual se encuentra más arriba y se contó cada vez que un emoji pertenecía a cierto grupo. \n",
        "\n",
        "Con eso se agrupan tipos similares de emoji y se disminuye la sparseness respecto a un método que solo cuente todos emoji. \n",
        "(41 types vs >1500 types)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.128600Z",
          "start_time": "2020-04-07T15:44:21.119624Z"
        },
        "colab_type": "code",
        "id": "eiDRBGxKOR64",
        "colab": {}
      },
      "source": [
        "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        num_hashtags = tweet.count('#')\n",
        "        num_exclamations = tweet.count('!')\n",
        "        num_interrogations = tweet.count('?')\n",
        "        num_ats = tweet.count('@')\n",
        "        num_mon = tweet.count('$')\n",
        "        num_num = tweet.count('ª')\n",
        "        num_tim = tweet.count('TIME')\n",
        "        num_and = tweet.count('&')\n",
        "        return [num_hashtags, num_exclamations, num_interrogations, num_ats, num_mon, num_num, num_tim, num_and]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class EmojiCountTransformer(BaseEstimator, TransformerMixin): # Se agrega un contador de categorias de emoji.\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        L = []\n",
        "        for i in emojiClassificationList: \n",
        "            num=0\n",
        "            for j in i:\n",
        "                num += tweet.count(j)\n",
        "            L.append(num)\n",
        "        return L\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.145564Z",
          "start_time": "2020-04-07T15:44:21.131593Z"
        },
        "colab_type": "code",
        "id": "plzsHgzoOR7B",
        "colab": {}
      },
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train['anger'].sample(5).tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1uVxlz2C2vEL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "231b0d35-38f1-41c7-9e54-6f1c84c34405"
      },
      "source": [
        "pd.DataFrame(zip(sample, CharsCountTransformer().transform(sample)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@oscar_perria a multimillionaire spoiled brat ...</td>\n",
              "      <td>[0, 1, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#Awareness seek #shelter .#Letgo Old #habit of...</td>\n",
              "      <td>[12, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the girl sitting in front of me is chewing her...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When anger rises, think of the consequences. #...</td>\n",
              "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@judahandthelion TONIGHT. Legit can't wait to ...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                          1\n",
              "0  @oscar_perria a multimillionaire spoiled brat ...   [0, 1, 0, 1, 0, 0, 0, 0]\n",
              "1  #Awareness seek #shelter .#Letgo Old #habit of...  [12, 0, 0, 0, 0, 0, 0, 0]\n",
              "2  the girl sitting in front of me is chewing her...   [0, 0, 0, 0, 0, 0, 0, 1]\n",
              "3  When anger rises, think of the consequences. #...   [3, 0, 0, 0, 0, 0, 0, 0]\n",
              "4  @judahandthelion TONIGHT. Legit can't wait to ...   [0, 0, 0, 1, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eHGQI2VI2mWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48e41ab6-7d75-44c5-e53c-80c203868e6d"
      },
      "source": [
        "pd.DataFrame(zip(sample, EmojiCountTransformer().transform(sample)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@oscar_perria a multimillionaire spoiled brat ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#Awareness seek #shelter .#Letgo Old #habit of...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the girl sitting in front of me is chewing her...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When anger rises, think of the consequences. #...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@judahandthelion TONIGHT. Legit can't wait to ...</td>\n",
              "      <td>[0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                                                  1\n",
              "0  @oscar_perria a multimillionaire spoiled brat ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1  #Awareness seek #shelter .#Letgo Old #habit of...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2  the girl sitting in front of me is chewing her...  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3  When anger rises, think of the consequences. #...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4  @judahandthelion TONIGHT. Legit can't wait to ...  [0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r7lFiDPqOR7Q"
      },
      "source": [
        "### Definir la representación y el clasificador\n",
        "\n",
        "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n",
        "\n",
        "El pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n",
        "\n",
        "```python\n",
        "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
        "```\n",
        "\n",
        "\n",
        "Ahora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
        "\n",
        "```python\n",
        "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
        "                                        ('chars_count',CharsCountTransformer())])),\n",
        "              ('clf', MultinomialNB())])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ANzhBruLOR7R"
      },
      "source": [
        "Recuerden que cada pipeline representa un sistema de clasificación distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podrían solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IGcdNvLOOR7W"
      },
      "source": [
        "### Ejecutar el pipeline para algún dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.167498Z",
          "start_time": "2020-04-07T15:44:21.157540Z"
        },
        "colab_type": "code",
        "id": "Jo6h2FjTOR7X",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "def run(dataset, dataset_name, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        random_state=42069,\n",
        "        test_size=0.33)\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "\n",
        "    # Evaluamos:\n",
        "    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores\n",
        "\n",
        "# La funcion anterior se copia para poder realizar las pruebas masivas. \n",
        "# La diferencia con la anterior es la funcion de evaluación, ésta no imprime nada.\n",
        "# Solo retorna los scores.\n",
        "\n",
        "def run_class(dataset, dataset_name, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        random_state=42069,\n",
        "        test_size=0.33)\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "\n",
        "    # Evaluamos:\n",
        "    scores = evaulate_class(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zuN1ZE0fOR7e"
      },
      "source": [
        "### Ejecutar el sistema creado por cada train set\n",
        "\n",
        "Este código crea y entrena los 4 sistemas de clasificación y luego los evalua para todas las combinaciones posibles entre clasificadores, vectorizadores y normalizadores de palabras. \n",
        "\n",
        "La primera celda corre una version modificada del método de evaluación, el cual no imprime scores individualmente, sino que se centra en el Kappa mayor de cada clasificador y bajo que condiciones se logra éste.\n",
        "\n",
        "Si llegan a ejecutar esto, toma tiempo, en colab solo el primer clasificador toma 145 segundos en terminar de ejecutar todas las permutaciones.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.384119Z",
          "start_time": "2020-04-07T15:44:21.170488Z"
        },
        "colab_type": "code",
        "id": "CmddOSuaOR7f",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f927874-e5e0-4f02-a7eb-b40eb178d200"
      },
      "source": [
        "#Utilizaremos el kappa más grande como referencia de lo mejor que podemos conseguir con esto.\n",
        "\n",
        "# Iteramos sobre cada combinacion posible.\n",
        "\n",
        "import time # Mediremos cuanto toma en clasificar todas las iteraciones por Clasificador.\n",
        "\n",
        "for FClassifier in FClassifiers:\n",
        "  start_time = time.time()\n",
        "  max_score=0\n",
        "  for vector in Vectorizers:\n",
        "    for normal in Normalizers:\n",
        "      classifiers = []\n",
        "      learned_labels_array = []\n",
        "      scores_array = []\n",
        "      #print(FClassifier, vector, normal)\n",
        "      for dataset_name, dataset in train.items(): # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "          # creamos el pipeline\n",
        "          pipeline = Pipeline([('preprocessor', Preproci()),\n",
        "                           ('features',\n",
        "                            FeatureUnion([('bow', vector),\n",
        "                                          ('chars_count', CharsCountTransformer()),\n",
        "                                          ('emoji_count', EmojiCountTransformer()) # Se puede agregar el EmojiCountTransformer() al Feature Union.\n",
        "                                          ])), ('clf', FClassifier)])  \n",
        "\n",
        "          # ejecutamos el pipeline sobre el dataset\n",
        "          #classifier, learned_labels, scores = run(dataset, dataset_name, pipeline) # Más detallado\n",
        "          classifier, learned_labels, scores = run_class(dataset, dataset_name, pipeline) \n",
        "\n",
        "          # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "          # classifiers.append(classifier), en esta sección no guardaremos los clasificadores, \n",
        "          # y simplemente veremos cómo se comportan respecto al set de validación.\n",
        "\n",
        "          # guardamos las labels aprendidas por el clasificador\n",
        "          learned_labels_array.append(learned_labels)\n",
        "\n",
        "          # guardamos los scores obtenidos\n",
        "          scores_array.append(scores)\n",
        "\n",
        "      # print avg scores\n",
        "      if np.array(scores_array).mean(axis=0)[1] > max_score:\n",
        "        max_score = np.array(scores_array).mean(axis=0)[1]\n",
        "        u = str(FClassifier)+str(vector)+str(normal)\n",
        "\n",
        "      #print(\n",
        "      #    \"Average scores:\\n\\n\",\n",
        "      #    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "      #    .format(*np.array(scores_array).mean(axis=0)))\n",
        "      #print()\n",
        "      #print('***************************************************************************')\n",
        "      #print()\n",
        "  print(\"Kappa = \",max_score, \"Usando:\", u)\n",
        "  # Obtenemos el máximo score para cada clasificador, para eso, retornamos el \n",
        "  # kappa promedio de la combinación más ventajosa.\n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kappa =  0.31575 Usando: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 144.57175636291504 seconds ---\n",
            "Kappa =  0.24574999999999997 Usando: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<PorterStemmer>\n",
            "--- 89.78056812286377 seconds ---\n",
            "Kappa =  0.165 Usando: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
            "                     weights='uniform')CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 89.95281291007996 seconds ---\n",
            "Kappa =  0.1905 Usando: SVC(C=0.025, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 203.32903122901917 seconds ---\n",
            "Kappa =  0.055 Usando: SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=2, kernel='rbf', max_iter=-1,\n",
            "    probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)<PorterStemmer>\n",
            "--- 221.20453476905823 seconds ---\n",
            "Kappa =  0.17 Usando: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use_idf=True, vocabulary=None)None\n",
            "--- 92.73153138160706 seconds ---\n",
            "Kappa =  0 Usando: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use_idf=True, vocabulary=None)None\n",
            "--- 92.46348977088928 seconds ---\n",
            "Kappa =  0.1825 Usando: AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
            "                   n_estimators=50, random_state=None)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)None\n",
            "--- 182.43755173683167 seconds ---\n",
            "Kappa =  0.21475 Usando: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
            "              hidden_layer_sizes=50, learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)None\n",
            "--- 321.00750637054443 seconds ---\n",
            "Kappa =  0.207 Usando: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
            "              hidden_layer_sizes=100, learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 463.3171536922455 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtiSZBQsOBoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f752488-a6ce-49c2-e549-2a342d3db6fb"
      },
      "source": [
        "#Usemos los resultados anteriores para utilizar el mejor clasificador de los anteriores. \n",
        "classifiers = []\n",
        "learned_labels_array = []\n",
        "scores_array = []\n",
        "\n",
        "normal = Snowball\n",
        "for dataset_name, dataset in train.items(): # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  # creamos el pipeline\n",
        "  pipeline = Pipeline([('preprocessor', Preproci()),\n",
        "                   ('features',\n",
        "                    FeatureUnion([('bow', CountVectorizer()),\n",
        "                                  ('chars_count', CharsCountTransformer()),\n",
        "                                  ('emoji_count', EmojiCountTransformer())\n",
        "                                  # Se puede agregar el EmojiCountTransformer() al Feature Union.\n",
        "                                  ])), ('clf', LogisticRegression())])  \n",
        "\n",
        "  # ejecutamos el pipeline sobre el dataset\n",
        "  classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
        "\n",
        "  # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "  classifiers.append(classifier)\n",
        "\n",
        "  # guardamos las labels aprendidas por el clasificador\n",
        "  learned_labels_array.append(learned_labels)\n",
        "\n",
        "  # guardamos los scores obtenidos\n",
        "  scores_array.append(scores)\n",
        "print(\n",
        "      \"Average scores:\\n\\n\",\n",
        "      \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "      .format(*np.array(scores_array).mean(axis=0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  41   3]\n",
            " [ 15 181   7]\n",
            " [  1  27  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.41      0.20      0.27        55\n",
            "      medium       0.73      0.89      0.80       203\n",
            "        high       0.71      0.47      0.57        53\n",
            "\n",
            "    accuracy                           0.70       311\n",
            "   macro avg       0.62      0.52      0.55       311\n",
            "weighted avg       0.67      0.70      0.67       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.715\tKappa: 0.318\tAccuracy: 0.698\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 38  59   3]\n",
            " [ 27 182  26]\n",
            " [  9  39  32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.51      0.38      0.44       100\n",
            "      medium       0.65      0.77      0.71       235\n",
            "        high       0.52      0.40      0.45        80\n",
            "\n",
            "    accuracy                           0.61       415\n",
            "   macro avg       0.56      0.52      0.53       415\n",
            "weighted avg       0.59      0.61      0.59       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.679\tKappa: 0.281\tAccuracy: 0.607\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 24  49   2]\n",
            " [ 15 119  19]\n",
            " [  3  30  37]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.57      0.32      0.41        75\n",
            "      medium       0.60      0.78      0.68       153\n",
            "        high       0.64      0.53      0.58        70\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.60      0.54      0.56       298\n",
            "weighted avg       0.60      0.60      0.59       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.315\tAccuracy: 0.604\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 29  37   2]\n",
            " [ 15 115  20]\n",
            " [  5  28  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.59      0.43      0.50        68\n",
            "      medium       0.64      0.77      0.70       150\n",
            "        high       0.60      0.50      0.55        66\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.61      0.56      0.58       284\n",
            "weighted avg       0.62      0.62      0.61       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.715\tKappa: 0.349\tAccuracy: 0.623\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.707\t Average Kappa: 0.316\t Average Accuracy: 0.633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:37:43.169737Z",
          "start_time": "2019-08-21T19:37:43.166744Z"
        },
        "colab_type": "text",
        "id": "pYcqr1xSOR7k"
      },
      "source": [
        "### Predecir los target set y crear la submission\n",
        "\n",
        "Aquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.392097Z",
          "start_time": "2020-04-07T15:44:21.386114Z"
        },
        "colab_type": "code",
        "id": "1BXUHx8uOR7l",
        "colab": {}
      },
      "source": [
        "def predict_target(dataset, classifier, labels):\n",
        "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
        "    # Agregar ids\n",
        "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
        "    predicted['id'] = dataset.id.values\n",
        "    # Reordenar las columnas\n",
        "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.588573Z",
          "start_time": "2020-04-07T15:44:21.394094Z"
        },
        "colab_type": "code",
        "id": "MgkFATsKOR7p",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "predicted_target = {}\n",
        "\n",
        "# Crear carpeta ./predictions\n",
        "if (not os.path.exists('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "# por cada target set:\n",
        "for idx, key in enumerate(target):\n",
        "    # Predecirlo\n",
        "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
        "                                           learned_labels_array[idx])\n",
        "    # Guardar predicciones en archivos separados. \n",
        "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
        "                                 sep='\\t',\n",
        "                                 header=False,\n",
        "                                 index=False)\n",
        "\n",
        "# Crear archivo zip\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lLyMLxj_3yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "921ab91a-d115-46e4-ab16-d8af87a66ff2"
      },
      "source": [
        "# Caso sin features hechas a mano, sin emoji ni preprocesador:\n",
        "\n",
        "classifiers = []\n",
        "learned_labels_array = []\n",
        "scores_array = []\n",
        "\n",
        "normal = Snowball\n",
        "for dataset_name, dataset in train.items(): # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  # creamos el pipeline\n",
        "  pipeline = Pipeline([\n",
        "                   ('features',\n",
        "                    FeatureUnion([('bow', CountVectorizer()),\n",
        "                                  ('chars_count', CharsCountTransformer())\n",
        "                                  # Se puede agregar el EmojiCountTransformer() al Feature Union.\n",
        "                                  ])), ('clf', LogisticRegression())])  \n",
        "  # ejecutamos el pipeline sobre el dataset\n",
        "  classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
        "\n",
        "  # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "  classifiers.append(classifier)\n",
        "\n",
        "  # guardamos las labels aprendidas por el clasificador\n",
        "  learned_labels_array.append(learned_labels)\n",
        "\n",
        "  # guardamos los scores obtenidos\n",
        "  scores_array.append(scores)\n",
        "print(\n",
        "      \"Average scores:\\n\\n\",\n",
        "      \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "      .format(*np.array(scores_array).mean(axis=0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 10  43   2]\n",
            " [ 15 180   8]\n",
            " [  1  29  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.18      0.25        55\n",
            "      medium       0.71      0.89      0.79       203\n",
            "        high       0.70      0.43      0.53        53\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.60      0.50      0.52       311\n",
            "weighted avg       0.65      0.68      0.65       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.281\tAccuracy: 0.685\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 35  62   3]\n",
            " [ 29 176  30]\n",
            " [  8  44  28]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.49      0.35      0.41       100\n",
            "      medium       0.62      0.75      0.68       235\n",
            "        high       0.46      0.35      0.40        80\n",
            "\n",
            "    accuracy                           0.58       415\n",
            "   macro avg       0.52      0.48      0.49       415\n",
            "weighted avg       0.56      0.58      0.56       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.222\tAccuracy: 0.576\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 24  50   1]\n",
            " [ 16 119  18]\n",
            " [  2  30  38]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.57      0.32      0.41        75\n",
            "      medium       0.60      0.78      0.68       153\n",
            "        high       0.67      0.54      0.60        70\n",
            "\n",
            "    accuracy                           0.61       298\n",
            "   macro avg       0.61      0.55      0.56       298\n",
            "weighted avg       0.61      0.61      0.59       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.728\tKappa: 0.319\tAccuracy: 0.607\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 23  44   1]\n",
            " [ 17 118  15]\n",
            " [  2  38  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.55      0.34      0.42        68\n",
            "      medium       0.59      0.79      0.67       150\n",
            "        high       0.62      0.39      0.48        66\n",
            "\n",
            "    accuracy                           0.59       284\n",
            "   macro avg       0.59      0.51      0.52       284\n",
            "weighted avg       0.59      0.59      0.57       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.262\tAccuracy: 0.588\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.707\t Average Kappa: 0.271\t Average Accuracy: 0.614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UJ0X-IRgOR7u"
      },
      "source": [
        "## 6. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EnxUv31gOR7x"
      },
      "source": [
        "Discutir resultados, proponer trabajo futuro. (1 punto) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "byENZnZnEA9v"
      },
      "source": [
        "**Consideraciones Previas**\n",
        "\n",
        "Primero que nada, se realizó el análisis de los resultados anteriores en base principalmente al Kappa, debido a que éste está normalizado respecto a una distribuición aleatoria, permite analizar resultados con datasets desbalanceados, como lo es en este caso, y es un buen indicador de correlación para problemas multiclase. \n",
        "\n",
        "En la celda de código en donde se ejecutan TODAS las clases, se prefirió retornar el promedio de cada una de las iteraciones de cada uno de los sentimientos para reducir el ruido de lo que es el análisis de un sentimiento en particular, y se retornó para cada una de los clasificadores la mejor combinación de features.\n",
        "\n",
        "Para hacer este informe breve, se adjunta un archivo de texto con los resultados de esa sección, y se correrá de nuevo esa celda en una versión simplificada que retornará unicamente la mejor combinación de features para cada clasificador.\n",
        "\n",
        "Lo anterior puede no ser una muy buena práctica, porque estamos en cierta manera sobreajustando respecto a un set de validación se fijó, sin embargo, nos da una estimación gruesa de hacia donde se debe ir.\n",
        "\n",
        "\n",
        "**Análisis de Resultados con Distintos Clasificadores y Features**\n",
        "\n",
        "Como se puede observar en la primera celda de código de \"Ejecutar el sistema creado por cada train set\", de la Sección 5, la Regresión Logística es el mejor clasificador en base al Kappa. Le sigue Naive Bayes Multinomial y Multilayer Perceptron con 50 Hidden Layers y un hidden layer.\n",
        "\n",
        "En tanto, el clasificador con mayor variación de puntajes segun los datos obtenidos es el Multilayer Perceptron, el cual si no convergía, retornaba puntajes con Kappa cercanos a cero, y cuando convergía, entregaba resultados cercanos o incluso superiores a la Regresión Logistica.\n",
        "\n",
        "Respecto al Vectorizador, usando la Regresión Logistica, el Vectorizador con mejores resultados es CountVectorizer de unigramas sin stopwords. \n",
        "Comparando CountVectorizer con Tfidf, el Kappa de CountVectorizer es 0.100 puntos en promedio más alto, y bajo Tfidf ocurre que que para *algunos casos de Bigramas y Trigramas*, el Clasificador directamente *no converge*.\n",
        "\n",
        "Podemos observar además que aumentar el modelo de unigramas a bigramas o incluso trigramas en un caso en donde éstos no estan relacionados probabilisticamente es detrimental al rendimiento del clasificador, debido a que aumentan la cantidad de dimensiones considerablemente, siguiendo la \"maldición de dimensionalidad\", reflejandose en la baja considerable del kappa entre unigramas y bigramas, y bigramas a trigramas, del orden de los 0.020 puntos.\n",
        "\n",
        "Apuntando a la normalización de texto, podemos concluir que es una buena practica utilizar ya sea un Lematizador como un Stemmer, es más, la diferencia entre usar uno o no usarlo se traduce en 0.01 puntos de Kappa en promedio.\n",
        "\n",
        "Por último, se corrió el mejor caso anterior posible, Regresión Logistica con CountVectorizer de unigramas sin stopwords y se removió del Feature Union los emoji. Este clasificador de emoji es de baja dimensionalidad (41), y permite clasificar 1500 emoji en categorias relacionadas con sentimientos, añadiendo contexto. Lo anterior se ve reflejado en el Kappa, añadir los emoji aumenta el Kappa en 0.045 puntos, una diferencia más considerable que considerar el uso de Normalizadores en tokens (+0.010 puntos). \n",
        "\n",
        "\n",
        "**Resultados de la competencia**\n",
        "\n",
        "Esta entrega terminó con los siguientes resultados en Codalab:\n",
        "\n",
        "- 0.689 AUC\n",
        "- 0.280 Kappa\n",
        "- 0.598 Accuracy\n",
        "\n",
        "Respecto a los resultados del baseline:\n",
        "\n",
        "- 0.651 AUC\n",
        "- 0.148 Kappa\n",
        "- 0.573 Accuracy\n",
        "\n",
        "Los resultados obtenidos en Codalab coinciden parcialmente con los resultados obtenidos debido a que la entrega hecha a Codalab es una version modificada de esta tarea, puesto que el archivo en la fecha de entrega estaba corrupto, haciendo que en las primeras entregas tuviesen kappa cercanos a cero, lo que indicaría no correlación y que el clasificador tenia un rendimiento similar a uno aleatorio. \n",
        "\n",
        "Sin embargo, en la competencia se pudo realizar una entrega parcial, la cual incorporaba casi todos los elementos de mayor rendimiento escritos en esta tarea, con la excepción de los emoji, pues existían problemas de longitud de features entre el train y el target, significando que la lista creada de emoji, no estaba optimizada para el pipeline existente. La adición de los emoji, que no estaban presentes en la entrega de la competencia, estimamos que aumentaría el kappa al menos en alrededor de 0.020 puntos.\n",
        "\n",
        "Respecto a los resultados actuales, se tiene que éstos superan al baseline en todos los ámbitos, destacando el aumento de Kappa, que indica correlación entre los resultados, de 0.132 puntos, es decir, casi al doble del inicial. \n",
        "\n",
        "\n",
        "**Respecto al trabajo**\n",
        "\n",
        "Pese a lo anterior, no nos vemos conformes con nuestro resultado porque no se pudo aplicar varias features más por falta de tiempo. \n",
        "\n",
        "Sin embargo, aprendimos mucho sobre distintos clasificadores, y cómo infuyen las features utilizadas en el resultado de éstos, obteniendo información útil de ciertos rasgos sutiles del texto, y que tenemos a nuestro alcance herramientas de clasificación de sentimientos que nos podrán ser útiles más adelante.\n",
        "\n",
        "Como trabajo a futuro estaríamos interesados en realizar los siguientes hitos:\n",
        "\n",
        "- Disminuir la sparseness, usando Word Embeddings.\n",
        "- Evaluar el uso de bigaramas y trigramas usando Naive Bayes correctamente, en un intento de mejorar el contexto, puesto que los modelos que probamos con bigramas y trigramas establecían independencia absoluta, y solamente aumentaban la sparseness.\n",
        "- Aplicar mark_neg, para mejorar el contexto de las palabras.\n",
        "- Poder aplicar correctamente la clasificación de Emoji.\n",
        "- Hacer un estudio correcto del algoritmo de Random Forests.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tg4IfRmLKsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}