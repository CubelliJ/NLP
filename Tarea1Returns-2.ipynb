{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea1Returns.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rf9ZEugQOR5U",
        "PkMKhSLzOR5e",
        "9EJ0v5e9OR5o",
        "3lQLmd8WOR5u",
        "Rdgxo2VCOR6J",
        "YrsqpUWCOR6U",
        "IBOSYWxxOR6o",
        "QkcqLzhZOR62",
        "pYcqr1xSOR7k"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hJ-Y8iPlOR5L",
        "toc": true
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objetivo-e-Instrucciones:\" data-toc-modified-id=\"Objetivo-e-Instrucciones:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objetivo e Instrucciones:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objetivo\" data-toc-modified-id=\"Objetivo-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objetivo</a></span></li><li><span><a href=\"#Fecha-de-Entrega:\" data-toc-modified-id=\"Fecha-de-Entrega:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Fecha de Entrega:</a></span></li><li><span><a href=\"#Detalles-e-instrucciones-de-la-competencia:\" data-toc-modified-id=\"Detalles-e-instrucciones-de-la-competencia:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Detalles e instrucciones de la competencia:</a></span></li><li><span><a href=\"#Reporte\" data-toc-modified-id=\"Reporte-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Reporte</a></span></li><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Baseline</a></span></li></ul></li><li><span><a href=\"#1.-Introducci√≥n\" data-toc-modified-id=\"1.-Introducci√≥n-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>1. Introducci√≥n</a></span></li><li><span><a href=\"#2.-Representaciones\" data-toc-modified-id=\"2.-Representaciones-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>2. Representaciones</a></span></li><li><span><a href=\"#3.-Algoritmos\" data-toc-modified-id=\"3.-Algoritmos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>3. Algoritmos</a></span></li><li><span><a href=\"#4.-M√©tricas-de-Evaluaci√≥n\" data-toc-modified-id=\"4.-M√©tricas-de-Evaluaci√≥n-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>4. M√©tricas de Evaluaci√≥n</a></span></li><li><span><a href=\"#5.-Experimentos\" data-toc-modified-id=\"5.-Experimentos-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>5. Experimentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importar-librer√≠as-y-utiles\" data-toc-modified-id=\"Importar-librer√≠as-y-utiles-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Importar librer√≠as y utiles</a></span></li><li><span><a href=\"#Definir-m√©todos-de-evaluaci√≥n\" data-toc-modified-id=\"Definir-m√©todos-de-evaluaci√≥n-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Definir m√©todos de evaluaci√≥n</a></span></li><li><span><a href=\"#Datos\" data-toc-modified-id=\"Datos-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Datos</a></span></li><li><span><a href=\"#Analizar-los-datos\" data-toc-modified-id=\"Analizar-los-datos-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Analizar los datos</a></span></li><li><span><a href=\"#Custom-Features\" data-toc-modified-id=\"Custom-Features-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Custom Features</a></span></li><li><span><a href=\"#Definir-la-representaci√≥n-y-el-clasificador\" data-toc-modified-id=\"Definir-la-representaci√≥n-y-el-clasificador-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Definir la representaci√≥n y el clasificador</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-para-alg√∫n-dataset\" data-toc-modified-id=\"Ejecutar-el-pipeline-para-alg√∫n-dataset-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Ejecutar el pipeline para alg√∫n dataset</a></span></li><li><span><a href=\"#Ejecutar-el-sistema-creado-por-cada-train-set\" data-toc-modified-id=\"Ejecutar-el-sistema-creado-por-cada-train-set-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Ejecutar el sistema creado por cada train set</a></span></li><li><span><a href=\"#Predecir-los-target-set-y-crear-la-submission\" data-toc-modified-id=\"Predecir-los-target-set-y-crear-la-submission-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Predecir los target set y crear la submission</a></span></li></ul></li><li><span><a href=\"#6.-Conclusiones\" data-toc-modified-id=\"6.-Conclusiones-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>6. Conclusiones</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "colab_type": "text",
        "id": "MFxQBdRhOR5O"
      },
      "source": [
        "# Tarea 1 NLP : Competencia de Clasificaci√≥n de Texto\n",
        "-------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4vEPE71lOR5R"
      },
      "source": [
        "- **Nombres: Joaquin Cubelli de Le√≥n, Tom√°s de la Sotta Krause**\n",
        "\n",
        "- **Usuario o nombre de equipo en Codalab: Team NSNOUBN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rf9ZEugQOR5U"
      },
      "source": [
        "## Objetivo e Instrucciones:\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "Esta tarea consiste en participar en una competencia cuyo objetivo es la clasificaci√≥n de tweets seg√∫n su intensidad de emoci√≥n. Espec√≠ficamente: \n",
        "\n",
        "Tendr√°n 4 datasets de tweets de distintas emociones: `anger`, `fear`, `sadness` y `joy`. Para cada uno de estos datasets, deber√°n crear un clasificador que indique la intensidad de dicha emoci√≥n en sus tweets (`low`, `medium`, `high`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zs5tUUY-OR5V"
      },
      "source": [
        "###  Fecha de Entrega: \n",
        "\n",
        "Por ser anunciada una vez termine el paro. Se publicar√° la fecha en ucursos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T14:34:38.796217Z",
          "start_time": "2020-04-07T14:34:38.782255Z"
        },
        "colab_type": "text",
        "id": "IgNcSnajOR5X"
      },
      "source": [
        "### Detalles e instrucciones de la competencia:\n",
        "\n",
        "- La competencia consiste en resolver 4 problemas de clasificaci√≥n distintos, cada uno de tres clases. Por cada problema deber√°n crear un clasificador distinto. La evaluaci√≥n de la competencia se realiza en base a 4 m√©tricas: AUC, Kappa y Accuracy. Los mejores puntajes en cada √≠tem ser√°n los que ganen.\n",
        "\n",
        "- Para comenzar se les entregar√° en este notebook el baseline y la estructura del reporte. El baseline es el c√≥digo que realiza creaci√≥n de features y clasificaci√≥n b√°sica. Los puntajes de este ser√°n ocupados como base para la competencia: deben superar sus resultados para ser bien evaluados.  \n",
        "\n",
        "- Para participar, deben registrarse en Codalab y luego ingresar a la competencia usando el siguiente [link]( https://competitions.codalab.org/competitions/24121?secret_key=f5eb2d95-b36e-4aad-8fc5-4d9d77f4e4dc). \n",
        "\n",
        "- **Es requisito entregar el reporte con el c√≥digo y haber participado en la competencia para ser evaluado.**\n",
        "\n",
        "- Pueden hacer grupos de m√°ximo 2 alumnos. Cada grupo debe tener un nombre de equipo (En codalab, ir a settings y despu√©s cambiar Team Name). Solo una persona debe administrar la cuenta del grupo.\n",
        "\n",
        "- En total pueden hacer un **m√°ximo de 4 env√≠os/submissions** (tanto para equipos como para env√≠os indivuales).\n",
        "\n",
        "- Hagan varios experimentos haciendo cross-validation o evaluaci√≥n sobre una sub-partici√≥n antes de enviar sus predicciones a Codalab. Aseg√∫rense que la distribuci√≥n de las clases sea balanceada en las particiones de training y testing. Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les ser√° evaluado incorrectamente.\n",
        "\n",
        "- Estar top 5 en alguna m√©trica equivale a 1 punto extra en la nota final.\n",
        "\n",
        "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. ¬°Usen todo su conocimiento e ingenio en mejorar sus sistemas! \n",
        "\n",
        "- Todas las dudas escr√≠banlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente ser√°n remitidos a ese medio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:18:43.301002Z",
          "start_time": "2019-08-21T19:18:43.298037Z"
        },
        "colab_type": "text",
        "id": "EUYuMRm5OR5Z"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "Por √∫ltimo, el baseline contiene un c√≥digo b√°sico que:\n",
        "\n",
        "- Obtiene los dataset.\n",
        "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
        "- Crea un Pipeline que: \n",
        "    - Crea features personalizadas.\n",
        "    - Transforma los dataset a bag of words (BoW).  \n",
        "    - Entrena un clasificador usando cada train set.\n",
        "- Clasifica y evalua el sistema creado usando el test set.\n",
        "- Clasifica el target set.\n",
        "- Genera una submission con el target en formato zip en el directorio en donde se est√° ejecutando el notebook. \n",
        "\n",
        "\n",
        "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendr√° mas sentido cuando vean el c√≥digo)\n",
        "\n",
        "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. Tambi√©n, el par√°metro `ngram_range` (Ojo que el clf naive bayes no deber√≠a usarse con n-gramas, ya que rompe el supuesto de independencia). Adem√°s, implementar los atributos que crean √∫tiles desde el listado del el enunciado. Investigar tambi√©n el vectorizador tf-idf.\n",
        "\n",
        "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la funci√≥n `predict_proba`).\n",
        "\n",
        "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aqu√≠ les adjuntamos algunos ejemplos:\n",
        "    -\tWord n-grams.\n",
        "    -\tCharacter n-grams. \n",
        "    -\tPart-of-speech tags.\n",
        "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
        "        - Count the number of positive and negative words within a sentence.\n",
        "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
        "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
        "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
        "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
        "    -\tThe number of words with all characters in uppercase.\n",
        "    -\tThe presence and the number of positive or negative emoticons.\n",
        "    -\tThe number of individual negations.\n",
        "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
        "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
        "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
        "\n",
        "- **Reducci√≥n de dimensionalidad**: Tambi√©n puede serles de ayuda. Referencias [aqu√≠](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
        "\n",
        "- Por √∫ltimo, pueden encontrar mas referencias de c√≥mo mejorar sus features, el vectorizador y el clasificador [aqu√≠](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:25:19.677190Z",
          "start_time": "2020-04-07T15:25:19.671206Z"
        },
        "colab_type": "text",
        "id": "lNHzN8pIOR5a"
      },
      "source": [
        "(Pueden eliminar cualquier celda con instrucciones...)\n",
        "\n",
        "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO ser√°n evaluados Notebooks sin nombre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jMbkpWWmOR5c"
      },
      "source": [
        "----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "colab_type": "text",
        "id": "PkMKhSLzOR5e"
      },
      "source": [
        "## 1. Introducci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M13d7dmvOR5f"
      },
      "source": [
        "**Presentar brevemente el problema a resolver, los m√©todos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)**\n",
        "\n",
        "En esta Tarea, se busca clasificar la intensidad de sentimientos de 4 sentimientos distintos, basado en un dataset de tweets ya categorizados en cada sentimiento. \n",
        "Para ello, se utiliza el siguiente algoritmo:\n",
        "- Se dividen los datos de entrenamiento en dos conjuntos, un conjunto para entrenar al clasificador, y un conjunto para probar la efectividad del clasificador. \n",
        "- Se crea un pipeline, que consiste en un preprocesador, que arregla fallas ortograficas, y agrupa parte de la informaci√≥n, para luego, generar un bag of words usando CountVectorizer, y usando un FeatureUnion se le agregan al bag of words conjuntos de emoji que representan ciertas emociones, y caracteres especiales que permiten agregar contexto al sistema, el resultado de esta union entre las palabras vectorizadas, los emoji, y los caracteres pasan a un clasificador, en este caso Regresi√≥n Logistica.\n",
        "- Se entrena el clasificador utilizando el train data y luego se valida con el test data, obteniendo diversos resultados. \n",
        "- Finalmente se clasifica el target set y se compara con los datos Gold en la competencia obteniendo el puntaje final.\n",
        "\n",
        "Durante el transcurso de la tarea, se iter√≥ con distintos tipos de Vectorizadores, Lemmatizadores y Stemmers, y Clasificadores. \n",
        "\n",
        "Los Features que se mencionaron en el pipeline fueron los que terminaron siendo usados en la entrega final. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "colab_type": "text",
        "id": "AzIxVVxgOR5g"
      },
      "source": [
        "## 2. Representaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:17.719268Z",
          "start_time": "2020-03-31T13:47:17.709207Z"
        },
        "colab_type": "text",
        "id": "pFK240L5OR5i"
      },
      "source": [
        "**Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluaci√≥n agregando m√°s atributos y representaciones dise√±adas a mano. Mas abajo encontrar√°n una lista √∫til de estos que les podr√° ser de utilidad. (1.5 puntos)** \n",
        "\n",
        "Se utiliz√≥ en conjunto al bag of words, dos contadores de caracteres, uno de emoji, en donde se clasifican distintos tipos de emoji por categor√≠a, y por otra parte, un contador de simbolos. \n",
        "Ambos contadores entregan vectores para cada tweet y son agregadas a trav√©s de un Feature Union al Bag of Words.\n",
        "\n",
        "En tanto, en las pruebas se utilizaron diversos Vectorizadores:\n",
        "- CountVectorizer\n",
        "- TfIdfVectorizer\n",
        "- TweetTokenizer\n",
        "\n",
        "Todos estos vectorizadores fueron utilizados con sus versiones de uni, bi y trigramas y se aplicaron sobre ellos stopwords de nltk en diversos rangosjajaja.\n",
        "\n",
        "Respecto a la normalizaci√≥n de tokens, se utilizaron:\n",
        "- Porter Stemmer\n",
        "- Snowball Stemmer\n",
        "- WordNetLemmatizer\n",
        "- noLemmatizer\n",
        "\n",
        "Tambi√©n se prob√≥ usar un autocorrector, SpellChecker con nulos resultados y no se alcanz√≥ a implementar el Mark_Negation.\n",
        "\n",
        "Por √∫ltimo, se agreg√≥ al pipeline previo al vectorizador, un preprocesador de palabras que se utiliz√≥ principalmente para agrupar algunos types raros por tipo, por ejemplo, los nombres de usuario, reemplazandolos por \"@USER\", y otorgando a ciertos numeros, distintos significados, a modo de agregar contexto, como lo son el tiempo, el dinero, entre otros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zS9PFJmoOR5j"
      },
      "source": [
        "## 3. Algoritmos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jly0lKzGOR5n"
      },
      "source": [
        "Describir brevemente los algoritmos de clasificaci√≥n usados. (0.5 puntos)\n",
        "Dentro de los algoritmos evaluados para la realizaci√≥n de esta competencia, se utilizaron:\n",
        "\n",
        "*   Multinomial Naive Bayes: Teorema de Bayes aplicado al BoW, creando un modelo probabil√≠stico seg√∫n el uso de elementos del vocabulario.\n",
        "*   Logistic Regression: Modela la probabilidad de que un evento ocurra sobre otro dadas ciertas condiciones, utilizando un sigmoide. Este modelo itera sobre si mismo hasta converger en los ponderadores de cada vector. Puede aplicarse a casos multivariables.\n",
        "*   K Neighbours Classifier: Se tienen n conjuntos, con vectores iniciales aleatorios, y se asocian a distintas palabras a trav√©s de distancia euclidiana, luego, se calcula el centro de los vectores asociados de cada conjunto, reemplazando al vector inicial. Esto se itera hasta converger.\n",
        "*   Support Vector Machines: Este algoritmo busca un hiperplano que clasifica distintivamente los puntos. Se soporta en vecores \n",
        "*   Decision Tree Classifier: Estructurado en base a diferentes evaluaciones, la ra√≠z (vector original) es dividido en ramas (subvectores) seg√∫n los resultados obtenidos en estas pruebas. Es un algoritmo que busca dividir el conjunto de la mejor manera, por lo que busca par√°metros que dividan de mejor forma el conjunto.\n",
        "*   Random Forest Classifier: En base a vectores aleatorios de elementos del conjunto construye varios √°rboles los cuales son luego promediados.\n",
        "*   Quadratic Cost Classifier: Utilizando la funci√≥n de costo cuadr√°tico, este algoritmo optimiza seg√∫n un m√©todo establecido (por ejemplo gradiente).\n",
        "*   Multilayer Perceptron Classifier: Basado en redes neuronales, utiliza neuronas tipo perceptr√≥n en capas y cantidades predefinidas para calcular los pesos de las variables. Suelen ser en dos o m√°s capas, habiendo correlaci√≥n entre las combinaciones de elementos. Los perceptrones se estructuran en base a una funci√≥n de costo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "colab_type": "text",
        "id": "9EJ0v5e9OR5o"
      },
      "source": [
        "## 4. M√©tricas de Evaluaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RjPjp7F7OR5q"
      },
      "source": [
        "**Describir brevemente las m√©tricas utilizadas en la evaluaci√≥n indicando que miden y su interpretaci√≥n. (0.5 puntos)**\n",
        "\n",
        "Primero definamos lo que es la curva ROC, √©sta es una representacion grafica de la sensibilidad de un clasificador binario seg√∫n se varia el umbral de discriminaci√≥n, es decir, es el ratio entre verdaderos positivos frente al ratio de falsos positivos. \n",
        "\n",
        "- AUC: Del ingl√©s 'Area Under Curve', representa el √°rea bajo la curva ROC. Dado que la curva ROC se utiliza para calificar sistemas binarios, no es muy buena en sistemas multivariables. Se tiene que para un clasificador aleatorio, el AUC es de 0.5 sin importar la distribuici√≥n de las clases, por lo que un puntaje igual o inferior a esto significar√≠a que el sistema no funciona correctamente.\n",
        "\n",
        "- Kappa: Es la clasificaci√≥n de Accuracy pero con la excepcion de que est√° normalizada en la base de que se clasificara el problema aleatoriamente.\n",
        "Adem√°s, es una m√©trica √∫til para problemas con clases desbalanceadas. Soporta muy bien la multivariabilidad. Dada que esta normalizada en base a si el sistema se clasificara aleatoriamente, el minimo Kappa es de 0.\n",
        "Existe un criterio de clasificacion creado por Landis and Koch (1977), que muestra correlacion en base a cierto puntaje Kappa. < 0, indica que no existe concordancia, 0-0.2, muy poca; 0.2-0.4 adecuada, 0.4-0.6 moderada, 0.6-0.8 substancial, 0.8-1 casi perfecta a perfecta.\n",
        "\n",
        "- Accuracy: Es el porcentaje de clasificaciones correctas en todas las instancias respecto al total de instancias. Es m√°s √∫til en clasificacion binaria, que en clasificacion mulivariable, pues puede volverse menos claro c√≥mo se divide la exactitud en cada variable. Para obtener esa informaci√≥n, se puede usar una matriz de confusi√≥n. Esta depende de la distribuici√≥n de las palabras, por lo que puede tener un cesgo importante en caso de que exista una clase con probabilidad mayor, dado que si se elige un clasificador que solo clasifique todo el dataset en sola esa clase, se tendr√≠a una puntuaci√≥n artificialmente alta. Por otra parte, en un dataset equilibrado, si se usa un clasificador aleatorio, se tiene que el Accurary es del orden del 0.5.\n",
        "\n",
        "En esta tarea, la metrica a la que le prestaremos mayor inter√©s es el kappa, pues al ser un problema multinomial, representa el nivel de correlaci√≥n entre los resultados, y nos apoyaremos en la matriz de confusi√≥n, para observar que tan alejados estan los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxnZdUfnOR5r"
      },
      "source": [
        "## 5. Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q-8g2R-1OR5s"
      },
      "source": [
        "Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partici√≥n de evaluaci√≥n que deben crear (o pueden usar cross-validation). Incluyan todo el c√≥digo de sus experimentos aqu√≠. ¬°Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3lQLmd8WOR5u"
      },
      "source": [
        "###Lista de Emojis\n",
        "\n",
        "(Abrir solo para inspeccionar, idealmente dejar cerrada, baja la velocidad de scrolling)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KJJPVAnPOR5v",
        "colab": {}
      },
      "source": [
        "emojiClassificationList = [\n",
        "    ('üê∂','üê±','üê≠','üêπ','üê∞','ü¶ä','üêª','üêº','üê®','üêØ','ü¶Å','üêÆ','üê∑','üêΩ','üê∏','üêí','üêî','üêß','üê¶','üê§',\n",
        "       'üê£','üê•','ü¶Ü','ü¶©','ü¶Ö','ü¶â','ü¶á','üê∫','üêó','üê¥','ü¶Ñ','üêù','üêõ','ü¶ã','üêå','üêö','ü¶ó','üêû','üêú','üï∑',\n",
        "       'üï∏','üê¢','üêç','ü¶é','ü¶Ç','ü¶Ä','ü¶ë','üêô','ü¶ê','üê†','üêü','üê°','üê¨','ü¶à','üê≥','üêã','üêä','üêÜ','üêÖ','üêÉ',\n",
        "       'üêÇ','üêÑ','ü¶å','üê™','üê´','üêò','ü¶è','ü¶ç','üêé','üêñ','üêê','üêè','üêë','üêï','üê©','ü¶Æ','ü¶∫','üêà','üêì',\n",
        "       'ü¶É','üïä','üêá','üêÅ','üêÄ','üêø','ü¶ì','ü¶í','ü¶î','ü¶ß','ü¶•','ü¶¶','ü¶®','ü¶ï','ü¶ñ','üêâ','üê≤'),\n",
        "    ('üêµ','üôä','üôâ','üôä'),\n",
        "    ('üåµ', 'üéÑ', 'üå≤', 'üå≥', 'üå¥', 'üå±', 'üåø', '‚òòÔ∏è', 'üçÄ', 'üéç', 'üéã', 'üçÉ', 'üçÇ', 'üçÅ', 'üçÑ', 'üåæ', 'üíê',\n",
        "        'üå∑', 'üåπ', 'ü•Ä', 'üåª', 'üåº', 'üå∏', 'üå∫'),\n",
        "    ('üòÄ', 'üòÉ', 'üòÑ', 'üòÅ', 'üòÜ', 'ü§©', 'üòä', 'üòá', 'üôÇ', '‚ò∫Ô∏è', 'üòâ', 'ü§ó', 'ü§ì', 'üòé', 'ü§†', 'ü§ô', 'üí™',\n",
        "             'ü§û', '‚úåÔ∏è', 'ü§ò', '‚úä', 'ü§õ', 'ü§ú', 'üò∫', 'üò∏'),\n",
        "    ('üòÇ', 'ü§£', 'üòÑ', 'üòÅ', 'üòÜ', 'üòπ'),\n",
        "    ('üòç', 'üòò', 'üòó', 'üòô', 'üòö', 'ü•∞', 'üòª', 'üòΩ', '‚ù§Ô∏è', 'üß°', 'üíõ', 'üíö', 'üíô', 'üíú', 'üñ§', '‚ù£Ô∏è', 'üíï',\n",
        "        'üíû', 'üíì', 'üíó', 'üíñ', 'üíò', 'üíù', 'üíü', 'üíå'),\n",
        "    ('üòå', 'üò¥'),\n",
        "    ('ü§¢', 'ü§Æ', 'ü§ß', 'üò∑', 'ü§í', 'ü§ï'),\n",
        "    ('üòà', 'üëø', 'üëπ', 'üë∫', 'üòº'),\n",
        "    ('üò§', 'üò†', 'üò°', 'ü§¨'),\n",
        "    ('üòí','üòê', 'üòë', 'üôÉ', 'üòæ'),\n",
        "    ('üòû', 'üòî', 'üòü', 'üòï', 'üôÅ', '‚òπÔ∏è', 'üò£', 'üòñ', 'üò´', 'üò©', 'üòØ', 'üò¶', 'üòß', 'üòÆ', 'üò≤', 'üòµ', 'üò≠', 'üòì',\n",
        "        'üò™', 'üòæ', 'üòø'),\n",
        "    ('üíî',),\n",
        "    ('üò≥', 'üò®', 'üò∞', 'üò∂'),\n",
        "    ('üòã', 'ü§™', 'üòú', 'üòù', 'üòõ'),\n",
        "    ('ü§ë', 'üí∏', 'üíµ', 'üí¥', 'üí∂', 'üí∑', 'üí∞', 'üí≥', 'üíé'),\n",
        "    ('ü§®', 'üßê', 'ü§î', 'ü§Ø', 'üëÄ', 'ü§´', 'ü§≠', 'ü§•', 'ü§ê', 'üôÑ'),\n",
        "    ('üôÄ', 'üò±'),\n",
        "    ('üòè', 'üòº', 'ü§§', 'üçÜ', 'üçë', 'üçå', 'üî©', 'üí¶', 'üëÖ'),\n",
        "    ('üôè',),\n",
        "    ('üî•',),\n",
        "    ('üå•', 'üå¶', '‚òÅÔ∏è', 'üåß', '‚õà', 'üå©', 'üå®', '‚òÉÔ∏è', '‚õÑ', '‚ùÑÔ∏è', 'üå¨', 'üí®', 'üå™', 'üå´', '‚òî'),\n",
        "    ('üëê', 'üôå', 'üëè', 'ü§≤', 'ü§ù', 'üëç', 'üëé', 'üëä', 'üëå', 'üëà', 'üëâ', 'üëÜ', 'üëá', '‚òùÔ∏è', '‚úã', 'ü§ö', 'üñê',\n",
        "        'üññ', 'üëã', 'ü§ü', '‚úçÔ∏è', 'ü§≥', 'üíÖ', 'üññ', 'üëÇ', '\\U0001f9bb', 'üëÉ', 'ü¶µ', 'ü¶∂', 'üíÑ', 'üíã', 'üëÑ'),\n",
        "    ('üë∑', 'üíÇ',  'üïµÔ∏è', 'ü§∂', 'üéÖ', 'üë∏', 'ü§¥', 'üë∞', 'ü§µ', 'üëº', 'ü§∞', 'ü§±', 'üôá', 'üíÅ', 'üôÖ', 'üôÜ', 'üôã',\n",
        "        'üôé', 'üôç', 'üíÜ','üï¥', 'üë∂', 'üë¶', 'üëß', 'üßí', 'üë®', 'üë©', 'üßë','üë±', 'üßî', 'üë¥', 'üëµ', 'üßì', 'üë≤', 'üë≥',\n",
        "        'üßï', 'üö∂', 'üèÉ', 'üë´', 'üë≠', 'üë¨', 'üíë', 'üíè', 'üë™'),\n",
        "    ('üëö', 'üëï', 'üëñ', 'üëî', 'üëó', 'üëô', 'üëò', 'üë†', 'üë°', 'üë¢', 'üëû', 'üëü', 'üëí', 'üé©', 'üéì', 'üëë', '‚õë',\n",
        "        'üéí', 'üëù', 'üëõ', 'üëú', 'üíº', 'üëì', 'üï∂', '\\U0001f93f', 'üåÇ', '‚òÇÔ∏è', 'üß£', 'üß§', 'üß•', '\\U0001f9ba',\n",
        "        '\\U0001f97b', '\\U0001fa71', '\\U0001fa72', '\\U0001fa73', '\\U0001fa70', 'üß¶', 'üß¢'),\n",
        "    ('\\U0001f9be', '\\U0001f9bf', 'üë£', 'üëÅ', 'üó£', 'üë§', 'üë•', 'üíÉ', 'üï∫', 'üëØ', 'ü•µ', 'ü•∂', 'ü•≥', 'ü•¥',\n",
        "        'ü•∫', 'ü¶∏', 'ü¶π', 'üéó', 'üé´', 'üéü', 'üé™'),\n",
        "    ('üëª', 'üíÄ', '‚ò†Ô∏è', 'üëΩ', 'üëæ', 'ü§ñ', 'üéÉ', 'ü§°'),\n",
        "    ('‚öΩ', 'üèÄ', 'üèà', '‚öæ', 'üéæ', 'üèê', 'üèâ', 'üé±', 'üèì', 'üè∏', 'ü•Ö', 'üèí', 'üèë', 'üèè', '‚õ≥', 'üèπ', 'üé£',\n",
        "        'ü•ä', 'ü•ã', '‚õ∏', 'üéø', '‚õ∑', 'üèÇ', 'üèãÔ∏è', 'ü§∫',  '‚õπÔ∏è', 'üèåÔ∏è', 'üèÑ', 'üèä', 'üö£', 'üèá', 'üö¥', 'üöµ',\n",
        "        '\\U0001fa82', 'üéΩ', 'üèÖ', 'üéñ', 'ü•á', 'ü•à', 'ü•â', 'üèÜ'),\n",
        "    ('üé≠', 'üé®', 'üé¨', 'üé§', 'üéß', 'üéº', 'üéπ', 'ü•Å', 'üé∑', 'üé∫', 'üé∏', 'üéª', '\\U0001fa95'),\n",
        "    ('üçè', 'üçé', 'üçê', 'üçä', 'üçã', 'üçâ', 'üçá', 'üçì', 'üçà', 'üçí', 'üçç', 'ü•ù', 'ü•ë', 'üçÖ', 'ü•í', 'ü•ï', 'üåΩ',\n",
        "        'üå∂', '\\U0001f9c4', '\\U0001f9c5', 'ü•î', 'üç†', 'üå∞', 'ü•ú', 'üçØ', 'ü•ê', 'üçû', 'ü•ñ', 'üßÄ', 'ü•ö', 'üç≥',\n",
        "        'ü•ì', 'ü•û', 'üç§', 'üçó', 'üçñ', 'üçï', 'üå≠', 'üçî', 'üçü', 'ü•ô', 'üåÆ', 'üåØ', 'ü•ó', 'ü•ò', 'üçù', 'üçú', 'üç≤',\n",
        "        'üç•', 'üç£', 'üç±', 'üçõ', 'üçö', 'üçô', 'üçò', 'üç¢', 'üç°', 'üçß', 'üç®', 'üç¶', 'üç∞', 'üéÇ', 'üçÆ', 'üç≠', 'üç¨',\n",
        "        'üç´', 'üçø', 'üç©', 'üç™', 'ü•õ', 'üçº', '‚òï', 'üçµ', 'üç∂', 'ü•Ñ', 'üç¥', 'üçΩ', 'ü••', 'ü•®', 'ü•©', 'ü•™', 'ü•£',\n",
        "        'ü•´', '\\U0001f9c7', '\\U0001f9c6', '\\U0001f9c8', 'ü•ü', 'ü•†', 'ü•°', 'ü•ß', 'ü•§', 'ü•¢', '\\U0001f9c3',\n",
        "        '\\U0001f9c9'),\n",
        "    ('‚ôà', '‚ôâ', '‚ôä', '‚ôã', '‚ôå', '‚ôç', '‚ôé', '‚ôè', '‚ôê', '‚ôë', '‚ôí', '‚ôì', '‚òÆÔ∏è', '‚úùÔ∏è', '‚ò™Ô∏è', 'üïâ', '‚ò∏Ô∏è', '‚ú°Ô∏è', 'üîØ',\n",
        "        'üïé', '‚òØÔ∏è', '‚ò¶Ô∏è', 'üõê', '‚õé', 'üÜî', '‚öõÔ∏è'),\n",
        "    ('üöó', 'üöï', 'üöô', 'üöå', 'üöé', 'üèé', 'üöì', 'üöë', 'üöí', 'üöê', 'üöö', 'üöõ', 'üöú', 'üõ¥', 'üö≤', 'üõµ',\n",
        "        '\\U0001f6fa', 'üèç', '\\U0001f9bd', '\\U0001f9bc', 'üö®', 'üöî', 'üöç', 'üöò', 'üöñ', 'üö°', 'üö†', 'üöü',\n",
        "        'üöÉ', 'üöã', 'üöû', 'üöù', 'üöÑ', 'üöÖ', 'üöà', 'üöÇ', 'üöÜ', 'üöá', 'üöä', 'üöâ', 'üöÅ', 'üõ©', '‚úàÔ∏è', 'üõ´', 'üõ¨',\n",
        "        'üöÄ', 'üõ∞', 'üí∫', 'üõ∂', '‚õµ', 'üõ•', 'üö§', 'üõ≥', '‚õ¥', 'üö¢', '‚öì', 'üöß', '‚õΩ', 'üöè', 'üö¶', 'üö•',\n",
        "        'üó∫', 'üóø', 'üóΩ', '‚õ≤', 'üóº', 'üè∞', 'üèØ', 'üèü', 'üé°', 'üé¢', 'üé†', '‚õ±', 'üèñ', 'üèù', '‚õ∞', 'üèî',\n",
        "        'üóª', 'üåã', 'üèú', 'üèï', '‚õ∫', 'üõ§', 'üõ£', 'üèó', 'üè≠', 'üè†', 'üè°', 'üèò', 'üèö', 'üè¢', 'üè¨', 'üè£',\n",
        "        'üè§', 'üè•', 'üè¶', 'üè®', 'üè™', 'üè´', 'üè©', 'üíí', 'üèõ', '‚õ™', 'üïå', 'üïç', 'üïã', '\\U0001f6d5', '‚õ©',\n",
        "        'üóæ', 'üéë', 'üèû', 'üåÖ', 'üåÑ', 'üå†', 'üéá', 'üéÜ', 'üåá', 'üåÜ', 'üèô', 'üåÉ', 'üåå', 'üåâ', 'üåÅ', 'üõ∏'),\n",
        "    ('üéÅ', 'üéà', 'üéè', 'üéÄ', 'üéä', 'üéâ', 'üéé', 'üèÆ'),\n",
        "    ('üïê', 'üïë', 'üïí', 'üïì', 'üïî', 'üïï', 'üïñ', 'üïó', 'üïò', 'üïô', 'üïö', 'üïõ', 'üïú', 'üïù', 'üïû', 'üïü', 'üï†',\n",
        "        'üï°', 'üï¢', 'üï£', 'üï§', 'üï•', 'üï¶', 'üïß', '‚è±', '‚è≤', '‚è∞', 'üóì', 'üìÜ', 'üìÖ'),\n",
        "    ('‚ô†Ô∏è', '‚ô£Ô∏è', '‚ô•Ô∏è', '‚ô¶Ô∏è', 'üÉè', 'üé¥', 'üÄÑ', 'üé≤', 'üéØ', 'üé≥', 'üéÆ', 'üé∞', 'üõ∑', 'ü•å', '\\U0001fa80',\n",
        "        '\\U0001fa81'),\n",
        "    ('üî´', 'üí£', 'üî™', 'üó°', '‚öîÔ∏è', '\\U0001fa93', '‚åö', '\\U0001fa78', '‚ö∞Ô∏è', '‚ö±Ô∏è', 'üè∫', 'üîß', 'üî®', '‚öí',\n",
        "        'üõ†', '‚õè', '‚åõ', '‚è≥', '‚úÇÔ∏è'),\n",
        "    ('üâë', '‚ò¢Ô∏è', '‚ò£Ô∏è', 'üì¥', 'üì≥', 'üà∂', 'üàö', 'üà∏', 'üà∫', 'üà∑Ô∏è', '‚ú¥Ô∏è', 'üÜö', 'üíÆ', 'üâê', '„äôÔ∏è', '„äóÔ∏è', 'üà¥',\n",
        "        'üàµ', 'üàπ', 'üà≤', 'üÖ∞Ô∏è', 'üÖ±Ô∏è', 'üÜé', 'üÜë', 'üÖæÔ∏è', 'üÜò', '‚ùå', '‚≠ï', 'üõë', '‚õî', 'üìõ', 'üö´', 'üíØ', 'üí¢',\n",
        "        '‚ô®Ô∏è', 'üö∑', 'üöØ', 'üö≥', 'üö±', 'üîû', 'üìµ', 'üö≠', '‚ùó', '‚ùï', '‚ùì', '‚ùî', '‚ÄºÔ∏è', '‚ÅâÔ∏è', 'üîÖ', 'üîÜ', '„ÄΩÔ∏è',\n",
        "        '‚ö†Ô∏è', 'üö∏', 'üî±', '‚öúÔ∏è', 'üî∞', '‚ôªÔ∏è', '‚úÖ', 'üàØ', 'üíπ', '‚ùáÔ∏è', '‚ú≥Ô∏è', '‚ùé', 'üåê', 'üí†', '‚ìÇÔ∏è', 'üåÄ', 'üí§', 'üèß',\n",
        "        'üöæ', '‚ôø', 'üÖøÔ∏è', 'üà≥', 'üàÇÔ∏è', 'üõÇ', 'üõÉ', 'üõÑ', 'üõÖ', 'üöπ', 'üö∫', 'üöº', 'üöª', 'üöÆ', 'üé¶', 'üì∂', 'üàÅ',\n",
        "        'üî£', '‚ÑπÔ∏è', 'üî§', 'üî°', 'üî†', 'üÜñ', 'üÜó', 'üÜô', 'üÜí', 'üÜï', 'üÜì', '0Ô∏è‚É£', '1Ô∏è‚É£', '2Ô∏è‚É£', '3Ô∏è‚É£', '4Ô∏è‚É£', '5Ô∏è‚É£',\n",
        "        '6Ô∏è‚É£', '7Ô∏è‚É£', '8Ô∏è‚É£', '9Ô∏è‚É£', 'üîü', 'üî¢', '#Ô∏è‚É£', '*Ô∏è‚É£', '‚ñ∂Ô∏è', '‚è∏', '‚èØ', '‚èπ', '‚è∫', '‚è≠', '‚èÆ', '‚è©', '‚è™', '‚è´',\n",
        "        '‚è¨', '‚óÄÔ∏è', 'üîº', 'üîΩ', '‚û°Ô∏è', '‚¨ÖÔ∏è', '‚¨ÜÔ∏è', '‚¨áÔ∏è', '‚ÜóÔ∏è', '‚ÜòÔ∏è', '‚ÜôÔ∏è', '‚ÜñÔ∏è', '‚ÜïÔ∏è', '‚ÜîÔ∏è', '‚Ü™Ô∏è', '‚Ü©Ô∏è', '‚§¥Ô∏è', '‚§µÔ∏è',\n",
        "        'üîÄ', 'üîÅ', 'üîÇ', 'üîÑ', 'üîÉ', 'üéµ', 'üé∂', '‚ûï', '‚ûñ', '‚ûó', '‚úñÔ∏è', 'üí≤', 'üí±', '‚Ñ¢Ô∏è', '¬©Ô∏è', '¬ÆÔ∏è', '„Ä∞Ô∏è',\n",
        "        '‚û∞', '‚ûø', 'üîö', 'üîô', 'üîõ', 'üîù', '‚úîÔ∏è', '‚òëÔ∏è', 'üîò', 'üî¥', '\\U0001f7e0', '\\U0001f7e1', '\\U0001f7e2',\n",
        "        'üîµ', '\\U0001f7e3', '‚ö´', '‚ö™', '\\U0001f7e4', 'üî∫', 'üîª', 'üî∏', 'üîπ', 'üî∂', 'üî∑', 'üî≥', 'üî≤', '‚ñ™Ô∏è',\n",
        "        '‚ñ´Ô∏è', '‚óæ', '‚óΩ', '‚óºÔ∏è', '‚óªÔ∏è', '‚¨õ', '‚¨ú', '\\U0001f7e5', '\\U0001f7e7', '\\U0001f7e8', '\\U0001f7e9',\n",
        "        '\\U0001f7e6', '\\U0001f7ea', '\\U0001f7eb', 'üîà', 'üîá', 'üîâ', 'üîä', 'üîî', 'üîï', 'üì£', 'üì¢', 'üí¨',\n",
        "        'üí≠', 'üóØ', '‚èè', '‚ôÄ', '‚ôÇ', '‚öï', '‚ôæÔ∏è'),\n",
        "    ('üç∫', 'üçª', 'ü•Ç', 'üç∑', 'ü•É', 'üç∏', 'üçπ', 'üçæ', 'üßä', 'üíä', 'üíâ', 'üö¨'),\n",
        "    ('üè¥', 'üá¶üá´', 'üá¶üáΩ', 'üá¶üá±', 'üá©üáø', 'üá¶üá∏', 'üá¶üá©', 'üá¶üá¥', 'üá¶üáÆ', 'üá¶üá∂', 'üá¶üá¨', 'üá¶üá∑', 'üá¶üá≤', 'üá¶üáº', 'üá¶üá®', 'üá¶üá∫',\n",
        "        'üá¶üáπ', 'üá¶üáø', 'üáßüá∏', 'üáßüá≠', 'üáßüá©', 'üáßüáß', 'üáßüáæ', 'üáßüá™', 'üáßüáø', 'üáßüáØ', 'üáßüá≤', 'üáßüáπ', 'üáßüá¥', 'üáßüá¶', 'üáßüáº', 'üáßüáª',\n",
        "        'üáßüá∑', 'üáÆüá¥', 'üáªüá¨', 'üáßüá≥', 'üáßüá¨', 'üáßüá´', 'üáßüáÆ', 'üá∞üá≠', 'üá®üá≤', 'üá®üá¶', 'üáÆüá®', 'üá®üáª', 'üáßüá∂', 'üá∞üáæ', 'üá®üá´', 'üá™üá¶',\n",
        "        'üáπüá©', 'üá®üá±', 'üá®üá≥', 'üá®üáΩ', 'üá®üáµ', 'üá®üá®', 'üá®üá¥', 'üá∞üá≤', 'üá®üá¨', 'üá®üá©', 'üá®üá∞', 'üá®üá∑', 'üá®üáÆ', 'üá≠üá∑', 'üá®üá∫', 'üá®üáº',\n",
        "        'üá®üáæ', 'üá®üáø', 'üá©üá∞', 'üá©üá¨', 'üá©üáØ', 'üá©üá≤', 'üá©üá¥', 'üá™üá®', 'üá™üá¨', 'üá∏üáª', 'üá¨üá∂', 'üá™üá∑', 'üá™üá™', 'üá™üáπ', 'üá™üá∫', 'üá´üá∞',\n",
        "        'üá´üá¥', 'üá´üáØ', 'üá´üáÆ', 'üá´üá∑', 'üá¨üá´', 'üáµüá´', 'üáπüá´', 'üá¨üá¶', 'üá¨üá≤', 'üá¨üá™', 'üá©üá™', 'üá¨üá≠', 'üá¨üáÆ', 'üá¨üá∑', 'üá¨üá±', 'üá¨üá©',\n",
        "        'üá¨üáµ', 'üá¨üá∫', 'üá¨üáπ', 'üá¨üá¨', 'üá¨üá≥', 'üá¨üáº', 'üá¨üáæ', 'üá≠üáπ', 'üá≠üá≤', 'üá≠üá≥', 'üá≠üá∞', 'üá≠üá∫', 'üáÆüá∏', 'üáÆüá≥', 'üáÆüá©', 'üáÆüá∑',\n",
        "        'üáÆüá∂', 'üáÆüá™', 'üáÆüá≤', 'üáÆüá±', 'üáÆüáπ', 'üáØüá≤', 'üáØüáµ', 'üáØüá™', 'üáØüá¥', 'üá∞üáø', 'üá∞üá™', 'üá∞üáÆ', 'üáΩüá∞', 'üá∞üáº', 'üá∞üá¨', 'üá±üá¶',\n",
        "        'üá±üáª', 'üá±üáß', 'üá±üá∏', 'üá±üá∑', 'üá±üáæ', 'üá±üáÆ', 'üá±üáπ', 'üá±üá∫', 'üá≤üá¥', 'üá≤üá∞', 'üá≤üá¨', 'üá≤üáº', 'üá≤üáæ', 'üá≤üáª', 'üá≤üá±', 'üá≤üáπ',\n",
        "        'üá≤üá≠', 'üá≤üá∂', 'üá≤üá∑', 'üá≤üá∫', 'üáæüáπ', 'üá≤üáΩ', 'üá´üá≤', 'üá≤üá©', 'üá≤üá®', 'üá≤üá≥', 'üá≤üá™', 'üá≤üá∏', 'üá≤üá¶', 'üá≤üáø', 'üá≤üá≤', 'üá≥üá¶',\n",
        "        'üá≥üá∑', 'üá≥üáµ', 'üá≥üá±', 'üá≥üá®', 'üá≥üáø', 'üá≥üáÆ', 'üá≥üá™', 'üá≥üá¨', 'üá≥üá∫', 'üá≥üá´', 'üá≤üáµ', 'üá∞üáµ', 'üá≥üá¥', 'üá¥üá≤', 'üáµüá∞', 'üáµüáº',\n",
        "        'üáµüá∏', 'üáµüá¶', 'üáµüá¨', 'üáµüáæ', 'üáµüá™', 'üáµüá≠', 'üáµüá≥', 'üáµüá±', 'üáµüáπ', 'üáµüá∑', 'üá∂üá¶', 'üá∑üá™', 'üá∑üá¥', 'üá∑üá∫', 'üá∑üáº', 'üáºüá∏',\n",
        "        'üá∏üá≤', 'üá∏üáπ', 'üá∏üá¶', 'üá∏üá≥', 'üá∑üá∏', 'üá∏üá®', 'üá∏üá±', 'üá∏üá¨', 'üá∏üáΩ', 'üá∏üá∞', 'üá∏üáÆ', 'üá∏üáß', 'üá∏üá¥', 'üáøüá¶', 'üá¨üá∏', 'üá∞üá∑',\n",
        "        'üá∏üá∏', 'üá™üá∏', 'üá±üá∞', 'üáßüá±', 'üá∏üá≠', 'üá∞üá≥', 'üá±üá®', 'üá≤üá´', 'üáµüá≤', 'üáªüá®', 'üá∏üá©', 'üá∏üá∑', 'üá∏üáØ', 'üá∏üáø', 'üá∏üá™', 'üá®üá≠',\n",
        "        'üá∏üáæ', 'üáπüáº', 'üáπüáØ', 'üáπüáø', 'üáπüá≠', 'üáπüá±', 'üáπüá¨', 'üáπüá∞', 'üáπüá¥', 'üáπüáπ', 'üáπüá¶', 'üáπüá≥', 'üáπüá∑', 'üáπüá≤', 'üáπüá®', 'üáπüáª',\n",
        "        'üá∫üá¨', 'üá∫üá¶', 'üá¶üá™', 'üá¨üáß', 'üá∫üá∏', 'üá∫üáæ', 'üá∫üá≤', 'üá∫üá≥', 'üáªüáÆ', 'üá∫üáø', 'üáªüá∫', 'üáªüá¶', 'üáªüá™', 'üáªüá≥', 'üáºüá´', 'üá™üá≠',\n",
        "        'üáæüá™', 'üáøüá≤', 'üáøüáº', 'üè¥‚Äç‚ò†Ô∏è'),\n",
        "    ('üì±', 'üì≤', 'üíª', '‚å®Ô∏è', 'üñ•', 'üñ®', 'üñ±', 'üñ≤', 'üïπ', 'üóú', 'üíΩ', 'üíæ', 'üíø', 'üìÄ', 'üìº', 'üì∑',\n",
        "        'üì∏', 'üìπ', 'üé•', 'üìΩ', 'üéû', 'üìû', '‚òéÔ∏è', 'üìü', 'üì†', 'üì∫', 'üìª', 'üéô', 'üéö', 'üéõ', 'üï∞', 'üì°',\n",
        "        'üîã', 'üîå', 'üí°', 'üî¶', 'üïØ', 'üóë', 'üõ¢', '‚öñÔ∏è', '‚öôÔ∏è', '‚õì', '\\U0001f9af', 'üõ°', 'üîÆ', 'üìø', 'üíà',\n",
        "        '‚öóÔ∏è', 'üî≠', 'üî¨', 'üï≥', '\\U0001fa79', '\\U0001fa7a', 'üå°', '\\U0001fa92', 'üöΩ', 'üö∞', 'üöø', 'üõÅ',\n",
        "        'üõÄ', 'üõé', 'üîë', 'üóù', 'üö™', '\\U0001fa91', 'üõã', 'üõè', 'üõå', 'üñº', 'üõç', 'üõí', 'üéê', '‚úâÔ∏è',\n",
        "        'üì©', 'üì®', 'üìß', 'üì•', 'üì§', 'üì¶', 'üè∑', 'üì™', 'üì´', 'üì¨', 'üì≠', 'üìÆ', 'üìØ', 'üìú', 'üìÉ', 'üìÑ',\n",
        "        'üìë', 'üìä', 'üìà', 'üìâ', 'üóí', 'üìá', 'üóÉ', 'üó≥', 'üóÑ', 'üìã', 'üìÅ', 'üìÇ', 'üóÇ', 'üóû', 'üì∞', 'üìì',\n",
        "        'üìî', 'üìí', 'üìï', 'üìó', 'üìò', 'üìô', 'üìö', 'üìñ', 'üîñ', 'üîó', 'üìé', 'üñá', 'üìê', 'üìè', 'üìå', 'üìç',\n",
        "        'üìå', 'üéå', 'üè≥Ô∏è', 'üè¥', 'üèÅ', '\\U0001fa94', 'üñä', 'üñã', '‚úíÔ∏è', 'üñå', 'üñç', 'üìù', '‚úèÔ∏è', 'üîç', 'üîé',\n",
        "        'üîè', 'üîê', 'üîí', 'üîì'),\n",
        "    ('üåà',),\n",
        "    ('üåé', 'üåç', 'üåè', 'üåï', 'üåñ', 'üåó', 'üåò', 'üåë', 'üåí', 'üåì', 'üåî', 'üåû', 'üåõ', 'üåú', 'üåô',\n",
        "        '\\U0001fa90', 'üí´', '‚≠ê', 'üåü', '‚ú®', '‚ö°', 'üí•', '‚òÑÔ∏è', '‚òÄÔ∏è', 'üå§', '‚õÖ', 'üåä', 'üíß')\n",
        "                  \n",
        "    ]\n",
        "\n",
        "# Los deje al inicio asi cuando hago scroll no se laguea mi pantalla jajaj."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:31:40.023344Z",
          "start_time": "2020-03-31T13:31:40.003541Z"
        },
        "colab_type": "text",
        "id": "WtJr2bdNOR52"
      },
      "source": [
        "### Importar librer√≠as y utiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "houUEut6OR53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c3582af2-1efa-40a4-b15d-00868f59f83b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sVU8g6BrZ465",
        "colab": {}
      },
      "source": [
        "def noLemmatizer():\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.587160Z",
          "start_time": "2020-04-07T15:44:19.319386Z"
        },
        "colab_type": "code",
        "id": "FWoBKYn6OR6B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "87ecdf26-58b0-466a-920d-87a85f95d85d"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "# Tokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk import TweetTokenizer\n",
        "\n",
        "# StopWords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Lemmatizer\n",
        "from nltk.stem import  WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Functions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# BEGIN_QUOTE (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# END_QUOTE\n",
        "\n",
        "\n",
        "# Mark Negation\n",
        "from nltk.sentiment.util import mark_negation\n",
        "\n",
        "# Other\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "Porter = nltk.stem.PorterStemmer()\n",
        "Snowball = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "stp_wrds = list(stopwords.words('english')) # nltk stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wuQLUX6kiJi1",
        "colab": {}
      },
      "source": [
        "# Funciones Base:\n",
        "\n",
        "def isLemmatizer(Lemma):\n",
        "    if hasattr(Lemma, 'lemmatize'):\n",
        "        return True\n",
        "\n",
        "\n",
        "def isStemmer(Stemm):\n",
        "    if hasattr(Stemm, 'stem'):\n",
        "        return True\n",
        "\n",
        "\n",
        "def noNormalizer(): # Simplemente devuelve la misma palabra, sin normalizar los token.\n",
        "    pass\n",
        "\n",
        "Porter = nltk.stem.PorterStemmer()\n",
        "Snowball = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "#Tknzr = nltk.tokenize.TweetTokenizer() # No funciona, problemas de compatibilidad entre nltk y sklearn."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvf3mJ__jGfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Vectorizers = [CountVectorizer(),CountVectorizer(ngram_range=(1, 2)),CountVectorizer(ngram_range=(1, 3)),TfidfVectorizer(),TfidfVectorizer(ngram_range=(1, 2)),TfidfVectorizer(ngram_range=(1, 3)),\n",
        "               CountVectorizer(stop_words=stp_wrds),CountVectorizer(ngram_range=(1, 2),stop_words=stp_wrds),CountVectorizer(ngram_range=(1, 3),stop_words=stp_wrds),TfidfVectorizer(stop_words=stp_wrds),\n",
        "               TfidfVectorizer(ngram_range=(1, 2),stop_words=stp_wrds),TfidfVectorizer(ngram_range=(1, 3),stop_words=stp_wrds)]\n",
        "\n",
        "# En orden de vectorizadores, unigramas, uni y bigramas, uni, bi y trigramas. \n",
        "# Luego lo mismo pero con las stopwords de nltk.\n",
        "\n",
        "FClassifiers = [\n",
        "                    LogisticRegression(n_jobs=-1), \n",
        "                    MultinomialNB(), # Baseline.\n",
        "# BEGIN_QUOTE (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
        "                    KNeighborsClassifier(6),\n",
        "                    SVC(kernel=\"linear\", C=0.025, probability=True),\n",
        "                    SVC(gamma=2, C=1, probability=True),\n",
        "                    DecisionTreeClassifier(max_depth=5),\n",
        "                    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "                    AdaBoostClassifier(),\n",
        "                    MLPClassifier(hidden_layer_sizes=(50),early_stopping=True,max_iter=1000),\n",
        "                    MLPClassifier(hidden_layer_sizes=(100),early_stopping=True,max_iter=1000)\n",
        "# END_QUOTE\n",
        "] \n",
        "\n",
        "Normalizers = [WordNetLemmatizer(), Porter, Snowball, noNormalizer()] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rdgxo2VCOR6J"
      },
      "source": [
        "### Definir m√©todos de evaluaci√≥n\n",
        "\n",
        "Estas funciones est√°n a cargo de evaluar los resultados de la tarea. No deber√≠an cambiarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "colab_type": "code",
        "id": "4xl1bWBmOR6K",
        "colab": {}
      },
      "source": [
        "def auc_score(test_set, predicted_set):\n",
        "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
        "    medium_predicted = np.array(\n",
        "        [prediction[1] for prediction in predicted_set])\n",
        "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
        "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
        "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
        "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
        "    auc_high = roc_auc_score(high_test, high_predicted)\n",
        "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
        "    auc_low = roc_auc_score(low_test, low_predicted)\n",
        "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
        "             high_test.sum() * auc_high) / (\n",
        "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
        "    return auc_w\n",
        "\n",
        "def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    print(\n",
        "        confusion_matrix(y_test,\n",
        "                         predicted_labels,\n",
        "                         labels=['low', 'medium', 'high']))\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(\n",
        "        classification_report(y_test,\n",
        "                              predicted_labels,\n",
        "                              labels=['low', 'medium', 'high']))\n",
        "#     Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])\n",
        "\n",
        "# Copiamos la funcion anterior y comentamos todos los print, para hacer las pruebas\n",
        "# masivas obteniendo los resultados m√°s relevantes.\n",
        "\n",
        "def evaulate_class(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "    #print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    #print(\n",
        "    #    confusion_matrix(y_test,\n",
        "    #                     predicted_labels,\n",
        "    #                     labels=['low', 'medium', 'high']))\n",
        "\n",
        "    #print('\\nClassification Report:\\n')\n",
        "    #print(\n",
        "    #    classification_report(y_test,\n",
        "    #                          predicted_labels,\n",
        "    #                          labels=['low', 'medium', 'high']))\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    #print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    #print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    #print(\"Accuracy:\", accuracy)\n",
        "    #print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YrsqpUWCOR6U"
      },
      "source": [
        "### Datos\n",
        "\n",
        "Obtener los datasets desde el github del curso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "colab_type": "code",
        "id": "7-zqcg8YOR6X",
        "colab": {}
      },
      "source": [
        "# Datasets de entrenamiento.\n",
        "train = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
        "}\n",
        "# Datasets que deber√°n predecir para la competencia.\n",
        "target = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "colab_type": "code",
        "id": "v4SUr4g0OR6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "14ddedf7-7373-4d66-f623-4b1dc4e47996"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias:\n",
        "train['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>10152</td>\n",
              "      <td>@RobertTaitWHU against Chelsea anything is pos...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>10350</td>\n",
              "      <td>#disgracefulesin I resent all men in some way;...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>707</th>\n",
              "      <td>10707</td>\n",
              "      <td>@l1ght__eyes u tried boiling em takes years too</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>10443</td>\n",
              "      <td>Indignation: [whispers to date during that ter...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>10197</td>\n",
              "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... sentiment_intensity\n",
              "152  10152  ...              medium\n",
              "350  10350  ...              medium\n",
              "707  10707  ...              medium\n",
              "443  10443  ...              medium\n",
              "197  10197  ...              medium\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IBOSYWxxOR6o"
      },
      "source": [
        "### Analizar los datos \n",
        "\n",
        "Imprimir la cantidad de tweets de cada dataset, seg√∫n su intensidad de sentimiento. Noten que las clases est√°n desbalanceadas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.117633Z",
          "start_time": "2020-04-07T15:44:21.090703Z"
        },
        "colab_type": "code",
        "id": "W4nQkKsjOR6t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "fb5e5cd8-2664-4700-8f73-4f58b483f6ed"
      },
      "source": [
        "def get_group_dist(group_name, train):\n",
        "    print(group_name, \"\\n\",\n",
        "          train[group_name].groupby('sentiment_intensity').count(),\n",
        "          '\\n---------------------------------------\\n')\n",
        "for dataset_name in train:\n",
        "    get_group_dist(dataset_name, train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anger \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 163    163    163\n",
            "low                  161    161    161\n",
            "medium               617    617    617 \n",
            "---------------------------------------\n",
            "\n",
            "fear \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 270    270    270\n",
            "low                  288    288    288\n",
            "medium               699    699    699 \n",
            "---------------------------------------\n",
            "\n",
            "joy \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 195    195    195\n",
            "low                  219    219    219\n",
            "medium               488    488    488 \n",
            "---------------------------------------\n",
            "\n",
            "sadness \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 197    197    197\n",
            "low                  210    210    210\n",
            "medium               453    453    453 \n",
            "---------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qakI2e_EUcBq"
      },
      "source": [
        "###Preprocesador: \n",
        "Se nos ocurri√≥ la idea de preparar el texto previo a clasificar, para unificar ciertas fuentes de informaci√≥n, como algunos simbolos, si es algo relacionado con el tiempo, con el dinero, si es un usuario, o si es un numero.\n",
        "\n",
        "Se intent√≥ arreglar sin exito el hecho que existian saltos de linea (\\n).\n",
        "\n",
        "Se us√≥ en un momento el pyspellchecker, pero no di√≥ buenos resultados.\n",
        "\n",
        "Por ultimo, se utiliz√≥ un Stemmer sobre las palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SxOSEeaeUbIH",
        "colab": {}
      },
      "source": [
        "def ch_user(tweet):\n",
        "    words = tweet.split()\n",
        "    n = len(words)\n",
        "    symb = ['@','#','!','?','.',',','*', '\\'', '\\\"', 'k', 'M', 'K', 'm', '-', '+']\n",
        "    money = ['$', '‚Ç¨', '¬£', '¬•', 'USD', 'US$', 'k', 'M', 'K', 'm', 'c', '¬¢', '.', ',', '!', '?', 'th', '\\'']\n",
        "    time = ['am', 'pm', 'yrs', 'y', 'h', 'm', 'hrs', 'min', 's', '\\'', '/', ':']\n",
        "    for i in range(n):\n",
        "        words[i] = ' '.join(words[i].split('\\n'))\n",
        "        if words[i] == '&amp;':\n",
        "            words[i] = '&'\n",
        "        if words[i][0] == '@':\n",
        "            words[i] = '@'\n",
        "        for w in symb:\n",
        "            numb = words[i].replace(w, '')\n",
        "        if (numb.isdigit()):\n",
        "            words[i] = '¬™'\n",
        "        for m in money:\n",
        "            mon = words[i].replace(m, '')\n",
        "        if (mon.isdigit()):\n",
        "            words[i] = '$'\n",
        "        for t in time:\n",
        "            tim = words[i].replace(t, '')\n",
        "        if (tim.isdigit()):\n",
        "            words[i] = 'TIME'\n",
        "        if isStemmer(normal):\n",
        "            words[i] = normal.stem(words[i])\n",
        "        if isLemmatizer(normal):\n",
        "            words[i] = normal.lemmatize(words[i])\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Preprocesador:\n",
        "\n",
        "class Preproci(BaseEstimator, TransformerMixin):\n",
        "    # Se cambian todos los username por 'username', con el prop√≥sito de disminuir la sparseness.\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(ch_user(tweet))\n",
        "        return pd.Series(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QkcqLzhZOR62"
      },
      "source": [
        "### Custom Features \n",
        "\n",
        "Se modific√≥ el CharsCountTransformer inicial y se agregaron m√°s simbolos\n",
        "\n",
        "Por otra parte, fue creada una lista con distintos subtipos de emoji, la cual se encuentra m√°s arriba y se cont√≥ cada vez que un emoji pertenec√≠a a cierto grupo. \n",
        "\n",
        "Con eso se agrupan tipos similares de emoji y se disminuye la sparseness respecto a un m√©todo que solo cuente todos emoji. \n",
        "(41 types vs >1500 types)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.128600Z",
          "start_time": "2020-04-07T15:44:21.119624Z"
        },
        "colab_type": "code",
        "id": "eiDRBGxKOR64",
        "colab": {}
      },
      "source": [
        "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        num_hashtags = tweet.count('#')\n",
        "        num_exclamations = tweet.count('!')\n",
        "        num_interrogations = tweet.count('?')\n",
        "        num_ats = tweet.count('@')\n",
        "        num_mon = tweet.count('$')\n",
        "        num_num = tweet.count('¬™')\n",
        "        num_tim = tweet.count('TIME')\n",
        "        num_and = tweet.count('&')\n",
        "        return [num_hashtags, num_exclamations, num_interrogations, num_ats, num_mon, num_num, num_tim, num_and]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class EmojiCountTransformer(BaseEstimator, TransformerMixin): # Se agrega un contador de categorias de emoji.\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        L = []\n",
        "        for i in emojiClassificationList: \n",
        "            num=0\n",
        "            for j in i:\n",
        "                num += tweet.count(j)\n",
        "            L.append(num)\n",
        "        return L\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.145564Z",
          "start_time": "2020-04-07T15:44:21.131593Z"
        },
        "colab_type": "code",
        "id": "plzsHgzoOR7B",
        "colab": {}
      },
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train['anger'].sample(5).tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1uVxlz2C2vEL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "231b0d35-38f1-41c7-9e54-6f1c84c34405"
      },
      "source": [
        "pd.DataFrame(zip(sample, CharsCountTransformer().transform(sample)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@oscar_perria a multimillionaire spoiled brat ...</td>\n",
              "      <td>[0, 1, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#Awareness seek #shelter .#Letgo Old #habit of...</td>\n",
              "      <td>[12, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the girl sitting in front of me is chewing her...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When anger rises, think of the consequences. #...</td>\n",
              "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@judahandthelion TONIGHT. Legit can't wait to ...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                          1\n",
              "0  @oscar_perria a multimillionaire spoiled brat ...   [0, 1, 0, 1, 0, 0, 0, 0]\n",
              "1  #Awareness seek #shelter .#Letgo Old #habit of...  [12, 0, 0, 0, 0, 0, 0, 0]\n",
              "2  the girl sitting in front of me is chewing her...   [0, 0, 0, 0, 0, 0, 0, 1]\n",
              "3  When anger rises, think of the consequences. #...   [3, 0, 0, 0, 0, 0, 0, 0]\n",
              "4  @judahandthelion TONIGHT. Legit can't wait to ...   [0, 0, 0, 1, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eHGQI2VI2mWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48e41ab6-7d75-44c5-e53c-80c203868e6d"
      },
      "source": [
        "pd.DataFrame(zip(sample, EmojiCountTransformer().transform(sample)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@oscar_perria a multimillionaire spoiled brat ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#Awareness seek #shelter .#Letgo Old #habit of...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the girl sitting in front of me is chewing her...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When anger rises, think of the consequences. #...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@judahandthelion TONIGHT. Legit can't wait to ...</td>\n",
              "      <td>[0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                                                  1\n",
              "0  @oscar_perria a multimillionaire spoiled brat ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1  #Awareness seek #shelter .#Letgo Old #habit of...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2  the girl sitting in front of me is chewing her...  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3  When anger rises, think of the consequences. #...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4  @judahandthelion TONIGHT. Legit can't wait to ...  [0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r7lFiDPqOR7Q"
      },
      "source": [
        "### Definir la representaci√≥n y el clasificador\n",
        "\n",
        "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando as√≠ nuestra programaci√≥n.\n",
        "\n",
        "El pipeline m√°s b√°sico que podemos hacer es transformar el dataset a Bag of Words y despu√©s usar clasificar el BoW usando NaiveBayes:\n",
        "\n",
        "```python\n",
        "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
        "```\n",
        "\n",
        "\n",
        "Ahora, si queremos usar nuestra transformaci√≥n para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenar√° los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
        "\n",
        "```python\n",
        "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
        "                                        ('chars_count',CharsCountTransformer())])),\n",
        "              ('clf', MultinomialNB())])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ANzhBruLOR7R"
      },
      "source": [
        "Recuerden que cada pipeline representa un sistema de clasificaci√≥n distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podr√≠an solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IGcdNvLOOR7W"
      },
      "source": [
        "### Ejecutar el pipeline para alg√∫n dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.167498Z",
          "start_time": "2020-04-07T15:44:21.157540Z"
        },
        "colab_type": "code",
        "id": "Jo6h2FjTOR7X",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "def run(dataset, dataset_name, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        random_state=42069,\n",
        "        test_size=0.33)\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "\n",
        "    # Evaluamos:\n",
        "    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores\n",
        "\n",
        "# La funcion anterior se copia para poder realizar las pruebas masivas. \n",
        "# La diferencia con la anterior es la funcion de evaluaci√≥n, √©sta no imprime nada.\n",
        "# Solo retorna los scores.\n",
        "\n",
        "def run_class(dataset, dataset_name, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        random_state=42069,\n",
        "        test_size=0.33)\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "\n",
        "    # Evaluamos:\n",
        "    scores = evaulate_class(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zuN1ZE0fOR7e"
      },
      "source": [
        "### Ejecutar el sistema creado por cada train set\n",
        "\n",
        "Este c√≥digo crea y entrena los 4 sistemas de clasificaci√≥n y luego los evalua para todas las combinaciones posibles entre clasificadores, vectorizadores y normalizadores de palabras. \n",
        "\n",
        "La primera celda corre una version modificada del m√©todo de evaluaci√≥n, el cual no imprime scores individualmente, sino que se centra en el Kappa mayor de cada clasificador y bajo que condiciones se logra √©ste.\n",
        "\n",
        "Si llegan a ejecutar esto, toma tiempo, en colab solo el primer clasificador toma 145 segundos en terminar de ejecutar todas las permutaciones.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.384119Z",
          "start_time": "2020-04-07T15:44:21.170488Z"
        },
        "colab_type": "code",
        "id": "CmddOSuaOR7f",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f927874-e5e0-4f02-a7eb-b40eb178d200"
      },
      "source": [
        "#Utilizaremos el kappa m√°s grande como referencia de lo mejor que podemos conseguir con esto.\n",
        "\n",
        "# Iteramos sobre cada combinacion posible.\n",
        "\n",
        "import time # Mediremos cuanto toma en clasificar todas las iteraciones por Clasificador.\n",
        "\n",
        "for FClassifier in FClassifiers:\n",
        "  start_time = time.time()\n",
        "  max_score=0\n",
        "  for vector in Vectorizers:\n",
        "    for normal in Normalizers:\n",
        "      classifiers = []\n",
        "      learned_labels_array = []\n",
        "      scores_array = []\n",
        "      #print(FClassifier, vector, normal)\n",
        "      for dataset_name, dataset in train.items(): # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "          # creamos el pipeline\n",
        "          pipeline = Pipeline([('preprocessor', Preproci()),\n",
        "                           ('features',\n",
        "                            FeatureUnion([('bow', vector),\n",
        "                                          ('chars_count', CharsCountTransformer()),\n",
        "                                          ('emoji_count', EmojiCountTransformer()) # Se puede agregar el EmojiCountTransformer() al Feature Union.\n",
        "                                          ])), ('clf', FClassifier)])  \n",
        "\n",
        "          # ejecutamos el pipeline sobre el dataset\n",
        "          #classifier, learned_labels, scores = run(dataset, dataset_name, pipeline) # M√°s detallado\n",
        "          classifier, learned_labels, scores = run_class(dataset, dataset_name, pipeline) \n",
        "\n",
        "          # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "          # classifiers.append(classifier), en esta secci√≥n no guardaremos los clasificadores, \n",
        "          # y simplemente veremos c√≥mo se comportan respecto al set de validaci√≥n.\n",
        "\n",
        "          # guardamos las labels aprendidas por el clasificador\n",
        "          learned_labels_array.append(learned_labels)\n",
        "\n",
        "          # guardamos los scores obtenidos\n",
        "          scores_array.append(scores)\n",
        "\n",
        "      # print avg scores\n",
        "      if np.array(scores_array).mean(axis=0)[1] > max_score:\n",
        "        max_score = np.array(scores_array).mean(axis=0)[1]\n",
        "        u = str(FClassifier)+str(vector)+str(normal)\n",
        "\n",
        "      #print(\n",
        "      #    \"Average scores:\\n\\n\",\n",
        "      #    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "      #    .format(*np.array(scores_array).mean(axis=0)))\n",
        "      #print()\n",
        "      #print('***************************************************************************')\n",
        "      #print()\n",
        "  print(\"Kappa = \",max_score, \"Usando:\", u)\n",
        "  # Obtenemos el m√°ximo score para cada clasificador, para eso, retornamos el \n",
        "  # kappa promedio de la combinaci√≥n m√°s ventajosa.\n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kappa =  0.31575 Usando: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 144.57175636291504 seconds ---\n",
            "Kappa =  0.24574999999999997 Usando: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<PorterStemmer>\n",
            "--- 89.78056812286377 seconds ---\n",
            "Kappa =  0.165 Usando: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
            "                     weights='uniform')CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 89.95281291007996 seconds ---\n",
            "Kappa =  0.1905 Usando: SVC(C=0.025, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 203.32903122901917 seconds ---\n",
            "Kappa =  0.055 Usando: SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=2, kernel='rbf', max_iter=-1,\n",
            "    probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)<PorterStemmer>\n",
            "--- 221.20453476905823 seconds ---\n",
            "Kappa =  0.17 Usando: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use_idf=True, vocabulary=None)None\n",
            "--- 92.73153138160706 seconds ---\n",
            "Kappa =  0 Usando: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use_idf=True, vocabulary=None)None\n",
            "--- 92.46348977088928 seconds ---\n",
            "Kappa =  0.1825 Usando: AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
            "                   n_estimators=50, random_state=None)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)None\n",
            "--- 182.43755173683167 seconds ---\n",
            "Kappa =  0.21475 Usando: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
            "              hidden_layer_sizes=50, learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)None\n",
            "--- 321.00750637054443 seconds ---\n",
            "Kappa =  0.207 Usando: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
            "              hidden_layer_sizes=100, learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)<nltk.stem.snowball.SnowballStemmer object at 0x7f90419b4860>\n",
            "--- 463.3171536922455 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtiSZBQsOBoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f752488-a6ce-49c2-e549-2a342d3db6fb"
      },
      "source": [
        "#Usemos los resultados anteriores para utilizar el mejor clasificador de los anteriores. \n",
        "classifiers = []\n",
        "learned_labels_array = []\n",
        "scores_array = []\n",
        "\n",
        "normal = Snowball\n",
        "for dataset_name, dataset in train.items(): # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  # creamos el pipeline\n",
        "  pipeline = Pipeline([('preprocessor', Preproci()),\n",
        "                   ('features',\n",
        "                    FeatureUnion([('bow', CountVectorizer()),\n",
        "                                  ('chars_count', CharsCountTransformer()),\n",
        "                                  ('emoji_count', EmojiCountTransformer())\n",
        "                                  # Se puede agregar el EmojiCountTransformer() al Feature Union.\n",
        "                                  ])), ('clf', LogisticRegression())])  \n",
        "\n",
        "  # ejecutamos el pipeline sobre el dataset\n",
        "  classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
        "\n",
        "  # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "  classifiers.append(classifier)\n",
        "\n",
        "  # guardamos las labels aprendidas por el clasificador\n",
        "  learned_labels_array.append(learned_labels)\n",
        "\n",
        "  # guardamos los scores obtenidos\n",
        "  scores_array.append(scores)\n",
        "print(\n",
        "      \"Average scores:\\n\\n\",\n",
        "      \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "      .format(*np.array(scores_array).mean(axis=0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  41   3]\n",
            " [ 15 181   7]\n",
            " [  1  27  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.41      0.20      0.27        55\n",
            "      medium       0.73      0.89      0.80       203\n",
            "        high       0.71      0.47      0.57        53\n",
            "\n",
            "    accuracy                           0.70       311\n",
            "   macro avg       0.62      0.52      0.55       311\n",
            "weighted avg       0.67      0.70      0.67       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.715\tKappa: 0.318\tAccuracy: 0.698\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 38  59   3]\n",
            " [ 27 182  26]\n",
            " [  9  39  32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.51      0.38      0.44       100\n",
            "      medium       0.65      0.77      0.71       235\n",
            "        high       0.52      0.40      0.45        80\n",
            "\n",
            "    accuracy                           0.61       415\n",
            "   macro avg       0.56      0.52      0.53       415\n",
            "weighted avg       0.59      0.61      0.59       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.679\tKappa: 0.281\tAccuracy: 0.607\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 24  49   2]\n",
            " [ 15 119  19]\n",
            " [  3  30  37]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.57      0.32      0.41        75\n",
            "      medium       0.60      0.78      0.68       153\n",
            "        high       0.64      0.53      0.58        70\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.60      0.54      0.56       298\n",
            "weighted avg       0.60      0.60      0.59       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.315\tAccuracy: 0.604\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 29  37   2]\n",
            " [ 15 115  20]\n",
            " [  5  28  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.59      0.43      0.50        68\n",
            "      medium       0.64      0.77      0.70       150\n",
            "        high       0.60      0.50      0.55        66\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.61      0.56      0.58       284\n",
            "weighted avg       0.62      0.62      0.61       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.715\tKappa: 0.349\tAccuracy: 0.623\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.707\t Average Kappa: 0.316\t Average Accuracy: 0.633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:37:43.169737Z",
          "start_time": "2019-08-21T19:37:43.166744Z"
        },
        "colab_type": "text",
        "id": "pYcqr1xSOR7k"
      },
      "source": [
        "### Predecir los target set y crear la submission\n",
        "\n",
        "Aqu√≠ predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.392097Z",
          "start_time": "2020-04-07T15:44:21.386114Z"
        },
        "colab_type": "code",
        "id": "1BXUHx8uOR7l",
        "colab": {}
      },
      "source": [
        "def predict_target(dataset, classifier, labels):\n",
        "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
        "    # Agregar ids\n",
        "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
        "    predicted['id'] = dataset.id.values\n",
        "    # Reordenar las columnas\n",
        "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.588573Z",
          "start_time": "2020-04-07T15:44:21.394094Z"
        },
        "colab_type": "code",
        "id": "MgkFATsKOR7p",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "predicted_target = {}\n",
        "\n",
        "# Crear carpeta ./predictions\n",
        "if (not os.path.exists('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "# por cada target set:\n",
        "for idx, key in enumerate(target):\n",
        "    # Predecirlo\n",
        "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
        "                                           learned_labels_array[idx])\n",
        "    # Guardar predicciones en archivos separados. \n",
        "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
        "                                 sep='\\t',\n",
        "                                 header=False,\n",
        "                                 index=False)\n",
        "\n",
        "# Crear archivo zip\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lLyMLxj_3yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "921ab91a-d115-46e4-ab16-d8af87a66ff2"
      },
      "source": [
        "# Caso sin features hechas a mano, sin emoji ni preprocesador:\n",
        "\n",
        "classifiers = []\n",
        "learned_labels_array = []\n",
        "scores_array = []\n",
        "\n",
        "normal = Snowball\n",
        "for dataset_name, dataset in train.items(): # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  # creamos el pipeline\n",
        "  pipeline = Pipeline([\n",
        "                   ('features',\n",
        "                    FeatureUnion([('bow', CountVectorizer()),\n",
        "                                  ('chars_count', CharsCountTransformer())\n",
        "                                  # Se puede agregar el EmojiCountTransformer() al Feature Union.\n",
        "                                  ])), ('clf', LogisticRegression())])  \n",
        "  # ejecutamos el pipeline sobre el dataset\n",
        "  classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
        "\n",
        "  # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "  classifiers.append(classifier)\n",
        "\n",
        "  # guardamos las labels aprendidas por el clasificador\n",
        "  learned_labels_array.append(learned_labels)\n",
        "\n",
        "  # guardamos los scores obtenidos\n",
        "  scores_array.append(scores)\n",
        "print(\n",
        "      \"Average scores:\\n\\n\",\n",
        "      \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "      .format(*np.array(scores_array).mean(axis=0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 10  43   2]\n",
            " [ 15 180   8]\n",
            " [  1  29  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.18      0.25        55\n",
            "      medium       0.71      0.89      0.79       203\n",
            "        high       0.70      0.43      0.53        53\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.60      0.50      0.52       311\n",
            "weighted avg       0.65      0.68      0.65       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.281\tAccuracy: 0.685\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 35  62   3]\n",
            " [ 29 176  30]\n",
            " [  8  44  28]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.49      0.35      0.41       100\n",
            "      medium       0.62      0.75      0.68       235\n",
            "        high       0.46      0.35      0.40        80\n",
            "\n",
            "    accuracy                           0.58       415\n",
            "   macro avg       0.52      0.48      0.49       415\n",
            "weighted avg       0.56      0.58      0.56       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.222\tAccuracy: 0.576\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 24  50   1]\n",
            " [ 16 119  18]\n",
            " [  2  30  38]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.57      0.32      0.41        75\n",
            "      medium       0.60      0.78      0.68       153\n",
            "        high       0.67      0.54      0.60        70\n",
            "\n",
            "    accuracy                           0.61       298\n",
            "   macro avg       0.61      0.55      0.56       298\n",
            "weighted avg       0.61      0.61      0.59       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.728\tKappa: 0.319\tAccuracy: 0.607\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 23  44   1]\n",
            " [ 17 118  15]\n",
            " [  2  38  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.55      0.34      0.42        68\n",
            "      medium       0.59      0.79      0.67       150\n",
            "        high       0.62      0.39      0.48        66\n",
            "\n",
            "    accuracy                           0.59       284\n",
            "   macro avg       0.59      0.51      0.52       284\n",
            "weighted avg       0.59      0.59      0.57       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.262\tAccuracy: 0.588\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.707\t Average Kappa: 0.271\t Average Accuracy: 0.614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UJ0X-IRgOR7u"
      },
      "source": [
        "## 6. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EnxUv31gOR7x"
      },
      "source": [
        "Discutir resultados, proponer trabajo futuro. (1 punto) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "byENZnZnEA9v"
      },
      "source": [
        "**Consideraciones Previas**\n",
        "\n",
        "Primero que nada, se realiz√≥ el an√°lisis de los resultados anteriores en base principalmente al Kappa, debido a que √©ste est√° normalizado respecto a una distribuici√≥n aleatoria, permite analizar resultados con datasets desbalanceados, como lo es en este caso, y es un buen indicador de correlaci√≥n para problemas multiclase. \n",
        "\n",
        "En la celda de c√≥digo en donde se ejecutan TODAS las clases, se prefiri√≥ retornar el promedio de cada una de las iteraciones de cada uno de los sentimientos para reducir el ruido de lo que es el an√°lisis de un sentimiento en particular, y se retorn√≥ para cada una de los clasificadores la mejor combinaci√≥n de features.\n",
        "\n",
        "Para hacer este informe breve, se adjunta un archivo de texto con los resultados de esa secci√≥n, y se correr√° de nuevo esa celda en una versi√≥n simplificada que retornar√° unicamente la mejor combinaci√≥n de features para cada clasificador.\n",
        "\n",
        "Lo anterior puede no ser una muy buena pr√°ctica, porque estamos en cierta manera sobreajustando respecto a un set de validaci√≥n se fij√≥, sin embargo, nos da una estimaci√≥n gruesa de hacia donde se debe ir.\n",
        "\n",
        "\n",
        "**An√°lisis de Resultados con Distintos Clasificadores y Features**\n",
        "\n",
        "Como se puede observar en la primera celda de c√≥digo de \"Ejecutar el sistema creado por cada train set\", de la Secci√≥n 5, la Regresi√≥n Log√≠stica es el mejor clasificador en base al Kappa. Le sigue Naive Bayes Multinomial y Multilayer Perceptron con 50 Hidden Layers y un hidden layer.\n",
        "\n",
        "En tanto, el clasificador con mayor variaci√≥n de puntajes segun los datos obtenidos es el Multilayer Perceptron, el cual si no converg√≠a, retornaba puntajes con Kappa cercanos a cero, y cuando converg√≠a, entregaba resultados cercanos o incluso superiores a la Regresi√≥n Logistica.\n",
        "\n",
        "Respecto al Vectorizador, usando la Regresi√≥n Logistica, el Vectorizador con mejores resultados es CountVectorizer de unigramas sin stopwords. \n",
        "Comparando CountVectorizer con Tfidf, el Kappa de CountVectorizer es 0.100 puntos en promedio m√°s alto, y bajo Tfidf ocurre que que para *algunos casos de Bigramas y Trigramas*, el Clasificador directamente *no converge*.\n",
        "\n",
        "Podemos observar adem√°s que aumentar el modelo de unigramas a bigramas o incluso trigramas en un caso en donde √©stos no estan relacionados probabilisticamente es detrimental al rendimiento del clasificador, debido a que aumentan la cantidad de dimensiones considerablemente, siguiendo la \"maldici√≥n de dimensionalidad\", reflejandose en la baja considerable del kappa entre unigramas y bigramas, y bigramas a trigramas, del orden de los 0.020 puntos.\n",
        "\n",
        "Apuntando a la normalizaci√≥n de texto, podemos concluir que es una buena practica utilizar ya sea un Lematizador como un Stemmer, es m√°s, la diferencia entre usar uno o no usarlo se traduce en 0.01 puntos de Kappa en promedio.\n",
        "\n",
        "Por √∫ltimo, se corri√≥ el mejor caso anterior posible, Regresi√≥n Logistica con CountVectorizer de unigramas sin stopwords y se removi√≥ del Feature Union los emoji. Este clasificador de emoji es de baja dimensionalidad (41), y permite clasificar 1500 emoji en categorias relacionadas con sentimientos, a√±adiendo contexto. Lo anterior se ve reflejado en el Kappa, a√±adir los emoji aumenta el Kappa en 0.045 puntos, una diferencia m√°s considerable que considerar el uso de Normalizadores en tokens (+0.010 puntos). \n",
        "\n",
        "\n",
        "**Resultados de la competencia**\n",
        "\n",
        "Esta entrega termin√≥ con los siguientes resultados en Codalab:\n",
        "\n",
        "- 0.689 AUC\n",
        "- 0.280 Kappa\n",
        "- 0.598 Accuracy\n",
        "\n",
        "Respecto a los resultados del baseline:\n",
        "\n",
        "- 0.651 AUC\n",
        "- 0.148 Kappa\n",
        "- 0.573 Accuracy\n",
        "\n",
        "Los resultados obtenidos en Codalab coinciden parcialmente con los resultados obtenidos debido a que la entrega hecha a Codalab es una version modificada de esta tarea, puesto que el archivo en la fecha de entrega estaba corrupto, haciendo que en las primeras entregas tuviesen kappa cercanos a cero, lo que indicar√≠a no correlaci√≥n y que el clasificador tenia un rendimiento similar a uno aleatorio. \n",
        "\n",
        "Sin embargo, en la competencia se pudo realizar una entrega parcial, la cual incorporaba casi todos los elementos de mayor rendimiento escritos en esta tarea, con la excepci√≥n de los emoji, pues exist√≠an problemas de longitud de features entre el train y el target, significando que la lista creada de emoji, no estaba optimizada para el pipeline existente. La adici√≥n de los emoji, que no estaban presentes en la entrega de la competencia, estimamos que aumentar√≠a el kappa al menos en alrededor de 0.020 puntos.\n",
        "\n",
        "Respecto a los resultados actuales, se tiene que √©stos superan al baseline en todos los √°mbitos, destacando el aumento de Kappa, que indica correlaci√≥n entre los resultados, de 0.132 puntos, es decir, casi al doble del inicial. \n",
        "\n",
        "\n",
        "**Respecto al trabajo**\n",
        "\n",
        "Pese a lo anterior, no nos vemos conformes con nuestro resultado porque no se pudo aplicar varias features m√°s por falta de tiempo. \n",
        "\n",
        "Sin embargo, aprendimos mucho sobre distintos clasificadores, y c√≥mo infuyen las features utilizadas en el resultado de √©stos, obteniendo informaci√≥n √∫til de ciertos rasgos sutiles del texto, y que tenemos a nuestro alcance herramientas de clasificaci√≥n de sentimientos que nos podr√°n ser √∫tiles m√°s adelante.\n",
        "\n",
        "Como trabajo a futuro estar√≠amos interesados en realizar los siguientes hitos:\n",
        "\n",
        "- Disminuir la sparseness, usando Word Embeddings.\n",
        "- Evaluar el uso de bigaramas y trigramas usando Naive Bayes correctamente, en un intento de mejorar el contexto, puesto que los modelos que probamos con bigramas y trigramas establec√≠an independencia absoluta, y solamente aumentaban la sparseness.\n",
        "- Aplicar mark_neg, para mejorar el contexto de las palabras.\n",
        "- Poder aplicar correctamente la clasificaci√≥n de Emoji.\n",
        "- Hacer un estudio correcto del algoritmo de Random Forests.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tg4IfRmLKsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}