{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4lL5hGw07yP"
   },
   "source": [
    "# Minitarea 4\n",
    "\n",
    "Por favor, en las respuestas de desarrollo expliquen lo que están haciendo. En las preguntas que son con desarrollo matemático pongan todos los pasos del cálculo (déjenlo bonito porfis :D).\n",
    "\n",
    "Usen $\\LaTeX$ para las fórmulas matemáticas.\n",
    "\n",
    "En la parte de programación pueden usar lo que quieran, pero la auxiliar 3 les puede servir de *inspiración*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWXD3D7RYKJ-"
   },
   "source": [
    "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n",
    "\n",
    "### Pregunta 1 (1 pt)\n",
    "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ D, N, V, P \\}$ y se tiene un Hidden Markov Model con los siguientes parámetros estimados a partir de un corpus de entrenamiento:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "q(D|N,P) &= 0.4 \\\\\n",
    "q(D|w,P) &= 0 \\qquad \\forall w \\in S, w \\neq N \\\\\n",
    "e(the|D) &= 0.6\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Luego para la oración: `Ella walks to the red house`, se tiene una tabla de programación dinámica con los siguientes valores:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\pi(3,D,P)&=0.1\\\\\n",
    "\\pi(3,N,P)&=0.2\\\\\n",
    "\\pi(3,V,P)&=0.01\\\\\n",
    "\\pi(3,P,P)&=0.5\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Con esta información, calcule el valor de $\\pi(4,P,D)$. Puede dejar el resultado expresado como una fracción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EzgysW9kGi-"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "$\\pi(4,P,D) = max_{\\omega \\epsilon S_{k-2}} (\\pi(3,\\omega,P)*q(D|\\omega,P)*e(the,D))$ \n",
    "\n",
    "$\\pi(3,\\omega,P)*q(D|\\omega,P)*e(the,D) = 0$ si $\\omega \\neq N$\n",
    "\n",
    "Si $\\omega = N$:\n",
    "\n",
    "$ \\pi(3,N,P)*q(D|N,P)*e(the,D) = 0.2*0.4*0.6 = \\frac{6}{125} $ \n",
    "\n",
    "Por lo tanto,\n",
    "\n",
    "$\\pi(4,P,D) = \\frac{6}{125} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiwJb_vmkKLZ"
   },
   "source": [
    "### Pregunta 2 (0.5 pts)\n",
    "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
    "* ¿Para qué tipo de tarea sirven? Dé dos ejemplo de este tipo de tarea y descríbalos brevemente.\n",
    "\n",
    "Todos estos modelos sirven para tareas de Part of Speech tagging o para problemas de Named Entity Recognition.\n",
    "\n",
    "El POS tagging es un problema de clasificar como su nombre lo dice etiquetas gramaticales, basadas en la palabra y en el contexto en el que aparecen.\n",
    "\n",
    "Ejemplos:\n",
    "- Input(El perro ladra) -> Output(DET, NOUN, VERB)\n",
    "- Input(Mauricio planta plantas rojas) -> Output(NOUN, VERB, NOUN, JJ)\n",
    "(Clasificación basada en el Treebank PoS)\n",
    "\n",
    "Por otra parte está en NER, que consiste en diferenciar Entidades como nombres, empresas o localidades del resto de la oración.\n",
    "\n",
    "Ejemplos:\n",
    "- Input(Este mes, CocaCola Co. lanza una nueva bebida.) -> Output(/NA, /NA, /SC, /CC, /NA, /NA, /NA, /NA)\n",
    "- Input(Javiera compra pasta de dientes Colgate en Las Condes) -> Output(/SP, /NA, /NA, /NA, /NA, /SC, /NA, /SL, /CL)\n",
    "\n",
    "\n",
    "* ¿Qué modelos usan features? ¿Qué ventajas conlleva esto?\n",
    "Los MEMMs y los CRFs utilizan features, por lo que la probabilidad de transición es sensible a cada palabra de la secuencia de input X. \n",
    "Además que permiten observar ciertos atributos, como capitalizacion, formato, entre otros, proveendo una representación rica en observaciones.\n",
    "\n",
    "Por ejemplo, al intentar extraer nombres de companias previamente desconocidas, la identidad de la palabra en sí no es muy predicitiva, sin embargo, conocer que la palabra está en mayusculas, que es un sustantivo y es usada en una aposición, que aparece en la parte superior del articulopuede ser muy predictivo. (A. McCallum, D. Freitag, F. Pereira, Maximum Entropy Markov Models for Information Extraction and Segmentation)\n",
    "\n",
    "* ¿Cómo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train?\n",
    "\n",
    "Los HMMs utilizan el siguiente principio:\n",
    "- Dividir el vocabulario en dos conjuntos, uno con palabras frecuentes >5 veces, y otro con palabras de baja frecuencia.\n",
    "- Se mapean las palabras de baja drecuencia a un conjunto pequeño finito dependiente de los prefijos, sufijos, etc.\n",
    "\n",
    "Los MEMMs y CRFs no poseen manejo de palabras de baja frecuencia, pero una practica usual es incorporar Word Embeddings previo al procesamiento de las palabras. \n",
    "\n",
    "* ¿Qué le permite a los CRF realizar decisiones globales? ¿Qué diferencia con respecto a los MEMMs permite lograr esto? ¿Por qué los HMMs tampoco son capaces de tomar decisiones globales?\n",
    "\n",
    "Los CRF poseen normalización global, mientras que la de los MEMMs es local. Por lo tanto, cada vez que se toma una desición en las CRFs se toma todo el conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClRAHR95Y8aB"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "### Pregunta 3 (1 pt)\n",
    "\n",
    "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
    "\n",
    "La siguiente matriz de embeddings, donde la i-ésima fila corresponde al vector de embedding de la i-ésima palabra, ordenadas segun aparecen en la frase. (vectores de largo 2).\n",
    "\\begin{equation}\n",
    "E = \\begin{pmatrix}\n",
    "2 & 2\\\\\n",
    "0 & -2\\\\\n",
    "0 & 1\\\\\n",
    "-2 & 1\\\\\n",
    "-2 & 0\\\\\n",
    "2 & -1\\\\\n",
    "0 & 2\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Los siguientes 3 filtros\n",
    "\\begin{equation}\n",
    "U = \\begin{pmatrix}\n",
    "-1 & -1 & 0\\\\\n",
    "1 & 1 & 0\\\\\n",
    "0 & 0 & -1\\\\\n",
    "-1 & -1 & -1\\\\\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 0 & -1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Y la función de activación\n",
    "\\begin{equation}\n",
    "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "\\end{equation}\n",
    "\n",
    "Usando estos paramátros calcule manualmente la representación (vector) resultante de aplicar la operación de convolución (sin padding) + max pooling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SlQ30Arkq0u4"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Utilicemos la transpuesta de E en el problema y dividamos U en sus tres filtros respectivos:\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\begin{pmatrix}\n",
    "2 & 0 & 0 & -2 & -2 & 2 & 0\\\\\n",
    "2 & -2 & 1 & 1 & 0 & -1 & 2\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "En donde cada columna ahora es cada vector de palabra.\n",
    "\n",
    "Ahora dividamos U en $U_1$, $U_2$ Y $U_3$.\n",
    "\n",
    "\\begin{equation}\n",
    "U_1 = \\begin{pmatrix}\n",
    "-1 & -1 & 0\\\\\n",
    "1 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "U_2 = \\begin{pmatrix}\n",
    "0 & 0 & -1\\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "U_3 = \\begin{pmatrix}\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 0 & -1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Si recorremos E con cada uno de los filtros, obtenemos los siguientes vectores fila denotados como C, como resultado de la convolución sin padding. Notese que el tamaño de cada uno de estos vectores es de 7 - 3 + 1 = 5.\n",
    "\n",
    "\\begin{equation}\n",
    "C_1 = \\begin{pmatrix}\n",
    "-2 & -1 & 4 & 5 & -1\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "C_2 = \\begin{pmatrix}\n",
    "-1 & 2 & 0 & -2 & -1\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "C_3 = \\begin{pmatrix}\n",
    "-1 & -5 & 1 & 8 & -2\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Y si obtenemos el max pooling de cada uno de los elementos de los resultado de la convolución, obtenemos la siguiente expresión:\n",
    "\n",
    "\\begin{equation}\n",
    "Max P = \\begin{pmatrix}\n",
    "-1 & 2 & 4 & 8 & -1\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Por último, si se utiliza como función de activación g(x) = tanh(x), termina con la siguiente salida:\n",
    "\n",
    "\\begin{equation}\n",
    "g(Max P) = \\begin{pmatrix}\n",
    "-0.761594 & 0.964028 & 0.999329 & 1 & -0.761594\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tj1V_sAzZCHY"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "### Pregunta 4 (1 pt)\n",
    "Sea la siguiente oración: `El perro ladra` representada como una secuencia de vectores $x1,x2,x3$. Donde cada vector corresponde al word embedding de cada palabra, que a la vez es un vector de dos dimensiones\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "x1 &= Embed(El)    &= [1, 0] \\\\\n",
    "x2 &= Embed(perro) &= [-1, 1] \\\\\n",
    "x3 &= Embed(ladra) &= [1,1]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Se tiene una red recurrente Elman: \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
    "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Con\n",
    "\\begin{equation}\n",
    "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad \\vec{x}_i \\in \\mathbb{R}^{d_x}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s}\n",
    "\\end{equation}\n",
    "y donde los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
    "\n",
    "Sea\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_0 &= [0,0,0]\\\\\n",
    "W^x &= \\begin{pmatrix}\n",
    "0 &  0 & 1\\\\\n",
    "1 & -1 & 0\n",
    "\\end{pmatrix} \\\\\n",
    "W^s &= \\begin{pmatrix}\n",
    "1 & 0 &  1\\\\\n",
    "0 & 1 & -1\\\\\n",
    "1 & 1 &  1\n",
    "\\end{pmatrix} \\\\\n",
    "\\vec{b} &= [0, 0, 0] \\\\\n",
    "g(x) &= ReLu(x) = max(0, x)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "Calcule manualmente los valores de $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7M7sqIQV-Q3a"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Me ha costado mucho reescribir todas las multiplicaciones y sumas de matrices en una linea, por lo que escribiré el resultado escrito a mano de cada estado y salida.\n",
    "\n",
    "\\begin{equation}\n",
    "s_1 = y_1 = \\begin{pmatrix}\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "s_2 = y_2 = \\begin{pmatrix}\n",
    "2 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "s_2 = y_2 = \\begin{pmatrix} 3 & 0 & 3\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Foto del calculo hecho:\n",
    "\n",
    "https://drive.google.com/file/d/1cyUHN5nLS4OLqmf8a3T6V7S-dkze4RJc/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4rAT6ELxRZW"
   },
   "source": [
    "### Pregunta 5 (0.5 pts)\n",
    "¿De qué forma las RNN y las CNN logran aprender representaciones específicas\n",
    "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* diseñadas manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6AXbQSgA_t8"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Las RNN aprenden representaciones utilizando vectores de estado, haciendo que varíen al entrenar, sobre funciones R y O que tambien son entrenadas.\n",
    "Mientras que las CNN aprenden las representaciones utilizando uno o más kernel (filtro/os) que extraen información de cada vector al usar el operador de convolución. Estos filtros van iterando con el objetivo de obtener una pérdida menor para la tarea determinada.\n",
    "\n",
    "Respecto a otros modelos que usan features manuales, las RNN y CNN varían sus vectores de estado, y los kernel respectivamente, mientras que los modelos que utilizan features manuales varían principalemtne los pesos que le dan a la salida de esas features fijas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxQIuO8axTUa"
   },
   "source": [
    "# Redes neuronales con PyTorch\n",
    "### Pregunta 6 (2 pts)\n",
    "En esta parte van a tener que implementar una red neuronal Feed Forward. Además, deberán entrenar el modelo usando uno de los datasets de TorchText. En la sección de la respuesta hay un esqueleto de lo que deben hacer, deberán completar los metodos del modelo y la parte asociada al entrenamiento la deben implementar ustedes. De todas formas, como les mencionamos en las auxiliares, el proceso de entrenamiento es bastante estándar, así que se pueden guiar en gran medida por los ejemplos que hemos visto y los que vamos a ver en las próximas auxiliares.\n",
    "\n",
    "### Bonus (0.5 pts)\n",
    "Agregue a la arquitectura una capa convolucional, para esto debe registrar el parametro $U$ en la red y realizar el computo de la convolución en el metodo forward de la red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVKEaQXZ3eGl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Nos aseguramos que torchtext este en la ultima version\n",
    "!pip install --upgrade torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "dJ-wrzFO5mCC",
    "outputId": "de898848-3def-4c0f-84c0-09896b34ab12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:06, 17641.82lines/s]\n",
      "120000lines [00:12, 9448.04lines/s] \n",
      "7600lines [00:00, 10618.09lines/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Un ejemplo aleatorio\n",
      " (1, tensor([ 4525,   541,   559,   392,  2998,    13,    10,  6509,  4525, 15577,\n",
      "            6, 12098,  1256,     8,     3,    70,   248,  2782,     5,   110,\n",
      "            6,   146,   527,  1786,    51,   418,     8,   559,     2]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Trabajen con el siguiente dataset\n",
    "import os\n",
    "from random import choice\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "os.makedirs(\".data\", exist_ok=True)\n",
    "train_dataset, test_dataset = AG_NEWS(root=\".data\")\n",
    "\n",
    "print(\"\\nUn ejemplo aleatorio\\n\", choice(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "wSHn-hYi8XN8",
    "outputId": "46e96894-5243-4fd8-9898-2ff5b8d388e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n",
      "('short series was long on highlights best player it was hard to argue with '\n",
      " 'boston #39 s manny ramirez , who was recognized as the mvp . a case could be '\n",
      " 'made for at least four or five other red sox , but ramirez made a '\n",
      " 'significant contribution with his bat in every game .')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Informacion relevante del dataset\n",
    "vocab = train_dataset.get_vocab()\n",
    "labels = train_dataset.get_labels()\n",
    "# El nombre de cada label lo pueden encontrar aqui\n",
    "# https://pytorch.org/text/datasets.html#ag-news, aunque no es necesario para \n",
    "# clasificar\n",
    "print(labels)\n",
    "# Un ejemplo convertido a texto\n",
    "pprint(\" \".join(vocab.itos[token] for token in choice(train_dataset)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXngUm9HxKvA"
   },
   "outputs": [],
   "source": [
    "# De aca para abajo viene su respuesta, completen las funciones en la red\n",
    "# y luego entrenen el modelo y evaluenlo usando los dataset que acaban de\n",
    "# cargar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim) # Parte con una capa de embedding\n",
    "        #self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=hidden_dim1, kernel_size=embed_dim*3) # Luego se aplica convolución\n",
    "        #self.ffc1 = nn.Linear(hidden_dim1, hidden_dim2) # Esto pasa por una capa oculta\n",
    "        #self.ffc2 = nn.Linear(hidden_dim2, output_dim) # Ultima capa\n",
    "        # Alternativo\n",
    "        self.ffc1 = nn.Linear(embed_dim, hidden_dim1) # Esto pasa por una capa oculta\n",
    "        self.ffc2 = nn.Linear(hidden_dim1, output_dim) # Ultima capa\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x) \n",
    "        #out = self.conv(out) # Aquí está el paso de convolución.\n",
    "        out = self.ffc1(out)\n",
    "        out = self.ffc2(out) \n",
    "        return torch.softmax(out, -1) # Se aplica una softmax a la salida de la ultima capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPHyqgId96ra"
   },
   "outputs": [],
   "source": [
    "# El resto de su respuesta\n",
    "# Aca deben programar el entrenamiento de la red\n",
    "\n",
    "def generate_batch(batch):\n",
    "    labels = torch.tensor([entry[0] for entry in batch])\n",
    "    sequences = [entry[1] for entry in batch]\n",
    "    text = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=train_dataset.get_vocab()[\"<pad>\"]\n",
    "    )\n",
    "    return text, labels\n",
    "\n",
    "# Esta celda es lo de toda la vida, definir funciones para entrenar y evaluar\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_, model):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss ,train_acc = 0, 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for text, cls in data:\n",
    "        optimizer.zero_grad()\n",
    "        text, cls = text.to(device), cls.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_, model):\n",
    "    loss, acc = 0, 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, cls in data:\n",
    "        text, cls = text.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo CNNClassifier(\n",
      "  (embedding): EmbeddingBag(95812, 64, mode=mean)\n",
      "  (ffc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (ffc2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Epoch: 1  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 27.8%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 31.8%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 15 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 30.2%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 27.0%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 32.0%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 25.2%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 33.0%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 25.4%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 12 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 33.8%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 36.8%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 12 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.1%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 38.3%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 12 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.8%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 26.3%(valid)\n",
      "Epoch: 8  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 33.6%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 42.8%(valid)\n",
      "Epoch: 9  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.8%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 41.5%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.7%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 26.6%(valid)\n",
      "Epoch: 11  | time in 0 minutes, 12 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 32.7%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 46.7%(valid)\n",
      "Epoch: 12  | time in 0 minutes, 14 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 35.1%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 51.2%(valid)\n",
      "Epoch: 13  | time in 0 minutes, 14 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.9%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 27.5%(valid)\n",
      "Epoch: 14  | time in 0 minutes, 12 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.7%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 25.5%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 12 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.1%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 46.0%(valid)\n",
      "Epoch: 16  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 35.5%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 43.1%(valid)\n",
      "Epoch: 17  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 36.0%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 28.2%(valid)\n",
      "Epoch: 18  | time in 0 minutes, 14 seconds\n",
      "\tLoss: 0.0026(train)\t|\tAcc: 36.4%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 29.2%(valid)\n",
      "Epoch: 19  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 34.9%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 37.0%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 15 seconds\n",
      "\tLoss: 0.0027(train)\t|\tAcc: 36.0%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 34.1%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "EMBED_DIM = 64\n",
    "LEARN_RATE = 3.0\n",
    "\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "NUM_CLASS = len(train_dataset.get_labels())\n",
    "\n",
    "HIDDEN_DIM1 = 128\n",
    "HIDDEN_DIM2 = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.85)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "model = CNNClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM1, HIDDEN_DIM2, NUM_CLASS).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "print(f\"\\nEntrenando modelo {model}\\n\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_, model)\n",
    "    valid_loss, valid_acc = test(sub_valid_, model)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No logro solucionar un problema con las dimensiones. Tiene que ver con la salida del embedding layer, o la entrada de la convolución pero todavia no sale :c \n",
    "\n",
    "De todas maneras, entrena la NN feed forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "minitarea4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
